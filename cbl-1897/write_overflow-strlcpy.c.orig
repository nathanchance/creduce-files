



struct ftrace_branch_data {
 const char *func;
 const char *file;
 unsigned line;
 union {
  struct {
   unsigned long correct;
   unsigned long incorrect;
  };
  struct {
   unsigned long miss;
   unsigned long hit;
  };
  unsigned long miss_hit[2];
 };
};

struct ftrace_likely_data {
 struct ftrace_branch_data data;
 unsigned long constant;
};









typedef __builtin_va_list va_list;










































typedef __signed__ char __s8;
typedef unsigned char __u8;

typedef __signed__ short __s16;
typedef unsigned short __u16;

typedef __signed__ int __s32;
typedef unsigned int __u32;


__extension__ typedef __signed__ long long __s64;
__extension__ typedef unsigned long long __u64;




typedef __s8 s8;
typedef __u8 u8;
typedef __s16 s16;
typedef __u16 u16;
typedef __s32 s32;
typedef __u32 u32;
typedef __s64 s64;
typedef __u64 u64;




















enum {
 false = 0,
 true = 1
};
typedef struct {
 unsigned long fds_bits[1024 / (8 * sizeof(long))];
} __kernel_fd_set;


typedef void (*__kernel_sighandler_t)(int);


typedef int __kernel_key_t;
typedef int __kernel_mqd_t;





typedef long __kernel_long_t;
typedef unsigned long __kernel_ulong_t;



typedef __kernel_ulong_t __kernel_ino_t;



typedef unsigned int __kernel_mode_t;



typedef int __kernel_pid_t;



typedef int __kernel_ipc_pid_t;



typedef unsigned int __kernel_uid_t;
typedef unsigned int __kernel_gid_t;



typedef __kernel_long_t __kernel_suseconds_t;



typedef int __kernel_daddr_t;



typedef unsigned int __kernel_uid32_t;
typedef unsigned int __kernel_gid32_t;



typedef __kernel_uid_t __kernel_old_uid_t;
typedef __kernel_gid_t __kernel_old_gid_t;



typedef unsigned int __kernel_old_dev_t;
typedef __kernel_ulong_t __kernel_size_t;
typedef __kernel_long_t __kernel_ssize_t;
typedef __kernel_long_t __kernel_ptrdiff_t;




typedef struct {
 int val[2];
} __kernel_fsid_t;





typedef __kernel_long_t __kernel_off_t;
typedef long long __kernel_loff_t;
typedef __kernel_long_t __kernel_old_time_t;



typedef long long __kernel_time64_t;
typedef __kernel_long_t __kernel_clock_t;
typedef int __kernel_timer_t;
typedef int __kernel_clockid_t;
typedef char * __kernel_caddr_t;
typedef unsigned short __kernel_uid16_t;
typedef unsigned short __kernel_gid16_t;


typedef __signed__ __int128 __s128 __attribute__((aligned(16)));
typedef unsigned __int128 __u128 __attribute__((aligned(16)));
typedef __u16 __le16;
typedef __u16 __be16;
typedef __u32 __le32;
typedef __u32 __be32;
typedef __u64 __le64;
typedef __u64 __be64;

typedef __u16 __sum16;
typedef __u32 __wsum;
typedef unsigned __poll_t;







typedef __s128 s128;
typedef __u128 u128;


typedef u32 __kernel_dev_t;

typedef __kernel_fd_set fd_set;
typedef __kernel_dev_t dev_t;
typedef __kernel_ulong_t ino_t;
typedef __kernel_mode_t mode_t;
typedef unsigned short umode_t;
typedef u32 nlink_t;
typedef __kernel_off_t off_t;
typedef __kernel_pid_t pid_t;
typedef __kernel_daddr_t daddr_t;
typedef __kernel_key_t key_t;
typedef __kernel_suseconds_t suseconds_t;
typedef __kernel_timer_t timer_t;
typedef __kernel_clockid_t clockid_t;
typedef __kernel_mqd_t mqd_t;

typedef _Bool bool;

typedef __kernel_uid32_t uid_t;
typedef __kernel_gid32_t gid_t;
typedef __kernel_uid16_t uid16_t;
typedef __kernel_gid16_t gid16_t;

typedef unsigned long uintptr_t;
typedef long intptr_t;
typedef __kernel_loff_t loff_t;
typedef __kernel_size_t size_t;




typedef __kernel_ssize_t ssize_t;




typedef __kernel_ptrdiff_t ptrdiff_t;




typedef __kernel_clock_t clock_t;




typedef __kernel_caddr_t caddr_t;



typedef unsigned char u_char;
typedef unsigned short u_short;
typedef unsigned int u_int;
typedef unsigned long u_long;


typedef unsigned char unchar;
typedef unsigned short ushort;
typedef unsigned int uint;
typedef unsigned long ulong;




typedef u8 u_int8_t;
typedef s8 int8_t;
typedef u16 u_int16_t;
typedef s16 int16_t;
typedef u32 u_int32_t;
typedef s32 int32_t;



typedef u8 uint8_t;
typedef u16 uint16_t;
typedef u32 uint32_t;


typedef u64 uint64_t;
typedef u64 u_int64_t;
typedef s64 int64_t;
typedef u64 sector_t;
typedef u64 blkcnt_t;
typedef u64 dma_addr_t;




typedef unsigned int gfp_t;
typedef unsigned int slab_flags_t;
typedef unsigned int fmode_t;


typedef u64 phys_addr_t;




typedef phys_addr_t resource_size_t;





typedef unsigned long irq_hw_number_t;

typedef struct {
 int counter;
} atomic_t;




typedef struct {
 s64 counter;
} atomic64_t;


typedef struct {
 atomic_t refcnt;
} rcuref_t;



struct list_head {
 struct list_head *next, *prev;
};

struct hlist_head {
 struct hlist_node *first;
};

struct hlist_node {
 struct hlist_node *next, **pprev;
};

struct ustat {
 __kernel_daddr_t f_tfree;



 unsigned long f_tinode;

 char f_fname[6];
 char f_fpack[6];
};
struct callback_head {
 struct callback_head *next;
 void (*func)(struct callback_head *head);
} __attribute__((aligned(sizeof(void *))));


typedef void (*rcu_callback_t)(struct callback_head *head);
typedef void (*call_rcu_func_t)(struct callback_head *head, rcu_callback_t func);

typedef void (*swap_r_func_t)(void *a, void *b, int size, const void *priv);
typedef void (*swap_func_t)(void *a, void *b, int size);

typedef int (*cmp_r_func_t)(const void *a, const void *b, const void *priv);
typedef int (*cmp_func_t)(const void *a, const void *b);









void ftrace_likely_update(struct ftrace_likely_data *f, int val,
     int expect, int is_constant);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void *offset_to_ptr(const int *off)
{
 return (void *)((unsigned long)off + *off);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool __kasan_check_read(const volatile void *p, unsigned int size)
{
 return true;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool __kasan_check_write(const volatile void *p, unsigned int size)
{
 return true;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool kasan_check_read(const volatile void *p, unsigned int size)
{
 return true;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool kasan_check_write(const volatile void *p, unsigned int size)
{
 return true;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __kcsan_check_access(const volatile void *ptr, size_t size,
     int type) { }

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __kcsan_mb(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __kcsan_wmb(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __kcsan_rmb(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __kcsan_release(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kcsan_disable_current(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kcsan_enable_current(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kcsan_enable_current_nowarn(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kcsan_nestable_atomic_begin(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kcsan_nestable_atomic_end(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kcsan_flat_atomic_begin(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kcsan_flat_atomic_end(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kcsan_atomic_next(int n) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kcsan_set_access_mask(unsigned long mask) { }

struct kcsan_scoped_access { };

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) struct kcsan_scoped_access *
kcsan_begin_scoped_access(const volatile void *ptr, size_t size, int type,
     struct kcsan_scoped_access *sa) { return sa; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kcsan_end_scoped_access(struct kcsan_scoped_access *sa) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kcsan_check_access(const volatile void *ptr, size_t size,
          int type) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __kcsan_enable_current(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __kcsan_disable_current(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__))
unsigned long __read_once_word_nocheck(const void *addr)
{
 return (*(const volatile typeof( _Generic((*(unsigned long *)addr), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*(unsigned long *)addr))) *)&(*(unsigned long *)addr));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__))
unsigned long read_word_at_a_time(const void *addr)
{
 kasan_check_read(addr, 1);
 return *(unsigned long *)addr;
}





























struct sysinfo {
 __kernel_long_t uptime;
 __kernel_ulong_t loads[3];
 __kernel_ulong_t totalram;
 __kernel_ulong_t freeram;
 __kernel_ulong_t sharedram;
 __kernel_ulong_t bufferram;
 __kernel_ulong_t totalswap;
 __kernel_ulong_t freeswap;
 __u16 procs;
 __u16 pad;
 __kernel_ulong_t totalhigh;
 __kernel_ulong_t freehigh;
 __u32 mem_unit;
 char _f[20-2*sizeof(__kernel_ulong_t)-sizeof(__u32)];
};
extern unsigned int __sw_hweight8(unsigned int w);
extern unsigned int __sw_hweight16(unsigned int w);
extern unsigned int __sw_hweight32(unsigned int w);
extern unsigned long __sw_hweight64(__u64 w);












static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long array_index_mask_nospec(unsigned long index,
          unsigned long size)
{
 unsigned long mask;

 __asm__ __volatile__(
  "sltu	%0, %1, %2\n\t"



  "sub.d	%0, $zero, %0\n\t"

  : "=r" (mask)
  : "r" (index), "r" (size)
  :);

 return mask;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
generic___set_bit(unsigned long nr, volatile unsigned long *addr)
{
 unsigned long mask = ((((1UL))) << ((nr) % 64));
 unsigned long *p = ((unsigned long *)addr) + ((nr) / 64);

 *p |= mask;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
generic___clear_bit(unsigned long nr, volatile unsigned long *addr)
{
 unsigned long mask = ((((1UL))) << ((nr) % 64));
 unsigned long *p = ((unsigned long *)addr) + ((nr) / 64);

 *p &= ~mask;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
generic___change_bit(unsigned long nr, volatile unsigned long *addr)
{
 unsigned long mask = ((((1UL))) << ((nr) % 64));
 unsigned long *p = ((unsigned long *)addr) + ((nr) / 64);

 *p ^= mask;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
generic___test_and_set_bit(unsigned long nr, volatile unsigned long *addr)
{
 unsigned long mask = ((((1UL))) << ((nr) % 64));
 unsigned long *p = ((unsigned long *)addr) + ((nr) / 64);
 unsigned long old = *p;

 *p = old | mask;
 return (old & mask) != 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
generic___test_and_clear_bit(unsigned long nr, volatile unsigned long *addr)
{
 unsigned long mask = ((((1UL))) << ((nr) % 64));
 unsigned long *p = ((unsigned long *)addr) + ((nr) / 64);
 unsigned long old = *p;

 *p = old & ~mask;
 return (old & mask) != 0;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
generic___test_and_change_bit(unsigned long nr, volatile unsigned long *addr)
{
 unsigned long mask = ((((1UL))) << ((nr) % 64));
 unsigned long *p = ((unsigned long *)addr) + ((nr) / 64);
 unsigned long old = *p;

 *p = old ^ mask;
 return (old & mask) != 0;
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
generic_test_bit(unsigned long nr, const volatile unsigned long *addr)
{





 return 1UL & (addr[((nr) / 64)] >> (nr & (64 -1)));
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
generic_test_bit_acquire(unsigned long nr, const volatile unsigned long *addr)
{
 unsigned long *p = ((unsigned long *)addr) + ((nr) / 64);
 return 1UL & (({ typeof(*p) ___p1 = ({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_0(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*p) == sizeof(char) || sizeof(*p) == sizeof(short) || sizeof(*p) == sizeof(int) || sizeof(*p) == sizeof(long)) || sizeof(*p) == sizeof(long long))) __compiletime_assert_0(); } while (0); (*(const volatile typeof( _Generic((*p), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*p))) *)&(*p)); }); do { __attribute__((__noreturn__)) extern void __compiletime_assert_1(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*p) == sizeof(char) || sizeof(*p) == sizeof(short) || sizeof(*p) == sizeof(int) || sizeof(*p) == sizeof(long)))) __compiletime_assert_1(); } while (0); __asm__ __volatile__("dbar %0 " : : "I"(0b10100) : "memory"); ___p1; }) >> (nr & (64 -1)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
const_test_bit(unsigned long nr, const volatile unsigned long *addr)
{
 const unsigned long *p = (const unsigned long *)addr + ((nr) / 64);
 unsigned long mask = ((((1UL))) << ((nr) % 64));
 unsigned long val = *p;

 return !!(val & mask);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int fls(unsigned int x)
{
 return x ? sizeof(x) * 8 - __builtin_clz(x) : 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) unsigned long __ffs(unsigned long word)
{
 return __builtin_ctzl(word);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) unsigned long __fls(unsigned long word)
{
 return (sizeof(word) * 8) - 1 - __builtin_clzl(word);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int fls64(__u64 x)
{
 if (x == 0)
  return 0;
 return __fls(x) + 1;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int sched_find_first_bit(const unsigned long *b)
{

 if (b[0])
  return __ffs(b[0]);
 return __ffs(b[1]) + 64;
}










static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned int __arch_hweight32(unsigned int w)
{
 return __sw_hweight32(w);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned int __arch_hweight16(unsigned int w)
{
 return __sw_hweight16(w);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned int __arch_hweight8(unsigned int w)
{
 return __sw_hweight8(w);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long __arch_hweight64(__u64 w)
{
 return __sw_hweight64(w);
}











static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned int __xchg_small(volatile void *ptr, unsigned int val,
     unsigned int size)
{
 unsigned int shift;
 u32 old32, mask, temp;
 volatile u32 *ptr32;


 mask = ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((0) > ((size * 8) - 1)) * 0l)) : (int *)8))), (0) > ((size * 8) - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (0)) + 1) & (~(((0UL))) >> (64 - 1 - ((size * 8) - 1)))));
 val &= mask;






 shift = (unsigned long)ptr & 0x3;
 shift *= 8;
 mask <<= shift;





 ptr32 = (volatile u32 *)((unsigned long)ptr & ~0x3);

 asm volatile (
 "1:	ll.w		%0, %3		\n"
 "	andn		%1, %0, %z4	\n"
 "	or		%1, %1, %z5	\n"
 "	sc.w		%1, %2		\n"
 "	beqz		%1, 1b		\n"
 : "=&r" (old32), "=&r" (temp), "=ZC" (*ptr32)
 : "ZC" (*ptr32), "Jr" (mask), "Jr" (val << shift)
 : "memory");

 return (old32 & mask) >> shift;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) unsigned long
__arch_xchg(volatile void *ptr, unsigned long x, int size)
{
 switch (size) {
 case 1:
 case 2:
  return __xchg_small(ptr, x, size);

 case 4:
  return ({ __typeof((u32)x) __ret; __asm__ __volatile__ ( " ""amswap_db.w"" %1, %z2, %0 \n" : "+ZB" (*(volatile u32 *)ptr), "=&r" (__ret) : "Jr" ((u32)x) : "memory"); __ret; });

 case 8:
  return ({ __typeof((u64)x) __ret; __asm__ __volatile__ ( " ""amswap_db.d"" %1, %z2, %0 \n" : "+ZB" (*(volatile u64 *)ptr), "=&r" (__ret) : "Jr" ((u64)x) : "memory"); __ret; });

 default:
  do { __attribute__((__noreturn__)) extern void __compiletime_assert_2(void) __attribute__((__error__("BUILD_BUG failed"))); if (!(!(1))) __compiletime_assert_2(); } while (0);
 }

 return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned int __cmpxchg_small(volatile void *ptr, unsigned int old,
        unsigned int new, unsigned int size)
{
 unsigned int shift;
 u32 old32, mask, temp;
 volatile u32 *ptr32;


 mask = ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((0) > ((size * 8) - 1)) * 0l)) : (int *)8))), (0) > ((size * 8) - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (0)) + 1) & (~(((0UL))) >> (64 - 1 - ((size * 8) - 1)))));
 old &= mask;
 new &= mask;






 shift = (unsigned long)ptr & 0x3;
 shift *= 8;
 old <<= shift;
 new <<= shift;
 mask <<= shift;





 ptr32 = (volatile u32 *)((unsigned long)ptr & ~0x3);

 asm volatile (
 "1:	ll.w		%0, %3		\n"
 "	and		%1, %0, %z4	\n"
 "	bne		%1, %z5, 2f	\n"
 "	andn		%1, %0, %z4	\n"
 "	or		%1, %1, %z6	\n"
 "	sc.w		%1, %2		\n"
 "	beqz		%1, 1b		\n"
 "	b		3f		\n"
 "2:					\n"
 "	dbar 0x700	\n"
 "3:					\n"
 : "=&r" (old32), "=&r" (temp), "=ZC" (*ptr32)
 : "ZC" (*ptr32), "Jr" (mask), "Jr" (old), "Jr" (new)
 : "memory");

 return (old32 & mask) >> shift;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) unsigned long
__cmpxchg(volatile void *ptr, unsigned long old, unsigned long new, unsigned int size)
{
 switch (size) {
 case 1:
 case 2:
  return __cmpxchg_small(ptr, old, new, size);

 case 4:
  return ({ __typeof((u32)old) __ret; __asm__ __volatile__( "1:	" "ll.w" "	%0, %2		# __cmpxchg_asm \n" "	bne	%0, %z3, 2f			\n" "	move	$t0, %z4			\n" "	" "sc.w" "	$t0, %1				\n" "	beqz	$t0, 1b				\n" "2:						\n" "	dbar 0x700	\n" : "=&r" (__ret), "=ZB"(*(volatile u32 *)ptr) : "ZB"(*(volatile u32 *)ptr), "Jr" ((u32)old), "Jr" (new) : "t0", "memory"); __ret; });


 case 8:
  return ({ __typeof((u64)old) __ret; __asm__ __volatile__( "1:	" "ll.d" "	%0, %2		# __cmpxchg_asm \n" "	bne	%0, %z3, 2f			\n" "	move	$t0, %z4			\n" "	" "sc.d" "	$t0, %1				\n" "	beqz	$t0, 1b				\n" "2:						\n" "	dbar 0x700	\n" : "=&r" (__ret), "=ZB"(*(volatile u64 *)ptr) : "ZB"(*(volatile u64 *)ptr), "Jr" ((u64)old), "Jr" (new) : "t0", "memory"); __ret; });


 default:
  do { __attribute__((__noreturn__)) extern void __compiletime_assert_3(void) __attribute__((__error__("BUILD_BUG failed"))); if (!(!(1))) __compiletime_assert_3(); } while (0);
 }

 return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void arch_atomic_add(int i, atomic_t *v) { __asm__ __volatile__( "am""add""_db.w" " $zero, %1, %0	\n" : "+ZB" (v->counter) : "r" (i) : "memory"); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int arch_atomic_add_return_relaxed(int i, atomic_t *v) { int result; __asm__ __volatile__( "am""add""_db.w" " %1, %2, %0		\n" : "+ZB" (v->counter), "=&r" (result) : "r" (i) : "memory"); return result + i; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int arch_atomic_fetch_add_relaxed(int i, atomic_t *v) { int result; __asm__ __volatile__( "am""add""_db.w" " %1, %2, %0		\n" : "+ZB" (v->counter), "=&r" (result) : "r" (i) : "memory"); return result; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void arch_atomic_sub(int i, atomic_t *v) { __asm__ __volatile__( "am""add""_db.w" " $zero, %1, %0	\n" : "+ZB" (v->counter) : "r" (-i) : "memory"); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int arch_atomic_sub_return_relaxed(int i, atomic_t *v) { int result; __asm__ __volatile__( "am""add""_db.w" " %1, %2, %0		\n" : "+ZB" (v->counter), "=&r" (result) : "r" (-i) : "memory"); return result + -i; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int arch_atomic_fetch_sub_relaxed(int i, atomic_t *v) { int result; __asm__ __volatile__( "am""add""_db.w" " %1, %2, %0		\n" : "+ZB" (v->counter), "=&r" (result) : "r" (-i) : "memory"); return result; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void arch_atomic_and(int i, atomic_t *v) { __asm__ __volatile__( "am""and""_db.w" " $zero, %1, %0	\n" : "+ZB" (v->counter) : "r" (i) : "memory"); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int arch_atomic_fetch_and_relaxed(int i, atomic_t *v) { int result; __asm__ __volatile__( "am""and""_db.w" " %1, %2, %0		\n" : "+ZB" (v->counter), "=&r" (result) : "r" (i) : "memory"); return result; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void arch_atomic_or(int i, atomic_t *v) { __asm__ __volatile__( "am""or""_db.w" " $zero, %1, %0	\n" : "+ZB" (v->counter) : "r" (i) : "memory"); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int arch_atomic_fetch_or_relaxed(int i, atomic_t *v) { int result; __asm__ __volatile__( "am""or""_db.w" " %1, %2, %0		\n" : "+ZB" (v->counter), "=&r" (result) : "r" (i) : "memory"); return result; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void arch_atomic_xor(int i, atomic_t *v) { __asm__ __volatile__( "am""xor""_db.w" " $zero, %1, %0	\n" : "+ZB" (v->counter) : "r" (i) : "memory"); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int arch_atomic_fetch_xor_relaxed(int i, atomic_t *v) { int result; __asm__ __volatile__( "am""xor""_db.w" " %1, %2, %0		\n" : "+ZB" (v->counter), "=&r" (result) : "r" (i) : "memory"); return result; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int arch_atomic_fetch_add_unless(atomic_t *v, int a, int u)
{
       int prev, rc;

 __asm__ __volatile__ (
  "0:	ll.w	%[p],  %[c]\n"
  "	beq	%[p],  %[u], 1f\n"
  "	add.w	%[rc], %[p], %[a]\n"
  "	sc.w	%[rc], %[c]\n"
  "	beqz	%[rc], 0b\n"
  "	b	2f\n"
  "1:\n"
  "	dbar 0x700	\n"
  "2:\n"
  : [p]"=&r" (prev), [rc]"=&r" (rc),
    [c]"=ZB" (v->counter)
  : [a]"r" (a), [u]"r" (u)
  : "memory");

 return prev;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int arch_atomic_sub_if_positive(int i, atomic_t *v)
{
 int result;
 int temp;

 if (__builtin_constant_p(i)) {
  __asm__ __volatile__(
  "1:	ll.w	%1, %2		# atomic_sub_if_positive\n"
  "	addi.w	%0, %1, %3				\n"
  "	move	%1, %0					\n"
  "	bltz	%0, 2f					\n"
  "	sc.w	%1, %2					\n"
  "	beqz	%1, 1b					\n"
  "2:							\n"
  "	dbar 0x700	\n"
  : "=&r" (result), "=&r" (temp), "+ZC" (v->counter)
  : "I" (-i));
 } else {
  __asm__ __volatile__(
  "1:	ll.w	%1, %2		# atomic_sub_if_positive\n"
  "	sub.w	%0, %1, %3				\n"
  "	move	%1, %0					\n"
  "	bltz	%0, 2f					\n"
  "	sc.w	%1, %2					\n"
  "	beqz	%1, 1b					\n"
  "2:							\n"
  "	dbar 0x700	\n"
  : "=&r" (result), "=&r" (temp), "+ZC" (v->counter)
  : "r" (i));
 }

 return result;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void arch_atomic64_add(long i, atomic64_t *v) { __asm__ __volatile__( "am""add""_db.d " " $zero, %1, %0	\n" : "+ZB" (v->counter) : "r" (i) : "memory"); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) long arch_atomic64_add_return_relaxed(long i, atomic64_t *v) { long result; __asm__ __volatile__( "am""add""_db.d " " %1, %2, %0		\n" : "+ZB" (v->counter), "=&r" (result) : "r" (i) : "memory"); return result + i; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) long arch_atomic64_fetch_add_relaxed(long i, atomic64_t *v) { long result; __asm__ __volatile__( "am""add""_db.d " " %1, %2, %0		\n" : "+ZB" (v->counter), "=&r" (result) : "r" (i) : "memory"); return result; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void arch_atomic64_sub(long i, atomic64_t *v) { __asm__ __volatile__( "am""add""_db.d " " $zero, %1, %0	\n" : "+ZB" (v->counter) : "r" (-i) : "memory"); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) long arch_atomic64_sub_return_relaxed(long i, atomic64_t *v) { long result; __asm__ __volatile__( "am""add""_db.d " " %1, %2, %0		\n" : "+ZB" (v->counter), "=&r" (result) : "r" (-i) : "memory"); return result + -i; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) long arch_atomic64_fetch_sub_relaxed(long i, atomic64_t *v) { long result; __asm__ __volatile__( "am""add""_db.d " " %1, %2, %0		\n" : "+ZB" (v->counter), "=&r" (result) : "r" (-i) : "memory"); return result; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void arch_atomic64_and(long i, atomic64_t *v) { __asm__ __volatile__( "am""and""_db.d " " $zero, %1, %0	\n" : "+ZB" (v->counter) : "r" (i) : "memory"); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) long arch_atomic64_fetch_and_relaxed(long i, atomic64_t *v) { long result; __asm__ __volatile__( "am""and""_db.d " " %1, %2, %0		\n" : "+ZB" (v->counter), "=&r" (result) : "r" (i) : "memory"); return result; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void arch_atomic64_or(long i, atomic64_t *v) { __asm__ __volatile__( "am""or""_db.d " " $zero, %1, %0	\n" : "+ZB" (v->counter) : "r" (i) : "memory"); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) long arch_atomic64_fetch_or_relaxed(long i, atomic64_t *v) { long result; __asm__ __volatile__( "am""or""_db.d " " %1, %2, %0		\n" : "+ZB" (v->counter), "=&r" (result) : "r" (i) : "memory"); return result; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void arch_atomic64_xor(long i, atomic64_t *v) { __asm__ __volatile__( "am""xor""_db.d " " $zero, %1, %0	\n" : "+ZB" (v->counter) : "r" (i) : "memory"); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) long arch_atomic64_fetch_xor_relaxed(long i, atomic64_t *v) { long result; __asm__ __volatile__( "am""xor""_db.d " " %1, %2, %0		\n" : "+ZB" (v->counter), "=&r" (result) : "r" (i) : "memory"); return result; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) long arch_atomic64_fetch_add_unless(atomic64_t *v, long a, long u)
{
       long prev, rc;

 __asm__ __volatile__ (
  "0:	ll.d	%[p],  %[c]\n"
  "	beq	%[p],  %[u], 1f\n"
  "	add.d	%[rc], %[p], %[a]\n"
  "	sc.d	%[rc], %[c]\n"
  "	beqz	%[rc], 0b\n"
  "	b	2f\n"
  "1:\n"
  "	dbar 0x700	\n"
  "2:\n"
  : [p]"=&r" (prev), [rc]"=&r" (rc),
    [c] "=ZB" (v->counter)
  : [a]"r" (a), [u]"r" (u)
  : "memory");

 return prev;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) long arch_atomic64_sub_if_positive(long i, atomic64_t *v)
{
 long result;
 long temp;

 if (__builtin_constant_p(i)) {
  __asm__ __volatile__(
  "1:	ll.d	%1, %2	# atomic64_sub_if_positive	\n"
  "	addi.d	%0, %1, %3				\n"
  "	move	%1, %0					\n"
  "	bltz	%0, 2f					\n"
  "	sc.d	%1, %2					\n"
  "	beqz	%1, 1b					\n"
  "2:							\n"
  "	dbar 0x700	\n"
  : "=&r" (result), "=&r" (temp), "+ZC" (v->counter)
  : "I" (-i));
 } else {
  __asm__ __volatile__(
  "1:	ll.d	%1, %2	# atomic64_sub_if_positive	\n"
  "	sub.d	%0, %1, %3				\n"
  "	move	%1, %0					\n"
  "	bltz	%0, 2f					\n"
  "	sc.d	%1, %2					\n"
  "	beqz	%1, 1b					\n"
  "2:							\n"
  "	dbar 0x700	\n"
  : "=&r" (result), "=&r" (temp), "+ZC" (v->counter)
  : "r" (i));
 }

 return result;
}
extern void raw_cmpxchg128_not_implemented(void);
extern void raw_cmpxchg128_acquire_not_implemented(void);
extern void raw_cmpxchg128_release_not_implemented(void);
extern void raw_cmpxchg128_relaxed_not_implemented(void);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_read(const atomic_t *v)
{
 return ({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_4(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof((v)->counter) == sizeof(char) || sizeof((v)->counter) == sizeof(short) || sizeof((v)->counter) == sizeof(int) || sizeof((v)->counter) == sizeof(long)) || sizeof((v)->counter) == sizeof(long long))) __compiletime_assert_4(); } while (0); (*(const volatile typeof( _Generic(((v)->counter), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: ((v)->counter))) *)&((v)->counter)); });
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_read_acquire(const atomic_t *v)
{



 return ({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_5(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof((v)->counter) == sizeof(char) || sizeof((v)->counter) == sizeof(short) || sizeof((v)->counter) == sizeof(int) || sizeof((v)->counter) == sizeof(long)) || sizeof((v)->counter) == sizeof(long long))) __compiletime_assert_5(); } while (0); (*(const volatile typeof( _Generic(((v)->counter), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: ((v)->counter))) *)&((v)->counter)); });
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
raw_atomic_set(atomic_t *v, int i)
{
 do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_6(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof((v)->counter) == sizeof(char) || sizeof((v)->counter) == sizeof(short) || sizeof((v)->counter) == sizeof(int) || sizeof((v)->counter) == sizeof(long)) || sizeof((v)->counter) == sizeof(long long))) __compiletime_assert_6(); } while (0); do { *(volatile typeof((v)->counter) *)&((v)->counter) = ((i)); } while (0); } while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
raw_atomic_set_release(atomic_t *v, int i)
{



 do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_7(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof((v)->counter) == sizeof(char) || sizeof((v)->counter) == sizeof(short) || sizeof((v)->counter) == sizeof(int) || sizeof((v)->counter) == sizeof(long)) || sizeof((v)->counter) == sizeof(long long))) __compiletime_assert_7(); } while (0); do { *(volatile typeof((v)->counter) *)&((v)->counter) = ((i)); } while (0); } while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
raw_atomic_add(int i, atomic_t *v)
{
 arch_atomic_add(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_add_return(int i, atomic_t *v)
{



 int ret;
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 ret = arch_atomic_add_return_relaxed(i, v);
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return ret;



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_add_return_acquire(int i, atomic_t *v)
{



 int ret = arch_atomic_add_return_relaxed(i, v);
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return ret;





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_add_return_release(int i, atomic_t *v)
{



 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return arch_atomic_add_return_relaxed(i, v);





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_add_return_relaxed(int i, atomic_t *v)
{

 return arch_atomic_add_return_relaxed(i, v);





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_add(int i, atomic_t *v)
{



 int ret;
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 ret = arch_atomic_fetch_add_relaxed(i, v);
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return ret;



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_add_acquire(int i, atomic_t *v)
{



 int ret = arch_atomic_fetch_add_relaxed(i, v);
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return ret;





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_add_release(int i, atomic_t *v)
{



 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return arch_atomic_fetch_add_relaxed(i, v);





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_add_relaxed(int i, atomic_t *v)
{

 return arch_atomic_fetch_add_relaxed(i, v);





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
raw_atomic_sub(int i, atomic_t *v)
{
 arch_atomic_sub(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_sub_return(int i, atomic_t *v)
{



 int ret;
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 ret = arch_atomic_sub_return_relaxed(i, v);
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return ret;



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_sub_return_acquire(int i, atomic_t *v)
{



 int ret = arch_atomic_sub_return_relaxed(i, v);
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return ret;





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_sub_return_release(int i, atomic_t *v)
{



 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return arch_atomic_sub_return_relaxed(i, v);





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_sub_return_relaxed(int i, atomic_t *v)
{

 return arch_atomic_sub_return_relaxed(i, v);





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_sub(int i, atomic_t *v)
{



 int ret;
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 ret = arch_atomic_fetch_sub_relaxed(i, v);
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return ret;



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_sub_acquire(int i, atomic_t *v)
{



 int ret = arch_atomic_fetch_sub_relaxed(i, v);
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return ret;





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_sub_release(int i, atomic_t *v)
{



 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return arch_atomic_fetch_sub_relaxed(i, v);





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_sub_relaxed(int i, atomic_t *v)
{

 return arch_atomic_fetch_sub_relaxed(i, v);





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
raw_atomic_inc(atomic_t *v)
{



 raw_atomic_add(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_inc_return(atomic_t *v)
{
 return raw_atomic_add_return(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_inc_return_acquire(atomic_t *v)
{
 return raw_atomic_add_return_acquire(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_inc_return_release(atomic_t *v)
{
 return raw_atomic_add_return_release(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_inc_return_relaxed(atomic_t *v)
{





 return raw_atomic_add_return_relaxed(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_inc(atomic_t *v)
{
 return raw_atomic_fetch_add(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_inc_acquire(atomic_t *v)
{
 return raw_atomic_fetch_add_acquire(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_inc_release(atomic_t *v)
{
 return raw_atomic_fetch_add_release(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_inc_relaxed(atomic_t *v)
{





 return raw_atomic_fetch_add_relaxed(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
raw_atomic_dec(atomic_t *v)
{



 raw_atomic_sub(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_dec_return(atomic_t *v)
{
 return raw_atomic_sub_return(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_dec_return_acquire(atomic_t *v)
{
 return raw_atomic_sub_return_acquire(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_dec_return_release(atomic_t *v)
{
 return raw_atomic_sub_return_release(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_dec_return_relaxed(atomic_t *v)
{





 return raw_atomic_sub_return_relaxed(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_dec(atomic_t *v)
{
 return raw_atomic_fetch_sub(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_dec_acquire(atomic_t *v)
{
 return raw_atomic_fetch_sub_acquire(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_dec_release(atomic_t *v)
{
 return raw_atomic_fetch_sub_release(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_dec_relaxed(atomic_t *v)
{





 return raw_atomic_fetch_sub_relaxed(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
raw_atomic_and(int i, atomic_t *v)
{
 arch_atomic_and(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_and(int i, atomic_t *v)
{



 int ret;
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 ret = arch_atomic_fetch_and_relaxed(i, v);
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return ret;



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_and_acquire(int i, atomic_t *v)
{



 int ret = arch_atomic_fetch_and_relaxed(i, v);
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return ret;





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_and_release(int i, atomic_t *v)
{



 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return arch_atomic_fetch_and_relaxed(i, v);





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_and_relaxed(int i, atomic_t *v)
{

 return arch_atomic_fetch_and_relaxed(i, v);





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
raw_atomic_andnot(int i, atomic_t *v)
{



 raw_atomic_and(~i, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_andnot(int i, atomic_t *v)
{
 return raw_atomic_fetch_and(~i, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_andnot_acquire(int i, atomic_t *v)
{
 return raw_atomic_fetch_and_acquire(~i, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_andnot_release(int i, atomic_t *v)
{
 return raw_atomic_fetch_and_release(~i, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_andnot_relaxed(int i, atomic_t *v)
{





 return raw_atomic_fetch_and_relaxed(~i, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
raw_atomic_or(int i, atomic_t *v)
{
 arch_atomic_or(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_or(int i, atomic_t *v)
{



 int ret;
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 ret = arch_atomic_fetch_or_relaxed(i, v);
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return ret;



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_or_acquire(int i, atomic_t *v)
{



 int ret = arch_atomic_fetch_or_relaxed(i, v);
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return ret;





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_or_release(int i, atomic_t *v)
{



 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return arch_atomic_fetch_or_relaxed(i, v);





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_or_relaxed(int i, atomic_t *v)
{

 return arch_atomic_fetch_or_relaxed(i, v);





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
raw_atomic_xor(int i, atomic_t *v)
{
 arch_atomic_xor(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_xor(int i, atomic_t *v)
{



 int ret;
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 ret = arch_atomic_fetch_xor_relaxed(i, v);
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return ret;



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_xor_acquire(int i, atomic_t *v)
{



 int ret = arch_atomic_fetch_xor_relaxed(i, v);
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return ret;





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_xor_release(int i, atomic_t *v)
{



 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return arch_atomic_fetch_xor_relaxed(i, v);





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_xor_relaxed(int i, atomic_t *v)
{

 return arch_atomic_fetch_xor_relaxed(i, v);





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_xchg(atomic_t *v, int new)
{
 return ({ __typeof__(*(&v->counter)) __res; __res = (__typeof__(*(&v->counter))) __arch_xchg((&v->counter), (unsigned long)(new), sizeof(*(&v->counter))); __res; });

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_xchg_acquire(atomic_t *v, int new)
{
 return ({ __typeof__(*(&v->counter)) __res; __res = (__typeof__(*(&v->counter))) __arch_xchg((&v->counter), (unsigned long)(new), sizeof(*(&v->counter))); __res; });

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_xchg_release(atomic_t *v, int new)
{
 return ({ __typeof__(*(&v->counter)) __res; __res = (__typeof__(*(&v->counter))) __arch_xchg((&v->counter), (unsigned long)(new), sizeof(*(&v->counter))); __res; });

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_xchg_relaxed(atomic_t *v, int new)
{





 return ({ __typeof__(*(&v->counter)) __res; __res = (__typeof__(*(&v->counter))) __arch_xchg((&v->counter), (unsigned long)(new), sizeof(*(&v->counter))); __res; });

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_cmpxchg(atomic_t *v, int old, int new)
{
 return ({ __typeof__(*(&v->counter)) __res; __res = ((__typeof__(*((&v->counter)))) __cmpxchg(((&v->counter)), (unsigned long)(__typeof__(*((&v->counter))))((old)), (unsigned long)(__typeof__(*((&v->counter))))((new)), sizeof(*((&v->counter))))); __res; });

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_cmpxchg_acquire(atomic_t *v, int old, int new)
{
 return ({ __typeof__(*(&v->counter)) __res; __res = ((__typeof__(*((&v->counter)))) __cmpxchg(((&v->counter)), (unsigned long)(__typeof__(*((&v->counter))))((old)), (unsigned long)(__typeof__(*((&v->counter))))((new)), sizeof(*((&v->counter))))); __res; });

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_cmpxchg_release(atomic_t *v, int old, int new)
{
 return ({ __typeof__(*(&v->counter)) __res; __res = ((__typeof__(*((&v->counter)))) __cmpxchg(((&v->counter)), (unsigned long)(__typeof__(*((&v->counter))))((old)), (unsigned long)(__typeof__(*((&v->counter))))((new)), sizeof(*((&v->counter))))); __res; });

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_cmpxchg_relaxed(atomic_t *v, int old, int new)
{





 return ({ __typeof__(*(&v->counter)) __res; __res = ((__typeof__(*((&v->counter)))) __cmpxchg(((&v->counter)), (unsigned long)(__typeof__(*((&v->counter))))((old)), (unsigned long)(__typeof__(*((&v->counter))))((new)), sizeof(*((&v->counter))))); __res; });

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic_try_cmpxchg(atomic_t *v, int *old, int new)
{
 int r, o = *old;
 r = raw_atomic_cmpxchg(v, o, new);
 if (__builtin_expect(!!(r != o), 0))
  *old = r;
 return __builtin_expect(!!(r == o), 1);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic_try_cmpxchg_acquire(atomic_t *v, int *old, int new)
{
 int r, o = *old;
 r = raw_atomic_cmpxchg_acquire(v, o, new);
 if (__builtin_expect(!!(r != o), 0))
  *old = r;
 return __builtin_expect(!!(r == o), 1);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic_try_cmpxchg_release(atomic_t *v, int *old, int new)
{
 int r, o = *old;
 r = raw_atomic_cmpxchg_release(v, o, new);
 if (__builtin_expect(!!(r != o), 0))
  *old = r;
 return __builtin_expect(!!(r == o), 1);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic_try_cmpxchg_relaxed(atomic_t *v, int *old, int new)
{





 int r, o = *old;
 r = raw_atomic_cmpxchg_relaxed(v, o, new);
 if (__builtin_expect(!!(r != o), 0))
  *old = r;
 return __builtin_expect(!!(r == o), 1);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic_sub_and_test(int i, atomic_t *v)
{



 return raw_atomic_sub_return(i, v) == 0;

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic_dec_and_test(atomic_t *v)
{



 return raw_atomic_dec_return(v) == 0;

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic_inc_and_test(atomic_t *v)
{



 return raw_atomic_inc_return(v) == 0;

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic_add_negative(int i, atomic_t *v)
{
 return raw_atomic_add_return(i, v) < 0;

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic_add_negative_acquire(int i, atomic_t *v)
{
 return raw_atomic_add_return_acquire(i, v) < 0;

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic_add_negative_release(int i, atomic_t *v)
{
 return raw_atomic_add_return_release(i, v) < 0;

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic_add_negative_relaxed(int i, atomic_t *v)
{





 return raw_atomic_add_return_relaxed(i, v) < 0;

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_fetch_add_unless(atomic_t *v, int a, int u)
{

 return arch_atomic_fetch_add_unless(v, a, u);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic_add_unless(atomic_t *v, int a, int u)
{



 return raw_atomic_fetch_add_unless(v, a, u) != u;

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic_inc_not_zero(atomic_t *v)
{



 return raw_atomic_add_unless(v, 1, 0);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic_inc_unless_negative(atomic_t *v)
{



 int c = raw_atomic_read(v);

 do {
  if (__builtin_expect(!!(c < 0), 0))
   return false;
 } while (!raw_atomic_try_cmpxchg(v, &c, c + 1));

 return true;

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic_dec_unless_positive(atomic_t *v)
{



 int c = raw_atomic_read(v);

 do {
  if (__builtin_expect(!!(c > 0), 0))
   return false;
 } while (!raw_atomic_try_cmpxchg(v, &c, c - 1));

 return true;

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_atomic_dec_if_positive(atomic_t *v)
{

 return arch_atomic_sub_if_positive(1, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_read(const atomic64_t *v)
{
 return ({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_8(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof((v)->counter) == sizeof(char) || sizeof((v)->counter) == sizeof(short) || sizeof((v)->counter) == sizeof(int) || sizeof((v)->counter) == sizeof(long)) || sizeof((v)->counter) == sizeof(long long))) __compiletime_assert_8(); } while (0); (*(const volatile typeof( _Generic(((v)->counter), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: ((v)->counter))) *)&((v)->counter)); });
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_read_acquire(const atomic64_t *v)
{



 return ({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_9(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof((v)->counter) == sizeof(char) || sizeof((v)->counter) == sizeof(short) || sizeof((v)->counter) == sizeof(int) || sizeof((v)->counter) == sizeof(long)) || sizeof((v)->counter) == sizeof(long long))) __compiletime_assert_9(); } while (0); (*(const volatile typeof( _Generic(((v)->counter), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: ((v)->counter))) *)&((v)->counter)); });
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
raw_atomic64_set(atomic64_t *v, s64 i)
{
 do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_10(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof((v)->counter) == sizeof(char) || sizeof((v)->counter) == sizeof(short) || sizeof((v)->counter) == sizeof(int) || sizeof((v)->counter) == sizeof(long)) || sizeof((v)->counter) == sizeof(long long))) __compiletime_assert_10(); } while (0); do { *(volatile typeof((v)->counter) *)&((v)->counter) = ((i)); } while (0); } while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
raw_atomic64_set_release(atomic64_t *v, s64 i)
{



 do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_11(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof((v)->counter) == sizeof(char) || sizeof((v)->counter) == sizeof(short) || sizeof((v)->counter) == sizeof(int) || sizeof((v)->counter) == sizeof(long)) || sizeof((v)->counter) == sizeof(long long))) __compiletime_assert_11(); } while (0); do { *(volatile typeof((v)->counter) *)&((v)->counter) = ((i)); } while (0); } while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
raw_atomic64_add(s64 i, atomic64_t *v)
{
 arch_atomic64_add(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_add_return(s64 i, atomic64_t *v)
{



 s64 ret;
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 ret = arch_atomic64_add_return_relaxed(i, v);
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return ret;



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_add_return_acquire(s64 i, atomic64_t *v)
{



 s64 ret = arch_atomic64_add_return_relaxed(i, v);
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return ret;





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_add_return_release(s64 i, atomic64_t *v)
{



 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return arch_atomic64_add_return_relaxed(i, v);





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_add_return_relaxed(s64 i, atomic64_t *v)
{

 return arch_atomic64_add_return_relaxed(i, v);





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_add(s64 i, atomic64_t *v)
{



 s64 ret;
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 ret = arch_atomic64_fetch_add_relaxed(i, v);
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return ret;



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_add_acquire(s64 i, atomic64_t *v)
{



 s64 ret = arch_atomic64_fetch_add_relaxed(i, v);
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return ret;





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_add_release(s64 i, atomic64_t *v)
{



 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return arch_atomic64_fetch_add_relaxed(i, v);





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_add_relaxed(s64 i, atomic64_t *v)
{

 return arch_atomic64_fetch_add_relaxed(i, v);





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
raw_atomic64_sub(s64 i, atomic64_t *v)
{
 arch_atomic64_sub(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_sub_return(s64 i, atomic64_t *v)
{



 s64 ret;
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 ret = arch_atomic64_sub_return_relaxed(i, v);
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return ret;



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_sub_return_acquire(s64 i, atomic64_t *v)
{



 s64 ret = arch_atomic64_sub_return_relaxed(i, v);
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return ret;





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_sub_return_release(s64 i, atomic64_t *v)
{



 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return arch_atomic64_sub_return_relaxed(i, v);





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_sub_return_relaxed(s64 i, atomic64_t *v)
{

 return arch_atomic64_sub_return_relaxed(i, v);





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_sub(s64 i, atomic64_t *v)
{



 s64 ret;
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 ret = arch_atomic64_fetch_sub_relaxed(i, v);
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return ret;



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_sub_acquire(s64 i, atomic64_t *v)
{



 s64 ret = arch_atomic64_fetch_sub_relaxed(i, v);
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return ret;





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_sub_release(s64 i, atomic64_t *v)
{



 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return arch_atomic64_fetch_sub_relaxed(i, v);





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_sub_relaxed(s64 i, atomic64_t *v)
{

 return arch_atomic64_fetch_sub_relaxed(i, v);





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
raw_atomic64_inc(atomic64_t *v)
{



 raw_atomic64_add(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_inc_return(atomic64_t *v)
{
 return raw_atomic64_add_return(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_inc_return_acquire(atomic64_t *v)
{
 return raw_atomic64_add_return_acquire(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_inc_return_release(atomic64_t *v)
{
 return raw_atomic64_add_return_release(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_inc_return_relaxed(atomic64_t *v)
{





 return raw_atomic64_add_return_relaxed(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_inc(atomic64_t *v)
{
 return raw_atomic64_fetch_add(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_inc_acquire(atomic64_t *v)
{
 return raw_atomic64_fetch_add_acquire(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_inc_release(atomic64_t *v)
{
 return raw_atomic64_fetch_add_release(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_inc_relaxed(atomic64_t *v)
{





 return raw_atomic64_fetch_add_relaxed(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
raw_atomic64_dec(atomic64_t *v)
{



 raw_atomic64_sub(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_dec_return(atomic64_t *v)
{
 return raw_atomic64_sub_return(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_dec_return_acquire(atomic64_t *v)
{
 return raw_atomic64_sub_return_acquire(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_dec_return_release(atomic64_t *v)
{
 return raw_atomic64_sub_return_release(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_dec_return_relaxed(atomic64_t *v)
{





 return raw_atomic64_sub_return_relaxed(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_dec(atomic64_t *v)
{
 return raw_atomic64_fetch_sub(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_dec_acquire(atomic64_t *v)
{
 return raw_atomic64_fetch_sub_acquire(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_dec_release(atomic64_t *v)
{
 return raw_atomic64_fetch_sub_release(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_dec_relaxed(atomic64_t *v)
{





 return raw_atomic64_fetch_sub_relaxed(1, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
raw_atomic64_and(s64 i, atomic64_t *v)
{
 arch_atomic64_and(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_and(s64 i, atomic64_t *v)
{



 s64 ret;
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 ret = arch_atomic64_fetch_and_relaxed(i, v);
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return ret;



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_and_acquire(s64 i, atomic64_t *v)
{



 s64 ret = arch_atomic64_fetch_and_relaxed(i, v);
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return ret;





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_and_release(s64 i, atomic64_t *v)
{



 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return arch_atomic64_fetch_and_relaxed(i, v);





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_and_relaxed(s64 i, atomic64_t *v)
{

 return arch_atomic64_fetch_and_relaxed(i, v);





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
raw_atomic64_andnot(s64 i, atomic64_t *v)
{



 raw_atomic64_and(~i, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_andnot(s64 i, atomic64_t *v)
{
 return raw_atomic64_fetch_and(~i, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_andnot_acquire(s64 i, atomic64_t *v)
{
 return raw_atomic64_fetch_and_acquire(~i, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_andnot_release(s64 i, atomic64_t *v)
{
 return raw_atomic64_fetch_and_release(~i, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_andnot_relaxed(s64 i, atomic64_t *v)
{





 return raw_atomic64_fetch_and_relaxed(~i, v);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
raw_atomic64_or(s64 i, atomic64_t *v)
{
 arch_atomic64_or(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_or(s64 i, atomic64_t *v)
{



 s64 ret;
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 ret = arch_atomic64_fetch_or_relaxed(i, v);
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return ret;



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_or_acquire(s64 i, atomic64_t *v)
{



 s64 ret = arch_atomic64_fetch_or_relaxed(i, v);
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return ret;





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_or_release(s64 i, atomic64_t *v)
{



 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return arch_atomic64_fetch_or_relaxed(i, v);





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_or_relaxed(s64 i, atomic64_t *v)
{

 return arch_atomic64_fetch_or_relaxed(i, v);





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
raw_atomic64_xor(s64 i, atomic64_t *v)
{
 arch_atomic64_xor(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_xor(s64 i, atomic64_t *v)
{



 s64 ret;
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 ret = arch_atomic64_fetch_xor_relaxed(i, v);
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return ret;



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_xor_acquire(s64 i, atomic64_t *v)
{



 s64 ret = arch_atomic64_fetch_xor_relaxed(i, v);
 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return ret;





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_xor_release(s64 i, atomic64_t *v)
{



 do { do { } while (0); __asm__ __volatile__("": : :"memory"); } while (0);
 return arch_atomic64_fetch_xor_relaxed(i, v);





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_xor_relaxed(s64 i, atomic64_t *v)
{

 return arch_atomic64_fetch_xor_relaxed(i, v);





}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_xchg(atomic64_t *v, s64 new)
{
 return ({ __typeof__(*(&v->counter)) __res; __res = (__typeof__(*(&v->counter))) __arch_xchg((&v->counter), (unsigned long)(new), sizeof(*(&v->counter))); __res; });

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_xchg_acquire(atomic64_t *v, s64 new)
{
 return ({ __typeof__(*(&v->counter)) __res; __res = (__typeof__(*(&v->counter))) __arch_xchg((&v->counter), (unsigned long)(new), sizeof(*(&v->counter))); __res; });

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_xchg_release(atomic64_t *v, s64 new)
{
 return ({ __typeof__(*(&v->counter)) __res; __res = (__typeof__(*(&v->counter))) __arch_xchg((&v->counter), (unsigned long)(new), sizeof(*(&v->counter))); __res; });

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_xchg_relaxed(atomic64_t *v, s64 new)
{





 return ({ __typeof__(*(&v->counter)) __res; __res = (__typeof__(*(&v->counter))) __arch_xchg((&v->counter), (unsigned long)(new), sizeof(*(&v->counter))); __res; });

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_cmpxchg(atomic64_t *v, s64 old, s64 new)
{
 return ({ __typeof__(*(&v->counter)) __res; __res = ((__typeof__(*((&v->counter)))) __cmpxchg(((&v->counter)), (unsigned long)(__typeof__(*((&v->counter))))((old)), (unsigned long)(__typeof__(*((&v->counter))))((new)), sizeof(*((&v->counter))))); __res; });

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_cmpxchg_acquire(atomic64_t *v, s64 old, s64 new)
{
 return ({ __typeof__(*(&v->counter)) __res; __res = ((__typeof__(*((&v->counter)))) __cmpxchg(((&v->counter)), (unsigned long)(__typeof__(*((&v->counter))))((old)), (unsigned long)(__typeof__(*((&v->counter))))((new)), sizeof(*((&v->counter))))); __res; });

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_cmpxchg_release(atomic64_t *v, s64 old, s64 new)
{
 return ({ __typeof__(*(&v->counter)) __res; __res = ((__typeof__(*((&v->counter)))) __cmpxchg(((&v->counter)), (unsigned long)(__typeof__(*((&v->counter))))((old)), (unsigned long)(__typeof__(*((&v->counter))))((new)), sizeof(*((&v->counter))))); __res; });

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_cmpxchg_relaxed(atomic64_t *v, s64 old, s64 new)
{





 return ({ __typeof__(*(&v->counter)) __res; __res = ((__typeof__(*((&v->counter)))) __cmpxchg(((&v->counter)), (unsigned long)(__typeof__(*((&v->counter))))((old)), (unsigned long)(__typeof__(*((&v->counter))))((new)), sizeof(*((&v->counter))))); __res; });

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic64_try_cmpxchg(atomic64_t *v, s64 *old, s64 new)
{
 s64 r, o = *old;
 r = raw_atomic64_cmpxchg(v, o, new);
 if (__builtin_expect(!!(r != o), 0))
  *old = r;
 return __builtin_expect(!!(r == o), 1);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic64_try_cmpxchg_acquire(atomic64_t *v, s64 *old, s64 new)
{
 s64 r, o = *old;
 r = raw_atomic64_cmpxchg_acquire(v, o, new);
 if (__builtin_expect(!!(r != o), 0))
  *old = r;
 return __builtin_expect(!!(r == o), 1);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic64_try_cmpxchg_release(atomic64_t *v, s64 *old, s64 new)
{
 s64 r, o = *old;
 r = raw_atomic64_cmpxchg_release(v, o, new);
 if (__builtin_expect(!!(r != o), 0))
  *old = r;
 return __builtin_expect(!!(r == o), 1);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic64_try_cmpxchg_relaxed(atomic64_t *v, s64 *old, s64 new)
{





 s64 r, o = *old;
 r = raw_atomic64_cmpxchg_relaxed(v, o, new);
 if (__builtin_expect(!!(r != o), 0))
  *old = r;
 return __builtin_expect(!!(r == o), 1);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic64_sub_and_test(s64 i, atomic64_t *v)
{



 return raw_atomic64_sub_return(i, v) == 0;

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic64_dec_and_test(atomic64_t *v)
{



 return raw_atomic64_dec_return(v) == 0;

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic64_inc_and_test(atomic64_t *v)
{



 return raw_atomic64_inc_return(v) == 0;

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic64_add_negative(s64 i, atomic64_t *v)
{
 return raw_atomic64_add_return(i, v) < 0;

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic64_add_negative_acquire(s64 i, atomic64_t *v)
{
 return raw_atomic64_add_return_acquire(i, v) < 0;

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic64_add_negative_release(s64 i, atomic64_t *v)
{
 return raw_atomic64_add_return_release(i, v) < 0;

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic64_add_negative_relaxed(s64 i, atomic64_t *v)
{





 return raw_atomic64_add_return_relaxed(i, v) < 0;

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_fetch_add_unless(atomic64_t *v, s64 a, s64 u)
{

 return arch_atomic64_fetch_add_unless(v, a, u);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic64_add_unless(atomic64_t *v, s64 a, s64 u)
{



 return raw_atomic64_fetch_add_unless(v, a, u) != u;

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic64_inc_not_zero(atomic64_t *v)
{



 return raw_atomic64_add_unless(v, 1, 0);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic64_inc_unless_negative(atomic64_t *v)
{



 s64 c = raw_atomic64_read(v);

 do {
  if (__builtin_expect(!!(c < 0), 0))
   return false;
 } while (!raw_atomic64_try_cmpxchg(v, &c, c + 1));

 return true;

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic64_dec_unless_positive(atomic64_t *v)
{



 s64 c = raw_atomic64_read(v);

 do {
  if (__builtin_expect(!!(c > 0), 0))
   return false;
 } while (!raw_atomic64_try_cmpxchg(v, &c, c - 1));

 return true;

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
raw_atomic64_dec_if_positive(atomic64_t *v)
{

 return arch_atomic64_sub_if_positive(1, v);
}
typedef atomic64_t atomic_long_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_read(const atomic_long_t *v)
{

 return raw_atomic64_read(v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_read_acquire(const atomic_long_t *v)
{

 return raw_atomic64_read_acquire(v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
raw_atomic_long_set(atomic_long_t *v, long i)
{

 raw_atomic64_set(v, i);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
raw_atomic_long_set_release(atomic_long_t *v, long i)
{

 raw_atomic64_set_release(v, i);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
raw_atomic_long_add(long i, atomic_long_t *v)
{

 raw_atomic64_add(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_add_return(long i, atomic_long_t *v)
{

 return raw_atomic64_add_return(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_add_return_acquire(long i, atomic_long_t *v)
{

 return raw_atomic64_add_return_acquire(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_add_return_release(long i, atomic_long_t *v)
{

 return raw_atomic64_add_return_release(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_add_return_relaxed(long i, atomic_long_t *v)
{

 return raw_atomic64_add_return_relaxed(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_add(long i, atomic_long_t *v)
{

 return raw_atomic64_fetch_add(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_add_acquire(long i, atomic_long_t *v)
{

 return raw_atomic64_fetch_add_acquire(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_add_release(long i, atomic_long_t *v)
{

 return raw_atomic64_fetch_add_release(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_add_relaxed(long i, atomic_long_t *v)
{

 return raw_atomic64_fetch_add_relaxed(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
raw_atomic_long_sub(long i, atomic_long_t *v)
{

 raw_atomic64_sub(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_sub_return(long i, atomic_long_t *v)
{

 return raw_atomic64_sub_return(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_sub_return_acquire(long i, atomic_long_t *v)
{

 return raw_atomic64_sub_return_acquire(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_sub_return_release(long i, atomic_long_t *v)
{

 return raw_atomic64_sub_return_release(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_sub_return_relaxed(long i, atomic_long_t *v)
{

 return raw_atomic64_sub_return_relaxed(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_sub(long i, atomic_long_t *v)
{

 return raw_atomic64_fetch_sub(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_sub_acquire(long i, atomic_long_t *v)
{

 return raw_atomic64_fetch_sub_acquire(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_sub_release(long i, atomic_long_t *v)
{

 return raw_atomic64_fetch_sub_release(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_sub_relaxed(long i, atomic_long_t *v)
{

 return raw_atomic64_fetch_sub_relaxed(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
raw_atomic_long_inc(atomic_long_t *v)
{

 raw_atomic64_inc(v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_inc_return(atomic_long_t *v)
{

 return raw_atomic64_inc_return(v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_inc_return_acquire(atomic_long_t *v)
{

 return raw_atomic64_inc_return_acquire(v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_inc_return_release(atomic_long_t *v)
{

 return raw_atomic64_inc_return_release(v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_inc_return_relaxed(atomic_long_t *v)
{

 return raw_atomic64_inc_return_relaxed(v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_inc(atomic_long_t *v)
{

 return raw_atomic64_fetch_inc(v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_inc_acquire(atomic_long_t *v)
{

 return raw_atomic64_fetch_inc_acquire(v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_inc_release(atomic_long_t *v)
{

 return raw_atomic64_fetch_inc_release(v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_inc_relaxed(atomic_long_t *v)
{

 return raw_atomic64_fetch_inc_relaxed(v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
raw_atomic_long_dec(atomic_long_t *v)
{

 raw_atomic64_dec(v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_dec_return(atomic_long_t *v)
{

 return raw_atomic64_dec_return(v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_dec_return_acquire(atomic_long_t *v)
{

 return raw_atomic64_dec_return_acquire(v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_dec_return_release(atomic_long_t *v)
{

 return raw_atomic64_dec_return_release(v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_dec_return_relaxed(atomic_long_t *v)
{

 return raw_atomic64_dec_return_relaxed(v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_dec(atomic_long_t *v)
{

 return raw_atomic64_fetch_dec(v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_dec_acquire(atomic_long_t *v)
{

 return raw_atomic64_fetch_dec_acquire(v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_dec_release(atomic_long_t *v)
{

 return raw_atomic64_fetch_dec_release(v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_dec_relaxed(atomic_long_t *v)
{

 return raw_atomic64_fetch_dec_relaxed(v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
raw_atomic_long_and(long i, atomic_long_t *v)
{

 raw_atomic64_and(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_and(long i, atomic_long_t *v)
{

 return raw_atomic64_fetch_and(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_and_acquire(long i, atomic_long_t *v)
{

 return raw_atomic64_fetch_and_acquire(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_and_release(long i, atomic_long_t *v)
{

 return raw_atomic64_fetch_and_release(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_and_relaxed(long i, atomic_long_t *v)
{

 return raw_atomic64_fetch_and_relaxed(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
raw_atomic_long_andnot(long i, atomic_long_t *v)
{

 raw_atomic64_andnot(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_andnot(long i, atomic_long_t *v)
{

 return raw_atomic64_fetch_andnot(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_andnot_acquire(long i, atomic_long_t *v)
{

 return raw_atomic64_fetch_andnot_acquire(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_andnot_release(long i, atomic_long_t *v)
{

 return raw_atomic64_fetch_andnot_release(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_andnot_relaxed(long i, atomic_long_t *v)
{

 return raw_atomic64_fetch_andnot_relaxed(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
raw_atomic_long_or(long i, atomic_long_t *v)
{

 raw_atomic64_or(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_or(long i, atomic_long_t *v)
{

 return raw_atomic64_fetch_or(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_or_acquire(long i, atomic_long_t *v)
{

 return raw_atomic64_fetch_or_acquire(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_or_release(long i, atomic_long_t *v)
{

 return raw_atomic64_fetch_or_release(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_or_relaxed(long i, atomic_long_t *v)
{

 return raw_atomic64_fetch_or_relaxed(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
raw_atomic_long_xor(long i, atomic_long_t *v)
{

 raw_atomic64_xor(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_xor(long i, atomic_long_t *v)
{

 return raw_atomic64_fetch_xor(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_xor_acquire(long i, atomic_long_t *v)
{

 return raw_atomic64_fetch_xor_acquire(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_xor_release(long i, atomic_long_t *v)
{

 return raw_atomic64_fetch_xor_release(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_xor_relaxed(long i, atomic_long_t *v)
{

 return raw_atomic64_fetch_xor_relaxed(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_xchg(atomic_long_t *v, long new)
{

 return raw_atomic64_xchg(v, new);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_xchg_acquire(atomic_long_t *v, long new)
{

 return raw_atomic64_xchg_acquire(v, new);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_xchg_release(atomic_long_t *v, long new)
{

 return raw_atomic64_xchg_release(v, new);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_xchg_relaxed(atomic_long_t *v, long new)
{

 return raw_atomic64_xchg_relaxed(v, new);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_cmpxchg(atomic_long_t *v, long old, long new)
{

 return raw_atomic64_cmpxchg(v, old, new);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_cmpxchg_acquire(atomic_long_t *v, long old, long new)
{

 return raw_atomic64_cmpxchg_acquire(v, old, new);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_cmpxchg_release(atomic_long_t *v, long old, long new)
{

 return raw_atomic64_cmpxchg_release(v, old, new);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_cmpxchg_relaxed(atomic_long_t *v, long old, long new)
{

 return raw_atomic64_cmpxchg_relaxed(v, old, new);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic_long_try_cmpxchg(atomic_long_t *v, long *old, long new)
{

 return raw_atomic64_try_cmpxchg(v, (s64 *)old, new);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic_long_try_cmpxchg_acquire(atomic_long_t *v, long *old, long new)
{

 return raw_atomic64_try_cmpxchg_acquire(v, (s64 *)old, new);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic_long_try_cmpxchg_release(atomic_long_t *v, long *old, long new)
{

 return raw_atomic64_try_cmpxchg_release(v, (s64 *)old, new);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic_long_try_cmpxchg_relaxed(atomic_long_t *v, long *old, long new)
{

 return raw_atomic64_try_cmpxchg_relaxed(v, (s64 *)old, new);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic_long_sub_and_test(long i, atomic_long_t *v)
{

 return raw_atomic64_sub_and_test(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic_long_dec_and_test(atomic_long_t *v)
{

 return raw_atomic64_dec_and_test(v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic_long_inc_and_test(atomic_long_t *v)
{

 return raw_atomic64_inc_and_test(v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic_long_add_negative(long i, atomic_long_t *v)
{

 return raw_atomic64_add_negative(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic_long_add_negative_acquire(long i, atomic_long_t *v)
{

 return raw_atomic64_add_negative_acquire(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic_long_add_negative_release(long i, atomic_long_t *v)
{

 return raw_atomic64_add_negative_release(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic_long_add_negative_relaxed(long i, atomic_long_t *v)
{

 return raw_atomic64_add_negative_relaxed(i, v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_fetch_add_unless(atomic_long_t *v, long a, long u)
{

 return raw_atomic64_fetch_add_unless(v, a, u);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic_long_add_unless(atomic_long_t *v, long a, long u)
{

 return raw_atomic64_add_unless(v, a, u);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic_long_inc_not_zero(atomic_long_t *v)
{

 return raw_atomic64_inc_not_zero(v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic_long_inc_unless_negative(atomic_long_t *v)
{

 return raw_atomic64_inc_unless_negative(v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
raw_atomic_long_dec_unless_positive(atomic_long_t *v)
{

 return raw_atomic64_dec_unless_positive(v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
raw_atomic_long_dec_if_positive(atomic_long_t *v)
{

 return raw_atomic64_dec_if_positive(v);



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kmsan_poison_memory(const void *address, size_t size,
           gfp_t flags)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kmsan_unpoison_memory(const void *address, size_t size)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kmsan_check_memory(const void *address, size_t size)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kmsan_copy_to_user(void *to, const void *from,
          size_t to_copy, size_t left)
{
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void instrument_read(const volatile void *v, size_t size)
{
 kasan_check_read(v, size);
 kcsan_check_access(v, size, 0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void instrument_write(const volatile void *v, size_t size)
{
 kasan_check_write(v, size);
 kcsan_check_access(v, size, (1 << 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void instrument_read_write(const volatile void *v, size_t size)
{
 kasan_check_write(v, size);
 kcsan_check_access(v, size, (1 << 1) | (1 << 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void instrument_atomic_read(const volatile void *v, size_t size)
{
 kasan_check_read(v, size);
 kcsan_check_access(v, size, (1 << 2));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void instrument_atomic_write(const volatile void *v, size_t size)
{
 kasan_check_write(v, size);
 kcsan_check_access(v, size, (1 << 2) | (1 << 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void instrument_atomic_read_write(const volatile void *v, size_t size)
{
 kasan_check_write(v, size);
 kcsan_check_access(v, size, (1 << 2) | (1 << 0) | (1 << 1));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
instrument_copy_to_user(void *to, const void *from, unsigned long n)
{
 kasan_check_read(from, n);
 kcsan_check_access(from, n, 0);
 kmsan_copy_to_user(to, from, n, 0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
instrument_copy_from_user_before(const void *to, const void *from, unsigned long n)
{
 kasan_check_write(to, n);
 kcsan_check_access(to, n, (1 << 0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
instrument_copy_from_user_after(const void *to, const void *from,
    unsigned long n, unsigned long left)
{
 kmsan_unpoison_memory(to, n - left);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_read(const atomic_t *v)
{
 instrument_atomic_read(v, sizeof(*v));
 return raw_atomic_read(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_read_acquire(const atomic_t *v)
{
 instrument_atomic_read(v, sizeof(*v));
 return raw_atomic_read_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
atomic_set(atomic_t *v, int i)
{
 instrument_atomic_write(v, sizeof(*v));
 raw_atomic_set(v, i);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
atomic_set_release(atomic_t *v, int i)
{
 do { } while (0);
 instrument_atomic_write(v, sizeof(*v));
 raw_atomic_set_release(v, i);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
atomic_add(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 raw_atomic_add(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_add_return(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_add_return(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_add_return_acquire(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_add_return_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_add_return_release(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_add_return_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_add_return_relaxed(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_add_return_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_add(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_add(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_add_acquire(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_add_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_add_release(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_add_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_add_relaxed(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_add_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
atomic_sub(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 raw_atomic_sub(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_sub_return(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_sub_return(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_sub_return_acquire(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_sub_return_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_sub_return_release(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_sub_return_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_sub_return_relaxed(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_sub_return_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_sub(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_sub(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_sub_acquire(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_sub_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_sub_release(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_sub_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_sub_relaxed(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_sub_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
atomic_inc(atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 raw_atomic_inc(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_inc_return(atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_inc_return(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_inc_return_acquire(atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_inc_return_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_inc_return_release(atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_inc_return_release(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_inc_return_relaxed(atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_inc_return_relaxed(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_inc(atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_inc(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_inc_acquire(atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_inc_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_inc_release(atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_inc_release(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_inc_relaxed(atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_inc_relaxed(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
atomic_dec(atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 raw_atomic_dec(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_dec_return(atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_dec_return(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_dec_return_acquire(atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_dec_return_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_dec_return_release(atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_dec_return_release(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_dec_return_relaxed(atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_dec_return_relaxed(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_dec(atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_dec(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_dec_acquire(atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_dec_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_dec_release(atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_dec_release(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_dec_relaxed(atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_dec_relaxed(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
atomic_and(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 raw_atomic_and(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_and(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_and(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_and_acquire(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_and_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_and_release(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_and_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_and_relaxed(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_and_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
atomic_andnot(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 raw_atomic_andnot(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_andnot(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_andnot(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_andnot_acquire(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_andnot_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_andnot_release(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_andnot_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_andnot_relaxed(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_andnot_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
atomic_or(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 raw_atomic_or(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_or(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_or(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_or_acquire(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_or_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_or_release(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_or_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_or_relaxed(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_or_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
atomic_xor(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 raw_atomic_xor(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_xor(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_xor(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_xor_acquire(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_xor_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_xor_release(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_xor_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_xor_relaxed(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_xor_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_xchg(atomic_t *v, int new)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_xchg(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_xchg_acquire(atomic_t *v, int new)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_xchg_acquire(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_xchg_release(atomic_t *v, int new)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_xchg_release(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_xchg_relaxed(atomic_t *v, int new)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_xchg_relaxed(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_cmpxchg(atomic_t *v, int old, int new)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_cmpxchg(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_cmpxchg_acquire(atomic_t *v, int old, int new)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_cmpxchg_acquire(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_cmpxchg_release(atomic_t *v, int old, int new)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_cmpxchg_release(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_cmpxchg_relaxed(atomic_t *v, int old, int new)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_cmpxchg_relaxed(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic_try_cmpxchg(atomic_t *v, int *old, int new)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 instrument_atomic_read_write(old, sizeof(*old));
 return raw_atomic_try_cmpxchg(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic_try_cmpxchg_acquire(atomic_t *v, int *old, int new)
{
 instrument_atomic_read_write(v, sizeof(*v));
 instrument_atomic_read_write(old, sizeof(*old));
 return raw_atomic_try_cmpxchg_acquire(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic_try_cmpxchg_release(atomic_t *v, int *old, int new)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 instrument_atomic_read_write(old, sizeof(*old));
 return raw_atomic_try_cmpxchg_release(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic_try_cmpxchg_relaxed(atomic_t *v, int *old, int new)
{
 instrument_atomic_read_write(v, sizeof(*v));
 instrument_atomic_read_write(old, sizeof(*old));
 return raw_atomic_try_cmpxchg_relaxed(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic_sub_and_test(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_sub_and_test(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic_dec_and_test(atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_dec_and_test(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic_inc_and_test(atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_inc_and_test(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic_add_negative(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_add_negative(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic_add_negative_acquire(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_add_negative_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic_add_negative_release(int i, atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_add_negative_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic_add_negative_relaxed(int i, atomic_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_add_negative_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_fetch_add_unless(atomic_t *v, int a, int u)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_fetch_add_unless(v, a, u);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic_add_unless(atomic_t *v, int a, int u)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_add_unless(v, a, u);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic_inc_not_zero(atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_inc_not_zero(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic_inc_unless_negative(atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_inc_unless_negative(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic_dec_unless_positive(atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_dec_unless_positive(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
atomic_dec_if_positive(atomic_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_dec_if_positive(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_read(const atomic64_t *v)
{
 instrument_atomic_read(v, sizeof(*v));
 return raw_atomic64_read(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_read_acquire(const atomic64_t *v)
{
 instrument_atomic_read(v, sizeof(*v));
 return raw_atomic64_read_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
atomic64_set(atomic64_t *v, s64 i)
{
 instrument_atomic_write(v, sizeof(*v));
 raw_atomic64_set(v, i);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
atomic64_set_release(atomic64_t *v, s64 i)
{
 do { } while (0);
 instrument_atomic_write(v, sizeof(*v));
 raw_atomic64_set_release(v, i);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
atomic64_add(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 raw_atomic64_add(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_add_return(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_add_return(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_add_return_acquire(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_add_return_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_add_return_release(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_add_return_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_add_return_relaxed(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_add_return_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_add(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_add(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_add_acquire(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_add_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_add_release(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_add_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_add_relaxed(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_add_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
atomic64_sub(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 raw_atomic64_sub(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_sub_return(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_sub_return(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_sub_return_acquire(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_sub_return_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_sub_return_release(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_sub_return_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_sub_return_relaxed(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_sub_return_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_sub(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_sub(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_sub_acquire(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_sub_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_sub_release(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_sub_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_sub_relaxed(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_sub_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
atomic64_inc(atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 raw_atomic64_inc(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_inc_return(atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_inc_return(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_inc_return_acquire(atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_inc_return_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_inc_return_release(atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_inc_return_release(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_inc_return_relaxed(atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_inc_return_relaxed(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_inc(atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_inc(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_inc_acquire(atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_inc_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_inc_release(atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_inc_release(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_inc_relaxed(atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_inc_relaxed(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
atomic64_dec(atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 raw_atomic64_dec(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_dec_return(atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_dec_return(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_dec_return_acquire(atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_dec_return_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_dec_return_release(atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_dec_return_release(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_dec_return_relaxed(atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_dec_return_relaxed(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_dec(atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_dec(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_dec_acquire(atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_dec_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_dec_release(atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_dec_release(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_dec_relaxed(atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_dec_relaxed(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
atomic64_and(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 raw_atomic64_and(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_and(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_and(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_and_acquire(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_and_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_and_release(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_and_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_and_relaxed(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_and_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
atomic64_andnot(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 raw_atomic64_andnot(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_andnot(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_andnot(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_andnot_acquire(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_andnot_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_andnot_release(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_andnot_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_andnot_relaxed(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_andnot_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
atomic64_or(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 raw_atomic64_or(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_or(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_or(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_or_acquire(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_or_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_or_release(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_or_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_or_relaxed(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_or_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
atomic64_xor(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 raw_atomic64_xor(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_xor(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_xor(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_xor_acquire(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_xor_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_xor_release(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_xor_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_xor_relaxed(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_xor_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_xchg(atomic64_t *v, s64 new)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_xchg(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_xchg_acquire(atomic64_t *v, s64 new)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_xchg_acquire(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_xchg_release(atomic64_t *v, s64 new)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_xchg_release(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_xchg_relaxed(atomic64_t *v, s64 new)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_xchg_relaxed(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_cmpxchg(atomic64_t *v, s64 old, s64 new)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_cmpxchg(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_cmpxchg_acquire(atomic64_t *v, s64 old, s64 new)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_cmpxchg_acquire(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_cmpxchg_release(atomic64_t *v, s64 old, s64 new)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_cmpxchg_release(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_cmpxchg_relaxed(atomic64_t *v, s64 old, s64 new)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_cmpxchg_relaxed(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic64_try_cmpxchg(atomic64_t *v, s64 *old, s64 new)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 instrument_atomic_read_write(old, sizeof(*old));
 return raw_atomic64_try_cmpxchg(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic64_try_cmpxchg_acquire(atomic64_t *v, s64 *old, s64 new)
{
 instrument_atomic_read_write(v, sizeof(*v));
 instrument_atomic_read_write(old, sizeof(*old));
 return raw_atomic64_try_cmpxchg_acquire(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic64_try_cmpxchg_release(atomic64_t *v, s64 *old, s64 new)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 instrument_atomic_read_write(old, sizeof(*old));
 return raw_atomic64_try_cmpxchg_release(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic64_try_cmpxchg_relaxed(atomic64_t *v, s64 *old, s64 new)
{
 instrument_atomic_read_write(v, sizeof(*v));
 instrument_atomic_read_write(old, sizeof(*old));
 return raw_atomic64_try_cmpxchg_relaxed(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic64_sub_and_test(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_sub_and_test(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic64_dec_and_test(atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_dec_and_test(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic64_inc_and_test(atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_inc_and_test(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic64_add_negative(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_add_negative(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic64_add_negative_acquire(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_add_negative_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic64_add_negative_release(s64 i, atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_add_negative_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic64_add_negative_relaxed(s64 i, atomic64_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_add_negative_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_fetch_add_unless(atomic64_t *v, s64 a, s64 u)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_fetch_add_unless(v, a, u);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic64_add_unless(atomic64_t *v, s64 a, s64 u)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_add_unless(v, a, u);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic64_inc_not_zero(atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_inc_not_zero(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic64_inc_unless_negative(atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_inc_unless_negative(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic64_dec_unless_positive(atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_dec_unless_positive(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) s64
atomic64_dec_if_positive(atomic64_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic64_dec_if_positive(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_read(const atomic_long_t *v)
{
 instrument_atomic_read(v, sizeof(*v));
 return raw_atomic_long_read(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_read_acquire(const atomic_long_t *v)
{
 instrument_atomic_read(v, sizeof(*v));
 return raw_atomic_long_read_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
atomic_long_set(atomic_long_t *v, long i)
{
 instrument_atomic_write(v, sizeof(*v));
 raw_atomic_long_set(v, i);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
atomic_long_set_release(atomic_long_t *v, long i)
{
 do { } while (0);
 instrument_atomic_write(v, sizeof(*v));
 raw_atomic_long_set_release(v, i);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
atomic_long_add(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 raw_atomic_long_add(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_add_return(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_add_return(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_add_return_acquire(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_add_return_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_add_return_release(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_add_return_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_add_return_relaxed(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_add_return_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_add(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_add(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_add_acquire(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_add_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_add_release(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_add_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_add_relaxed(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_add_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
atomic_long_sub(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 raw_atomic_long_sub(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_sub_return(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_sub_return(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_sub_return_acquire(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_sub_return_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_sub_return_release(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_sub_return_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_sub_return_relaxed(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_sub_return_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_sub(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_sub(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_sub_acquire(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_sub_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_sub_release(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_sub_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_sub_relaxed(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_sub_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
atomic_long_inc(atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 raw_atomic_long_inc(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_inc_return(atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_inc_return(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_inc_return_acquire(atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_inc_return_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_inc_return_release(atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_inc_return_release(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_inc_return_relaxed(atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_inc_return_relaxed(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_inc(atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_inc(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_inc_acquire(atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_inc_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_inc_release(atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_inc_release(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_inc_relaxed(atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_inc_relaxed(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
atomic_long_dec(atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 raw_atomic_long_dec(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_dec_return(atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_dec_return(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_dec_return_acquire(atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_dec_return_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_dec_return_release(atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_dec_return_release(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_dec_return_relaxed(atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_dec_return_relaxed(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_dec(atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_dec(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_dec_acquire(atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_dec_acquire(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_dec_release(atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_dec_release(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_dec_relaxed(atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_dec_relaxed(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
atomic_long_and(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 raw_atomic_long_and(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_and(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_and(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_and_acquire(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_and_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_and_release(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_and_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_and_relaxed(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_and_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
atomic_long_andnot(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 raw_atomic_long_andnot(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_andnot(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_andnot(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_andnot_acquire(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_andnot_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_andnot_release(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_andnot_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_andnot_relaxed(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_andnot_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
atomic_long_or(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 raw_atomic_long_or(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_or(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_or(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_or_acquire(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_or_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_or_release(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_or_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_or_relaxed(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_or_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
atomic_long_xor(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 raw_atomic_long_xor(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_xor(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_xor(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_xor_acquire(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_xor_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_xor_release(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_xor_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_xor_relaxed(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_xor_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_xchg(atomic_long_t *v, long new)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_xchg(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_xchg_acquire(atomic_long_t *v, long new)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_xchg_acquire(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_xchg_release(atomic_long_t *v, long new)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_xchg_release(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_xchg_relaxed(atomic_long_t *v, long new)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_xchg_relaxed(v, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_cmpxchg(atomic_long_t *v, long old, long new)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_cmpxchg(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_cmpxchg_acquire(atomic_long_t *v, long old, long new)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_cmpxchg_acquire(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_cmpxchg_release(atomic_long_t *v, long old, long new)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_cmpxchg_release(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_cmpxchg_relaxed(atomic_long_t *v, long old, long new)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_cmpxchg_relaxed(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic_long_try_cmpxchg(atomic_long_t *v, long *old, long new)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 instrument_atomic_read_write(old, sizeof(*old));
 return raw_atomic_long_try_cmpxchg(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic_long_try_cmpxchg_acquire(atomic_long_t *v, long *old, long new)
{
 instrument_atomic_read_write(v, sizeof(*v));
 instrument_atomic_read_write(old, sizeof(*old));
 return raw_atomic_long_try_cmpxchg_acquire(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic_long_try_cmpxchg_release(atomic_long_t *v, long *old, long new)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 instrument_atomic_read_write(old, sizeof(*old));
 return raw_atomic_long_try_cmpxchg_release(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic_long_try_cmpxchg_relaxed(atomic_long_t *v, long *old, long new)
{
 instrument_atomic_read_write(v, sizeof(*v));
 instrument_atomic_read_write(old, sizeof(*old));
 return raw_atomic_long_try_cmpxchg_relaxed(v, old, new);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic_long_sub_and_test(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_sub_and_test(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic_long_dec_and_test(atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_dec_and_test(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic_long_inc_and_test(atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_inc_and_test(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic_long_add_negative(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_add_negative(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic_long_add_negative_acquire(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_add_negative_acquire(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic_long_add_negative_release(long i, atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_add_negative_release(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic_long_add_negative_relaxed(long i, atomic_long_t *v)
{
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_add_negative_relaxed(i, v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_fetch_add_unless(atomic_long_t *v, long a, long u)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_fetch_add_unless(v, a, u);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic_long_add_unless(atomic_long_t *v, long a, long u)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_add_unless(v, a, u);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic_long_inc_not_zero(atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_inc_not_zero(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic_long_inc_unless_negative(atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_inc_unless_negative(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool
atomic_long_dec_unless_positive(atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_dec_unless_positive(v);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) long
atomic_long_dec_if_positive(atomic_long_t *v)
{
 do { } while (0);
 instrument_atomic_read_write(v, sizeof(*v));
 return raw_atomic_long_dec_if_positive(v);
}








static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
arch_set_bit(unsigned int nr, volatile unsigned long *p)
{
 p += ((nr) / 64);
 raw_atomic_long_or(((((1UL))) << ((nr) % 64)), (atomic_long_t *)p);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
arch_clear_bit(unsigned int nr, volatile unsigned long *p)
{
 p += ((nr) / 64);
 raw_atomic_long_andnot(((((1UL))) << ((nr) % 64)), (atomic_long_t *)p);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
arch_change_bit(unsigned int nr, volatile unsigned long *p)
{
 p += ((nr) / 64);
 raw_atomic_long_xor(((((1UL))) << ((nr) % 64)), (atomic_long_t *)p);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
arch_test_and_set_bit(unsigned int nr, volatile unsigned long *p)
{
 long old;
 unsigned long mask = ((((1UL))) << ((nr) % 64));

 p += ((nr) / 64);
 old = raw_atomic_long_fetch_or(mask, (atomic_long_t *)p);
 return !!(old & mask);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
arch_test_and_clear_bit(unsigned int nr, volatile unsigned long *p)
{
 long old;
 unsigned long mask = ((((1UL))) << ((nr) % 64));

 p += ((nr) / 64);
 old = raw_atomic_long_fetch_andnot(mask, (atomic_long_t *)p);
 return !!(old & mask);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
arch_test_and_change_bit(unsigned int nr, volatile unsigned long *p)
{
 long old;
 unsigned long mask = ((((1UL))) << ((nr) % 64));

 p += ((nr) / 64);
 old = raw_atomic_long_fetch_xor(mask, (atomic_long_t *)p);
 return !!(old & mask);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void set_bit(long nr, volatile unsigned long *addr)
{
 instrument_atomic_write(addr + ((nr) / 64), sizeof(long));
 arch_set_bit(nr, addr);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void clear_bit(long nr, volatile unsigned long *addr)
{
 instrument_atomic_write(addr + ((nr) / 64), sizeof(long));
 arch_clear_bit(nr, addr);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void change_bit(long nr, volatile unsigned long *addr)
{
 instrument_atomic_write(addr + ((nr) / 64), sizeof(long));
 arch_change_bit(nr, addr);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool test_and_set_bit(long nr, volatile unsigned long *addr)
{
 do { } while (0);
 instrument_atomic_read_write(addr + ((nr) / 64), sizeof(long));
 return arch_test_and_set_bit(nr, addr);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool test_and_clear_bit(long nr, volatile unsigned long *addr)
{
 do { } while (0);
 instrument_atomic_read_write(addr + ((nr) / 64), sizeof(long));
 return arch_test_and_clear_bit(nr, addr);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool test_and_change_bit(long nr, volatile unsigned long *addr)
{
 do { } while (0);
 instrument_atomic_read_write(addr + ((nr) / 64), sizeof(long));
 return arch_test_and_change_bit(nr, addr);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
arch_test_and_set_bit_lock(unsigned int nr, volatile unsigned long *p)
{
 long old;
 unsigned long mask = ((((1UL))) << ((nr) % 64));

 p += ((nr) / 64);
 if (({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_12(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*p) == sizeof(char) || sizeof(*p) == sizeof(short) || sizeof(*p) == sizeof(int) || sizeof(*p) == sizeof(long)) || sizeof(*p) == sizeof(long long))) __compiletime_assert_12(); } while (0); (*(const volatile typeof( _Generic((*p), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*p))) *)&(*p)); }) & mask)
  return 1;

 old = raw_atomic_long_fetch_or_acquire(mask, (atomic_long_t *)p);
 return !!(old & mask);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
arch_clear_bit_unlock(unsigned int nr, volatile unsigned long *p)
{
 p += ((nr) / 64);
 raw_atomic_long_fetch_andnot_release(((((1UL))) << ((nr) % 64)), (atomic_long_t *)p);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void
arch___clear_bit_unlock(unsigned int nr, volatile unsigned long *p)
{
 unsigned long old;

 p += ((nr) / 64);
 old = ({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_13(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*p) == sizeof(char) || sizeof(*p) == sizeof(short) || sizeof(*p) == sizeof(int) || sizeof(*p) == sizeof(long)) || sizeof(*p) == sizeof(long long))) __compiletime_assert_13(); } while (0); (*(const volatile typeof( _Generic((*p), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*p))) *)&(*p)); });
 old &= ~((((1UL))) << ((nr) % 64));
 raw_atomic_long_set_release((atomic_long_t *)p, old);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool arch_clear_bit_unlock_is_negative_byte(unsigned int nr,
         volatile unsigned long *p)
{
 long old;
 unsigned long mask = ((((1UL))) << ((nr) % 64));

 p += ((nr) / 64);
 old = raw_atomic_long_fetch_andnot_release(mask, (atomic_long_t *)p);
 return !!(old & ((((1UL))) << (7)));
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void clear_bit_unlock(long nr, volatile unsigned long *addr)
{
 do { } while (0);
 instrument_atomic_write(addr + ((nr) / 64), sizeof(long));
 arch_clear_bit_unlock(nr, addr);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __clear_bit_unlock(long nr, volatile unsigned long *addr)
{
 do { } while (0);
 instrument_write(addr + ((nr) / 64), sizeof(long));
 arch___clear_bit_unlock(nr, addr);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool test_and_set_bit_lock(long nr, volatile unsigned long *addr)
{
 instrument_atomic_read_write(addr + ((nr) / 64), sizeof(long));
 return arch_test_and_set_bit_lock(nr, addr);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool
clear_bit_unlock_is_negative_byte(long nr, volatile unsigned long *addr)
{
 do { } while (0);
 instrument_atomic_write(addr + ((nr) / 64), sizeof(long));
 return arch_clear_bit_unlock_is_negative_byte(nr, addr);
}























static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__const__)) __u16 __fswab16(__u16 val)
{



 return ((__u16)( (((__u16)(val) & (__u16)0x00ffU) << 8) | (((__u16)(val) & (__u16)0xff00U) >> 8)));

}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__const__)) __u32 __fswab32(__u32 val)
{



 return ((__u32)( (((__u32)(val) & (__u32)0x000000ffUL) << 24) | (((__u32)(val) & (__u32)0x0000ff00UL) << 8) | (((__u32)(val) & (__u32)0x00ff0000UL) >> 8) | (((__u32)(val) & (__u32)0xff000000UL) >> 24)));

}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__const__)) __u64 __fswab64(__u64 val)
{







 return ((__u64)( (((__u64)(val) & (__u64)0x00000000000000ffULL) << 56) | (((__u64)(val) & (__u64)0x000000000000ff00ULL) << 40) | (((__u64)(val) & (__u64)0x0000000000ff0000ULL) << 24) | (((__u64)(val) & (__u64)0x00000000ff000000ULL) << 8) | (((__u64)(val) & (__u64)0x000000ff00000000ULL) >> 8) | (((__u64)(val) & (__u64)0x0000ff0000000000ULL) >> 24) | (((__u64)(val) & (__u64)0x00ff000000000000ULL) >> 40) | (((__u64)(val) & (__u64)0xff00000000000000ULL) >> 56)));

}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__const__)) __u32 __fswahw32(__u32 val)
{



 return ((__u32)( (((__u32)(val) & (__u32)0x0000ffffUL) << 16) | (((__u32)(val) & (__u32)0xffff0000UL) >> 16)));

}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__const__)) __u32 __fswahb32(__u32 val)
{



 return ((__u32)( (((__u32)(val) & (__u32)0x00ff00ffUL) << 8) | (((__u32)(val) & (__u32)0xff00ff00UL) >> 8)));

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) unsigned long __swab(const unsigned long y)
{

 return (__u64)__builtin_bswap64((__u64)(y));



}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __u16 __swab16p(const __u16 *p)
{



 return (__u16)__builtin_bswap16((__u16)(*p));

}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __u32 __swab32p(const __u32 *p)
{



 return (__u32)__builtin_bswap32((__u32)(*p));

}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __u64 __swab64p(const __u64 *p)
{



 return (__u64)__builtin_bswap64((__u64)(*p));

}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __u32 __swahw32p(const __u32 *p)
{



 return (__builtin_constant_p((__u32)(*p)) ? ((__u32)( (((__u32)(*p) & (__u32)0x0000ffffUL) << 16) | (((__u32)(*p) & (__u32)0xffff0000UL) >> 16))) : __fswahw32(*p));

}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __u32 __swahb32p(const __u32 *p)
{



 return (__builtin_constant_p((__u32)(*p)) ? ((__u32)( (((__u32)(*p) & (__u32)0x00ff00ffUL) << 8) | (((__u32)(*p) & (__u32)0xff00ff00UL) >> 8))) : __fswahb32(*p));

}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __swab16s(__u16 *p)
{



 *p = __swab16p(p);

}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __swab32s(__u32 *p)
{



 *p = __swab32p(p);

}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __swab64s(__u64 *p)
{



 *p = __swab64p(p);

}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __swahw32s(__u32 *p)
{



 *p = __swahw32p(p);

}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __swahb32s(__u32 *p)
{



 *p = __swahb32p(p);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void swab16_array(u16 *buf, unsigned int words)
{
 while (words--) {
  __swab16s(buf);
  buf++;
 }
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void swab32_array(u32 *buf, unsigned int words)
{
 while (words--) {
  __swab32s(buf);
  buf++;
 }
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void swab64_array(u64 *buf, unsigned int words)
{
 while (words--) {
  __swab64s(buf);
  buf++;
 }
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __le64 __cpu_to_le64p(const __u64 *p)
{
 return ( __le64)*p;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __u64 __le64_to_cpup(const __le64 *p)
{
 return ( __u64)*p;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __le32 __cpu_to_le32p(const __u32 *p)
{
 return ( __le32)*p;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __u32 __le32_to_cpup(const __le32 *p)
{
 return ( __u32)*p;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __le16 __cpu_to_le16p(const __u16 *p)
{
 return ( __le16)*p;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __u16 __le16_to_cpup(const __le16 *p)
{
 return ( __u16)*p;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __be64 __cpu_to_be64p(const __u64 *p)
{
 return ( __be64)__swab64p(p);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __u64 __be64_to_cpup(const __be64 *p)
{
 return __swab64p((__u64 *)p);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __be32 __cpu_to_be32p(const __u32 *p)
{
 return ( __be32)__swab32p(p);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __u32 __be32_to_cpup(const __be32 *p)
{
 return __swab32p((__u32 *)p);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __be16 __cpu_to_be16p(const __u16 *p)
{
 return ( __be16)__swab16p(p);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __u16 __be16_to_cpup(const __be16 *p)
{
 return __swab16p((__u16 *)p);
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void le16_add_cpu(__le16 *var, u16 val)
{
 *var = (( __le16)(__u16)((( __u16)(__le16)(*var)) + val));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void le32_add_cpu(__le32 *var, u32 val)
{
 *var = (( __le32)(__u32)((( __u32)(__le32)(*var)) + val));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void le64_add_cpu(__le64 *var, u64 val)
{
 *var = (( __le64)(__u64)((( __u64)(__le64)(*var)) + val));
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void le32_to_cpu_array(u32 *buf, unsigned int words)
{
 while (words--) {
  do { (void)(buf); } while (0);
  buf++;
 }
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void cpu_to_le32_array(u32 *buf, unsigned int words)
{
 while (words--) {
  do { (void)(buf); } while (0);
  buf++;
 }
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void be16_add_cpu(__be16 *var, u16 val)
{
 *var = (( __be16)(__u16)__builtin_bswap16((__u16)(((__u16)__builtin_bswap16((__u16)(( __u16)(__be16)(*var))) + val))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void be32_add_cpu(__be32 *var, u32 val)
{
 *var = (( __be32)(__u32)__builtin_bswap32((__u32)(((__u32)__builtin_bswap32((__u32)(( __u32)(__be32)(*var))) + val))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void be64_add_cpu(__be64 *var, u64 val)
{
 *var = (( __be64)(__u64)__builtin_bswap64((__u64)(((__u64)__builtin_bswap64((__u64)(( __u64)(__be64)(*var))) + val))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void cpu_to_be32_array(__be32 *dst, const u32 *src, size_t len)
{
 size_t i;

 for (i = 0; i < len; i++)
  dst[i] = (( __be32)(__u32)__builtin_bswap32((__u32)((src[i]))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void be32_to_cpu_array(u32 *dst, const __be32 *src, size_t len)
{
 size_t i;

 for (i = 0; i < len; i++)
  dst[i] = (__u32)__builtin_bswap32((__u32)(( __u32)(__be32)(src[i])));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int test_bit_le(int nr, const void *addr)
{
 return ((__builtin_constant_p(nr ^ 0) && __builtin_constant_p((uintptr_t)(addr) != (uintptr_t)((void *)0)) && (uintptr_t)(addr) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(addr))) ? const_test_bit(nr ^ 0, addr) : generic_test_bit(nr ^ 0, addr));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void set_bit_le(int nr, void *addr)
{
 set_bit(nr ^ 0, addr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void clear_bit_le(int nr, void *addr)
{
 clear_bit(nr ^ 0, addr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __set_bit_le(int nr, void *addr)
{
 ((__builtin_constant_p(nr ^ 0) && __builtin_constant_p((uintptr_t)(addr) != (uintptr_t)((void *)0)) && (uintptr_t)(addr) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(addr))) ? generic___set_bit(nr ^ 0, addr) : generic___set_bit(nr ^ 0, addr));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __clear_bit_le(int nr, void *addr)
{
 ((__builtin_constant_p(nr ^ 0) && __builtin_constant_p((uintptr_t)(addr) != (uintptr_t)((void *)0)) && (uintptr_t)(addr) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(addr))) ? generic___clear_bit(nr ^ 0, addr) : generic___clear_bit(nr ^ 0, addr));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int test_and_set_bit_le(int nr, void *addr)
{
 return test_and_set_bit(nr ^ 0, addr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int test_and_clear_bit_le(int nr, void *addr)
{
 return test_and_clear_bit(nr ^ 0, addr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int __test_and_set_bit_le(int nr, void *addr)
{
 return ((__builtin_constant_p(nr ^ 0) && __builtin_constant_p((uintptr_t)(addr) != (uintptr_t)((void *)0)) && (uintptr_t)(addr) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(addr))) ? generic___test_and_set_bit(nr ^ 0, addr) : generic___test_and_set_bit(nr ^ 0, addr));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int __test_and_clear_bit_le(int nr, void *addr)
{
 return ((__builtin_constant_p(nr ^ 0) && __builtin_constant_p((uintptr_t)(addr) != (uintptr_t)((void *)0)) && (uintptr_t)(addr) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(addr))) ? generic___test_and_clear_bit(nr ^ 0, addr) : generic___test_and_clear_bit(nr ^ 0, addr));
}







_Static_assert(__builtin_types_compatible_p(typeof(generic___set_bit), typeof(generic___set_bit)) && __builtin_types_compatible_p(typeof(generic___set_bit), typeof(generic___set_bit)) && __builtin_types_compatible_p(typeof(generic___set_bit), typeof(generic___set_bit)), "__same_type(arch___set_bit, generic___set_bit) && __same_type(const___set_bit, generic___set_bit) && __same_type(___set_bit, generic___set_bit)");
_Static_assert(__builtin_types_compatible_p(typeof(generic___clear_bit), typeof(generic___clear_bit)) && __builtin_types_compatible_p(typeof(generic___clear_bit), typeof(generic___clear_bit)) && __builtin_types_compatible_p(typeof(generic___clear_bit), typeof(generic___clear_bit)), "__same_type(arch___clear_bit, generic___clear_bit) && __same_type(const___clear_bit, generic___clear_bit) && __same_type(___clear_bit, generic___clear_bit)");
_Static_assert(__builtin_types_compatible_p(typeof(generic___change_bit), typeof(generic___change_bit)) && __builtin_types_compatible_p(typeof(generic___change_bit), typeof(generic___change_bit)) && __builtin_types_compatible_p(typeof(generic___change_bit), typeof(generic___change_bit)), "__same_type(arch___change_bit, generic___change_bit) && __same_type(const___change_bit, generic___change_bit) && __same_type(___change_bit, generic___change_bit)");
_Static_assert(__builtin_types_compatible_p(typeof(generic___test_and_set_bit), typeof(generic___test_and_set_bit)) && __builtin_types_compatible_p(typeof(generic___test_and_set_bit), typeof(generic___test_and_set_bit)) && __builtin_types_compatible_p(typeof(generic___test_and_set_bit), typeof(generic___test_and_set_bit)), "__same_type(arch___test_and_set_bit, generic___test_and_set_bit) && __same_type(const___test_and_set_bit, generic___test_and_set_bit) && __same_type(___test_and_set_bit, generic___test_and_set_bit)");
_Static_assert(__builtin_types_compatible_p(typeof(generic___test_and_clear_bit), typeof(generic___test_and_clear_bit)) && __builtin_types_compatible_p(typeof(generic___test_and_clear_bit), typeof(generic___test_and_clear_bit)) && __builtin_types_compatible_p(typeof(generic___test_and_clear_bit), typeof(generic___test_and_clear_bit)), "__same_type(arch___test_and_clear_bit, generic___test_and_clear_bit) && __same_type(const___test_and_clear_bit, generic___test_and_clear_bit) && __same_type(___test_and_clear_bit, generic___test_and_clear_bit)");
_Static_assert(__builtin_types_compatible_p(typeof(generic___test_and_change_bit), typeof(generic___test_and_change_bit)) && __builtin_types_compatible_p(typeof(generic___test_and_change_bit), typeof(generic___test_and_change_bit)) && __builtin_types_compatible_p(typeof(generic___test_and_change_bit), typeof(generic___test_and_change_bit)), "__same_type(arch___test_and_change_bit, generic___test_and_change_bit) && __same_type(const___test_and_change_bit, generic___test_and_change_bit) && __same_type(___test_and_change_bit, generic___test_and_change_bit)");
_Static_assert(__builtin_types_compatible_p(typeof(generic_test_bit), typeof(generic_test_bit)) && __builtin_types_compatible_p(typeof(const_test_bit), typeof(generic_test_bit)) && __builtin_types_compatible_p(typeof(generic_test_bit), typeof(generic_test_bit)), "__same_type(arch_test_bit, generic_test_bit) && __same_type(const_test_bit, generic_test_bit) && __same_type(_test_bit, generic_test_bit)");



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int get_bitmask_order(unsigned int count)
{
 int order;

 order = fls(count);
 return order;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) unsigned long hweight_long(unsigned long w)
{
 return sizeof(w) == 4 ? (__builtin_constant_p(w) ? ((((unsigned int) ((!!((w) & (1ULL << 0))) + (!!((w) & (1ULL << 1))) + (!!((w) & (1ULL << 2))) + (!!((w) & (1ULL << 3))) + (!!((w) & (1ULL << 4))) + (!!((w) & (1ULL << 5))) + (!!((w) & (1ULL << 6))) + (!!((w) & (1ULL << 7))))) + ((unsigned int) ((!!(((w) >> 8) & (1ULL << 0))) + (!!(((w) >> 8) & (1ULL << 1))) + (!!(((w) >> 8) & (1ULL << 2))) + (!!(((w) >> 8) & (1ULL << 3))) + (!!(((w) >> 8) & (1ULL << 4))) + (!!(((w) >> 8) & (1ULL << 5))) + (!!(((w) >> 8) & (1ULL << 6))) + (!!(((w) >> 8) & (1ULL << 7)))))) + (((unsigned int) ((!!(((w) >> 16) & (1ULL << 0))) + (!!(((w) >> 16) & (1ULL << 1))) + (!!(((w) >> 16) & (1ULL << 2))) + (!!(((w) >> 16) & (1ULL << 3))) + (!!(((w) >> 16) & (1ULL << 4))) + (!!(((w) >> 16) & (1ULL << 5))) + (!!(((w) >> 16) & (1ULL << 6))) + (!!(((w) >> 16) & (1ULL << 7))))) + ((unsigned int) ((!!((((w) >> 16) >> 8) & (1ULL << 0))) + (!!((((w) >> 16) >> 8) & (1ULL << 1))) + (!!((((w) >> 16) >> 8) & (1ULL << 2))) + (!!((((w) >> 16) >> 8) & (1ULL << 3))) + (!!((((w) >> 16) >> 8) & (1ULL << 4))) + (!!((((w) >> 16) >> 8) & (1ULL << 5))) + (!!((((w) >> 16) >> 8) & (1ULL << 6))) + (!!((((w) >> 16) >> 8) & (1ULL << 7))))))) : __arch_hweight32(w)) : (__builtin_constant_p((__u64)w) ? (((((unsigned int) ((!!(((__u64)w) & (1ULL << 0))) + (!!(((__u64)w) & (1ULL << 1))) + (!!(((__u64)w) & (1ULL << 2))) + (!!(((__u64)w) & (1ULL << 3))) + (!!(((__u64)w) & (1ULL << 4))) + (!!(((__u64)w) & (1ULL << 5))) + (!!(((__u64)w) & (1ULL << 6))) + (!!(((__u64)w) & (1ULL << 7))))) + ((unsigned int) ((!!((((__u64)w) >> 8) & (1ULL << 0))) + (!!((((__u64)w) >> 8) & (1ULL << 1))) + (!!((((__u64)w) >> 8) & (1ULL << 2))) + (!!((((__u64)w) >> 8) & (1ULL << 3))) + (!!((((__u64)w) >> 8) & (1ULL << 4))) + (!!((((__u64)w) >> 8) & (1ULL << 5))) + (!!((((__u64)w) >> 8) & (1ULL << 6))) + (!!((((__u64)w) >> 8) & (1ULL << 7)))))) + (((unsigned int) ((!!((((__u64)w) >> 16) & (1ULL << 0))) + (!!((((__u64)w) >> 16) & (1ULL << 1))) + (!!((((__u64)w) >> 16) & (1ULL << 2))) + (!!((((__u64)w) >> 16) & (1ULL << 3))) + (!!((((__u64)w) >> 16) & (1ULL << 4))) + (!!((((__u64)w) >> 16) & (1ULL << 5))) + (!!((((__u64)w) >> 16) & (1ULL << 6))) + (!!((((__u64)w) >> 16) & (1ULL << 7))))) + ((unsigned int) ((!!(((((__u64)w) >> 16) >> 8) & (1ULL << 0))) + (!!(((((__u64)w) >> 16) >> 8) & (1ULL << 1))) + (!!(((((__u64)w) >> 16) >> 8) & (1ULL << 2))) + (!!(((((__u64)w) >> 16) >> 8) & (1ULL << 3))) + (!!(((((__u64)w) >> 16) >> 8) & (1ULL << 4))) + (!!(((((__u64)w) >> 16) >> 8) & (1ULL << 5))) + (!!(((((__u64)w) >> 16) >> 8) & (1ULL << 6))) + (!!(((((__u64)w) >> 16) >> 8) & (1ULL << 7))))))) + ((((unsigned int) ((!!((((__u64)w) >> 32) & (1ULL << 0))) + (!!((((__u64)w) >> 32) & (1ULL << 1))) + (!!((((__u64)w) >> 32) & (1ULL << 2))) + (!!((((__u64)w) >> 32) & (1ULL << 3))) + (!!((((__u64)w) >> 32) & (1ULL << 4))) + (!!((((__u64)w) >> 32) & (1ULL << 5))) + (!!((((__u64)w) >> 32) & (1ULL << 6))) + (!!((((__u64)w) >> 32) & (1ULL << 7))))) + ((unsigned int) ((!!(((((__u64)w) >> 32) >> 8) & (1ULL << 0))) + (!!(((((__u64)w) >> 32) >> 8) & (1ULL << 1))) + (!!(((((__u64)w) >> 32) >> 8) & (1ULL << 2))) + (!!(((((__u64)w) >> 32) >> 8) & (1ULL << 3))) + (!!(((((__u64)w) >> 32) >> 8) & (1ULL << 4))) + (!!(((((__u64)w) >> 32) >> 8) & (1ULL << 5))) + (!!(((((__u64)w) >> 32) >> 8) & (1ULL << 6))) + (!!(((((__u64)w) >> 32) >> 8) & (1ULL << 7)))))) + (((unsigned int) ((!!(((((__u64)w) >> 32) >> 16) & (1ULL << 0))) + (!!(((((__u64)w) >> 32) >> 16) & (1ULL << 1))) + (!!(((((__u64)w) >> 32) >> 16) & (1ULL << 2))) + (!!(((((__u64)w) >> 32) >> 16) & (1ULL << 3))) + (!!(((((__u64)w) >> 32) >> 16) & (1ULL << 4))) + (!!(((((__u64)w) >> 32) >> 16) & (1ULL << 5))) + (!!(((((__u64)w) >> 32) >> 16) & (1ULL << 6))) + (!!(((((__u64)w) >> 32) >> 16) & (1ULL << 7))))) + ((unsigned int) ((!!((((((__u64)w) >> 32) >> 16) >> 8) & (1ULL << 0))) + (!!((((((__u64)w) >> 32) >> 16) >> 8) & (1ULL << 1))) + (!!((((((__u64)w) >> 32) >> 16) >> 8) & (1ULL << 2))) + (!!((((((__u64)w) >> 32) >> 16) >> 8) & (1ULL << 3))) + (!!((((((__u64)w) >> 32) >> 16) >> 8) & (1ULL << 4))) + (!!((((((__u64)w) >> 32) >> 16) >> 8) & (1ULL << 5))) + (!!((((((__u64)w) >> 32) >> 16) >> 8) & (1ULL << 6))) + (!!((((((__u64)w) >> 32) >> 16) >> 8) & (1ULL << 7)))))))) : __arch_hweight64((__u64)w));
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __u64 rol64(__u64 word, unsigned int shift)
{
 return (word << (shift & 63)) | (word >> ((-shift) & 63));
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __u64 ror64(__u64 word, unsigned int shift)
{
 return (word >> (shift & 63)) | (word << ((-shift) & 63));
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __u32 rol32(__u32 word, unsigned int shift)
{
 return (word << (shift & 31)) | (word >> ((-shift) & 31));
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __u32 ror32(__u32 word, unsigned int shift)
{
 return (word >> (shift & 31)) | (word << ((-shift) & 31));
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __u16 rol16(__u16 word, unsigned int shift)
{
 return (word << (shift & 15)) | (word >> ((-shift) & 15));
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __u16 ror16(__u16 word, unsigned int shift)
{
 return (word >> (shift & 15)) | (word << ((-shift) & 15));
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __u8 rol8(__u8 word, unsigned int shift)
{
 return (word << (shift & 7)) | (word >> ((-shift) & 7));
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __u8 ror8(__u8 word, unsigned int shift)
{
 return (word >> (shift & 7)) | (word << ((-shift) & 7));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __s32 sign_extend32(__u32 value, int index)
{
 __u8 shift = 31 - index;
 return (__s32)(value << shift) >> shift;
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __s64 sign_extend64(__u64 value, int index)
{
 __u8 shift = 63 - index;
 return (__s64)(value << shift) >> shift;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned fls_long(unsigned long l)
{
 if (sizeof(l) == 4)
  return fls(l);
 return fls64(l);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int get_count_order(unsigned int count)
{
 if (count == 0)
  return -1;

 return fls(--count);
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int get_count_order_long(unsigned long l)
{
 if (l == 0UL)
  return -1;
 return (int)fls_long(--l);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long __ffs64(u64 word)
{






 return __ffs((unsigned long)word);
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long fns(unsigned long word, unsigned int n)
{
 unsigned int bit;

 while (word) {
  bit = __ffs(word);
  if (n-- == 0)
   return bit;
  ((__builtin_constant_p(bit) && __builtin_constant_p((uintptr_t)(&word) != (uintptr_t)((void *)0)) && (uintptr_t)(&word) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&word))) ? generic___clear_bit(bit, &word) : generic___clear_bit(bit, &word));
 }

 return 64;
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void assign_bit(long nr, volatile unsigned long *addr,
           bool value)
{
 if (value)
  set_bit(nr, addr);
 else
  clear_bit(nr, addr);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __assign_bit(long nr, volatile unsigned long *addr,
      bool value)
{
 if (value)
  ((__builtin_constant_p(nr) && __builtin_constant_p((uintptr_t)(addr) != (uintptr_t)((void *)0)) && (uintptr_t)(addr) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(addr))) ? generic___set_bit(nr, addr) : generic___set_bit(nr, addr));
 else
  ((__builtin_constant_p(nr) && __builtin_constant_p((uintptr_t)(addr) != (uintptr_t)((void *)0)) && (uintptr_t)(addr) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(addr))) ? generic___clear_bit(nr, addr) : generic___clear_bit(nr, addr));
}






extern const char hex_asc[];



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) char *hex_byte_pack(char *buf, u8 byte)
{
 *buf++ = hex_asc[((byte) & 0xf0) >> 4];
 *buf++ = hex_asc[((byte) & 0x0f)];
 return buf;
}

extern const char hex_asc_upper[];



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) char *hex_byte_pack_upper(char *buf, u8 byte)
{
 *buf++ = hex_asc_upper[((byte) & 0xf0) >> 4];
 *buf++ = hex_asc_upper[((byte) & 0x0f)];
 return buf;
}

extern int hex_to_bin(unsigned char ch);
extern int __attribute__((__warn_unused_result__)) hex2bin(u8 *dst, const char *src, size_t count);
extern char *bin2hex(char *dst, const void *src, size_t count);

bool mac_pton(const char *s, u8 *mac);








int __attribute__((__warn_unused_result__)) _kstrtoul(const char *s, unsigned int base, unsigned long *res);
int __attribute__((__warn_unused_result__)) _kstrtol(const char *s, unsigned int base, long *res);

int __attribute__((__warn_unused_result__)) kstrtoull(const char *s, unsigned int base, unsigned long long *res);
int __attribute__((__warn_unused_result__)) kstrtoll(const char *s, unsigned int base, long long *res);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int __attribute__((__warn_unused_result__)) kstrtoul(const char *s, unsigned int base, unsigned long *res)
{




 if (sizeof(unsigned long) == sizeof(unsigned long long) &&
     __alignof__(unsigned long) == __alignof__(unsigned long long))
  return kstrtoull(s, base, (unsigned long long *)res);
 else
  return _kstrtoul(s, base, res);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int __attribute__((__warn_unused_result__)) kstrtol(const char *s, unsigned int base, long *res)
{




 if (sizeof(long) == sizeof(long long) &&
     __alignof__(long) == __alignof__(long long))
  return kstrtoll(s, base, (long long *)res);
 else
  return _kstrtol(s, base, res);
}

int __attribute__((__warn_unused_result__)) kstrtouint(const char *s, unsigned int base, unsigned int *res);
int __attribute__((__warn_unused_result__)) kstrtoint(const char *s, unsigned int base, int *res);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int __attribute__((__warn_unused_result__)) kstrtou64(const char *s, unsigned int base, u64 *res)
{
 return kstrtoull(s, base, res);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int __attribute__((__warn_unused_result__)) kstrtos64(const char *s, unsigned int base, s64 *res)
{
 return kstrtoll(s, base, res);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int __attribute__((__warn_unused_result__)) kstrtou32(const char *s, unsigned int base, u32 *res)
{
 return kstrtouint(s, base, res);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int __attribute__((__warn_unused_result__)) kstrtos32(const char *s, unsigned int base, s32 *res)
{
 return kstrtoint(s, base, res);
}

int __attribute__((__warn_unused_result__)) kstrtou16(const char *s, unsigned int base, u16 *res);
int __attribute__((__warn_unused_result__)) kstrtos16(const char *s, unsigned int base, s16 *res);
int __attribute__((__warn_unused_result__)) kstrtou8(const char *s, unsigned int base, u8 *res);
int __attribute__((__warn_unused_result__)) kstrtos8(const char *s, unsigned int base, s8 *res);
int __attribute__((__warn_unused_result__)) kstrtobool(const char *s, bool *res);

int __attribute__((__warn_unused_result__)) kstrtoull_from_user(const char *s, size_t count, unsigned int base, unsigned long long *res);
int __attribute__((__warn_unused_result__)) kstrtoll_from_user(const char *s, size_t count, unsigned int base, long long *res);
int __attribute__((__warn_unused_result__)) kstrtoul_from_user(const char *s, size_t count, unsigned int base, unsigned long *res);
int __attribute__((__warn_unused_result__)) kstrtol_from_user(const char *s, size_t count, unsigned int base, long *res);
int __attribute__((__warn_unused_result__)) kstrtouint_from_user(const char *s, size_t count, unsigned int base, unsigned int *res);
int __attribute__((__warn_unused_result__)) kstrtoint_from_user(const char *s, size_t count, unsigned int base, int *res);
int __attribute__((__warn_unused_result__)) kstrtou16_from_user(const char *s, size_t count, unsigned int base, u16 *res);
int __attribute__((__warn_unused_result__)) kstrtos16_from_user(const char *s, size_t count, unsigned int base, s16 *res);
int __attribute__((__warn_unused_result__)) kstrtou8_from_user(const char *s, size_t count, unsigned int base, u8 *res);
int __attribute__((__warn_unused_result__)) kstrtos8_from_user(const char *s, size_t count, unsigned int base, s8 *res);
int __attribute__((__warn_unused_result__)) kstrtobool_from_user(const char *s, size_t count, bool *res);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int __attribute__((__warn_unused_result__)) kstrtou64_from_user(const char *s, size_t count, unsigned int base, u64 *res)
{
 return kstrtoull_from_user(s, count, base, res);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int __attribute__((__warn_unused_result__)) kstrtos64_from_user(const char *s, size_t count, unsigned int base, s64 *res)
{
 return kstrtoll_from_user(s, count, base, res);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int __attribute__((__warn_unused_result__)) kstrtou32_from_user(const char *s, size_t count, unsigned int base, u32 *res)
{
 return kstrtouint_from_user(s, count, base, res);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int __attribute__((__warn_unused_result__)) kstrtos32_from_user(const char *s, size_t count, unsigned int base, s32 *res)
{
 return kstrtoint_from_user(s, count, base, res);
}
extern unsigned long simple_strtoul(const char *,char **,unsigned int);
extern long simple_strtol(const char *,char **,unsigned int);
extern unsigned long long simple_strtoull(const char *,char **,unsigned int);
extern long long simple_strtoll(const char *,char **,unsigned int);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int strtobool(const char *s, bool *res)
{
 return kstrtobool(s, res);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __attribute__((const))
int __ilog2_u32(u32 n)
{
 return fls(n) - 1;
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __attribute__((const))
int __ilog2_u64(u64 n)
{
 return fls64(n) - 1;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((const))
bool is_power_of_2(unsigned long n)
{
 return (n != 0 && ((n & (n - 1)) == 0));
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((const))
unsigned long __roundup_pow_of_two(unsigned long n)
{
 return 1UL << fls_long(n - 1);
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((const))
unsigned long __rounddown_pow_of_two(unsigned long n)
{
 return 1UL << (fls_long(n) - 1);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__const__))
int __order_base_2(unsigned long n)
{
 return n > 1 ? ( __builtin_constant_p(n - 1) ? ((n - 1) < 2 ? 0 : 63 - __builtin_clzll(n - 1)) : (sizeof(n - 1) <= 4) ? __ilog2_u32(n - 1) : __ilog2_u64(n - 1) ) + 1 : 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((const))
int __bits_per(unsigned long n)
{
 if (n < 2)
  return 1;
 if (is_power_of_2(n))
  return ( __builtin_constant_p(n) ? ( ((n) == 0 || (n) == 1) ? 0 : ( __builtin_constant_p((n) - 1) ? (((n) - 1) < 2 ? 0 : 63 - __builtin_clzll((n) - 1)) : (sizeof((n) - 1) <= 4) ? __ilog2_u32((n) - 1) : __ilog2_u64((n) - 1) ) + 1) : __order_base_2(n) ) + 1;
 return ( __builtin_constant_p(n) ? ( ((n) == 0 || (n) == 1) ? 0 : ( __builtin_constant_p((n) - 1) ? (((n) - 1) < 2 ? 0 : 63 - __builtin_clzll((n) - 1)) : (sizeof((n) - 1) <= 4) ? __ilog2_u32((n) - 1) : __ilog2_u64((n) - 1) ) + 1) : __order_base_2(n) );
}





struct s16_fract { __s16 numerator; __s16 denominator; };
struct u16_fract { __u16 numerator; __u16 denominator; };
struct s32_fract { __s32 numerator; __s32 denominator; };
struct u32_fract { __u32 numerator; __u32 denominator; };
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u32 reciprocal_scale(u32 val, u32 ep_ro)
{
 return (u32)(((u64) val * ep_ro) >> 32);
}

u64 int_pow(u64 base, unsigned int exp);
unsigned long int_sqrt(unsigned long);




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u32 int_sqrt64(u64 x)
{
 return (u32)int_sqrt(x);
}








struct pt_regs;

extern long (*panic_blink)(int state);
__attribute__((__format__(printf, 1, 2)))
void panic(const char *fmt, ...) __attribute__((__noreturn__)) __attribute__((__cold__));
void nmi_panic(struct pt_regs *regs, const char *msg);
void check_panic_on_warn(const char *origin);
extern void oops_enter(void);
extern void oops_exit(void);
extern bool oops_may_print(void);

extern int panic_timeout;
extern unsigned long panic_print;
extern int panic_on_oops;
extern int panic_on_unrecovered_nmi;
extern int panic_on_io_nmi;
extern int panic_on_warn;

extern unsigned long panic_on_taint;
extern bool panic_on_taint_nousertaint;

extern int sysctl_panic_on_rcu_stall;
extern int sysctl_max_rcu_stall_to_panic;
extern int sysctl_panic_on_stackoverflow;

extern bool crash_kexec_post_notifiers;

extern void __stack_chk_fail(void);
void abort(void);






extern atomic_t panic_cpu;






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void set_arch_panic_timeout(int timeout, int arch_default_timeout)
{
 if (panic_timeout == arch_default_timeout)
  panic_timeout = timeout;
}
struct taint_flag {
 char c_true;
 char c_false;
 bool module;
};

extern const struct taint_flag taint_flags[19];

enum lockdep_ok {
 LOCKDEP_STILL_OK,
 LOCKDEP_NOW_UNRELIABLE,
};

extern const char *print_tainted(void);
extern void add_taint(unsigned flag, enum lockdep_ok);
extern int test_taint(unsigned flag);
extern unsigned long get_taint(void);





typedef int (*initcall_t)(void);
typedef void (*exitcall_t)(void);
typedef initcall_t initcall_entry_t;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) initcall_t initcall_from_entry(initcall_entry_t *entry)
{
 return *entry;
}


extern initcall_entry_t __con_initcall_start[], __con_initcall_end[];


typedef void (*ctor_fn_t)(void);

struct file_system_type;


extern int do_one_initcall(initcall_t fn);
extern char boot_command_line[];
extern char *saved_command_line;
extern unsigned int saved_command_line_len;
extern unsigned int reset_devices;
extern char saved_bootconfig_string[];


void setup_arch(char **);
void prepare_namespace(void);
void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) init_rootfs(void);

void init_IRQ(void);
void time_init(void);
void poking_init(void);
void pgtable_cache_init(void);

extern initcall_entry_t __initcall_start[];
extern initcall_entry_t __initcall0_start[];
extern initcall_entry_t __initcall1_start[];
extern initcall_entry_t __initcall2_start[];
extern initcall_entry_t __initcall3_start[];
extern initcall_entry_t __initcall4_start[];
extern initcall_entry_t __initcall5_start[];
extern initcall_entry_t __initcall6_start[];
extern initcall_entry_t __initcall7_start[];
extern initcall_entry_t __initcall_end[];

extern struct file_system_type rootfs_fs_type;
extern void (*late_time_init)(void);

extern bool initcall_debug;
struct obs_kernel_param {
 const char *str;
 int (*setup_func)(char *);
 int early;
};

extern const struct obs_kernel_param __setup_start[], __setup_end[];
void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) parse_early_param(void);
void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) parse_early_options(char *cmdline);



























typedef struct qspinlock {
 union {
  atomic_t val;







  struct {
   u8 locked;
   u8 pending;
  };
  struct {
   u16 locked_pending;
   u16 tail;
  };
 };
} arch_spinlock_t;











typedef struct qrwlock {
 union {
  atomic_t cnts;
  struct {

   u8 wlocked;
   u8 __lstate[3];




  };
 };
 arch_spinlock_t wait_lock;
} arch_rwlock_t;




enum lockdep_wait_type {
 LD_WAIT_INV = 0,

 LD_WAIT_FREE,
 LD_WAIT_SPIN,


 LD_WAIT_CONFIG,



 LD_WAIT_SLEEP,

 LD_WAIT_MAX,
};

enum lockdep_lock_type {
 LD_LOCK_NORMAL = 0,
 LD_LOCK_PERCPU,
 LD_LOCK_WAIT_OVERRIDE,
 LD_LOCK_MAX,
};
struct lockdep_subclass_key {
 char __one_byte;
} __attribute__ ((__packed__));


struct lock_class_key {
 union {
  struct hlist_node hash_entry;
  struct lockdep_subclass_key subkeys[8UL];
 };
};

extern struct lock_class_key __lockdep_no_validate__;

struct lock_trace;



struct lockdep_map;
typedef int (*lock_cmp_fn)(const struct lockdep_map *a,
      const struct lockdep_map *b);
typedef void (*lock_print_fn)(const struct lockdep_map *map);





struct lock_class {



 struct hlist_node hash_entry;






 struct list_head lock_entry;






 struct list_head locks_after, locks_before;

 const struct lockdep_subclass_key *key;
 lock_cmp_fn cmp_fn;
 lock_print_fn print_fn;

 unsigned int subclass;
 unsigned int dep_gen_id;




 unsigned long usage_mask;
 const struct lock_trace *usage_traces[(2*4 + 2)];





 int name_version;
 const char *name;

 u8 wait_type_inner;
 u8 wait_type_outer;
 u8 lock_type;



 unsigned long contention_point[4];
 unsigned long contending_point[4];

} __attribute__((no_randomize_layout));


struct lock_time {
 s64 min;
 s64 max;
 s64 total;
 unsigned long nr;
};

enum bounce_type {
 bounce_acquired_write,
 bounce_acquired_read,
 bounce_contended_write,
 bounce_contended_read,
 nr_bounce_types,

 bounce_acquired = bounce_acquired_write,
 bounce_contended = bounce_contended_write,
};

struct lock_class_stats {
 unsigned long contention_point[4];
 unsigned long contending_point[4];
 struct lock_time read_waittime;
 struct lock_time write_waittime;
 struct lock_time read_holdtime;
 struct lock_time write_holdtime;
 unsigned long bounces[nr_bounce_types];
};

struct lock_class_stats lock_stats(struct lock_class *class);
void clear_lock_stats(struct lock_class *class);






struct lockdep_map {
 struct lock_class_key *key;
 struct lock_class *class_cache[2];
 const char *name;
 u8 wait_type_outer;
 u8 wait_type_inner;
 u8 lock_type;


 int cpu;
 unsigned long ip;

};

struct pin_cookie { unsigned int val; };

typedef struct raw_spinlock {
 arch_spinlock_t raw_lock;

 unsigned int magic, owner_cpu;
 void *owner;


 struct lockdep_map dep_map;

} raw_spinlock_t;







struct ratelimit_state {
 raw_spinlock_t lock;

 int interval;
 int burst;
 int printed;
 int missed;
 unsigned long begin;
 unsigned long flags;
};
extern int ___ratelimit(struct ratelimit_state *rs, const char *func);

extern const char linux_banner[];
extern const char linux_proc_banner[];

extern int oops_in_progress;



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int printk_get_level(const char *buffer)
{
 if (buffer[0] == '\001' && buffer[1]) {
  switch (buffer[1]) {
  case '0' ... '7':
  case 'c':
   return buffer[1];
  }
 }
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) const char *printk_skip_level(const char *buffer)
{
 if (printk_get_level(buffer))
  return buffer + 2;

 return buffer;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) const char *printk_skip_headers(const char *buffer)
{
 while (printk_get_level(buffer))
  buffer = printk_skip_level(buffer);

 return buffer;
}
extern int console_printk[];






extern void console_verbose(void);



extern char devkmsg_log_str[];
struct ctl_table;

extern int suppress_printk;

struct va_format {
 const char *fmt;
 va_list *va;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__format__(printf, 1, 2))) __attribute__((__cold__))
void early_printk(const char *s, ...) { }


struct dev_printk_info;


           __attribute__((__format__(printf, 4, 0)))
int vprintk_emit(int facility, int level,
   const struct dev_printk_info *dev_info,
   const char *fmt, va_list args);

           __attribute__((__format__(printf, 1, 0)))
int vprintk(const char *fmt, va_list args);

           __attribute__((__format__(printf, 1, 2))) __attribute__((__cold__))
int _printk(const char *fmt, ...);




__attribute__((__format__(printf, 1, 2))) __attribute__((__cold__)) int _printk_deferred(const char *fmt, ...);

extern void __printk_safe_enter(void);
extern void __printk_safe_exit(void);
extern int __printk_ratelimit(const char *func);

extern bool printk_timed_ratelimit(unsigned long *caller_jiffies,
       unsigned int interval_msec);

extern int printk_delay_msec;
extern int dmesg_restrict;

extern void wake_up_klogd(void);

char *log_buf_addr_get(void);
u32 log_buf_len_get(void);
void log_buf_vmcoreinfo_setup(void);
void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) setup_log_buf(int early);
__attribute__((__format__(printf, 1, 2))) void dump_stack_set_arch_desc(const char *fmt, ...);
void dump_stack_print_info(const char *log_lvl);
void show_regs_print_info(const char *log_lvl);
extern void dump_stack_lvl(const char *log_lvl) __attribute__((__cold__));
extern void dump_stack(void) __attribute__((__cold__));
void printk_trigger_flush(void);
extern int __printk_cpu_sync_try_get(void);
extern void __printk_cpu_sync_wait(void);
extern void __printk_cpu_sync_put(void);
extern int kptr_restrict;
struct module;


struct pi_entry {
 const char *fmt;
 const char *func;
 const char *file;
 unsigned int line;
 const char *level;
 const char *subsys_fmt_prefix;
} __attribute__((__packed__));





extern bool static_key_initialized;





struct static_key {
 atomic_t enabled;
 union {
  unsigned long type;
  struct jump_entry *entries;
  struct static_key_mod *next;
 };

};




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool arch_static_branch(struct static_key * const key, const bool branch)
{
 asm goto("1:	nop			\n\t" ".pushsection	__jump_table, \"aw\"	\n\t" ".align	3			\n\t" ".long		1b - ., %l[l_yes] - .	\n\t" ".quad		%0 - .			\n\t" ".popsection				\n\t" : : "i"(&((char *)key)[branch]) : : l_yes);




 return false;

l_yes:
 return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool arch_static_branch_jump(struct static_key * const key, const bool branch)
{
 asm goto("1:	b	%l[l_yes]	\n\t" ".pushsection	__jump_table, \"aw\"	\n\t" ".align	3			\n\t" ".long		1b - ., %l[l_yes] - .	\n\t" ".quad		%0 - .			\n\t" ".popsection				\n\t" : : "i"(&((char *)key)[branch]) : : l_yes);




 return false;

l_yes:
 return true;
}




struct jump_entry {
 s32 code;
 s32 target;
 long key;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long jump_entry_code(const struct jump_entry *entry)
{
 return (unsigned long)&entry->code + entry->code;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long jump_entry_target(const struct jump_entry *entry)
{
 return (unsigned long)&entry->target + entry->target;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) struct static_key *jump_entry_key(const struct jump_entry *entry)
{
 long offset = entry->key & ~3L;

 return (struct static_key *)((unsigned long)&entry->key + offset);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool jump_entry_is_branch(const struct jump_entry *entry)
{
 return (unsigned long)entry->key & 1UL;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool jump_entry_is_init(const struct jump_entry *entry)
{
 return (unsigned long)entry->key & 2UL;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void jump_entry_set_init(struct jump_entry *entry, bool set)
{
 if (set)
  entry->key |= 2;
 else
  entry->key &= ~2;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int jump_entry_size(struct jump_entry *entry)
{

 return 4;



}






enum jump_label_type {
 JUMP_LABEL_NOP = 0,
 JUMP_LABEL_JMP,
};

struct module;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool static_key_false(struct static_key *key)
{
 return arch_static_branch(key, false);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool static_key_true(struct static_key *key)
{
 return !arch_static_branch(key, true);
}

extern struct jump_entry __start___jump_table[];
extern struct jump_entry __stop___jump_table[];

extern void jump_label_init(void);
extern void jump_label_lock(void);
extern void jump_label_unlock(void);
extern void arch_jump_label_transform(struct jump_entry *entry,
          enum jump_label_type type);
extern bool arch_jump_label_transform_queue(struct jump_entry *entry,
         enum jump_label_type type);
extern void arch_jump_label_transform_apply(void);
extern int jump_label_text_reserved(void *start, void *end);
extern bool static_key_slow_inc(struct static_key *key);
extern bool static_key_fast_inc_not_disabled(struct static_key *key);
extern void static_key_slow_dec(struct static_key *key);
extern bool static_key_slow_inc_cpuslocked(struct static_key *key);
extern void static_key_slow_dec_cpuslocked(struct static_key *key);
extern int static_key_count(struct static_key *key);
extern void static_key_enable(struct static_key *key);
extern void static_key_disable(struct static_key *key);
extern void static_key_enable_cpuslocked(struct static_key *key);
extern void static_key_disable_cpuslocked(struct static_key *key);
extern enum jump_label_type jump_label_init_type(struct jump_entry *entry);
struct static_key_true {
 struct static_key key;
};

struct static_key_false {
 struct static_key key;
};
extern bool ____wrong_branch_error(void);
struct _ddebug {




 const char *modname;
 const char *function;
 const char *filename;
 const char *format;
 unsigned int lineno:18;

 unsigned int class_id:6;
 unsigned int flags:8;

 union {
  struct static_key_true dd_key_true;
  struct static_key_false dd_key_false;
 } key;

} __attribute__((aligned(8)));

enum class_map_type {
 DD_CLASS_TYPE_DISJOINT_BITS,




 DD_CLASS_TYPE_LEVEL_NUM,




 DD_CLASS_TYPE_DISJOINT_NAMES,




 DD_CLASS_TYPE_LEVEL_NAMES,





};

struct ddebug_class_map {
 struct list_head link;
 struct module *mod;
 const char *mod_name;
 const char **class_names;
 const int length;
 const int base;
 enum class_map_type map_type;
};
struct _ddebug_info {
 struct _ddebug *descs;
 struct ddebug_class_map *classes;
 unsigned int num_descs;
 unsigned int num_classes;
};

struct ddebug_class_param {
 union {
  unsigned long *bits;
  unsigned int *lvl;
 };
 char flags[8];
 const struct ddebug_class_map *map;
};
extern __attribute__((__format__(printf, 2, 3)))
void __dynamic_pr_debug(struct _ddebug *descriptor, const char *fmt, ...);

struct device;

extern __attribute__((__format__(printf, 3, 4)))
void __dynamic_dev_dbg(struct _ddebug *descriptor, const struct device *dev,
         const char *fmt, ...);

struct net_device;

extern __attribute__((__format__(printf, 3, 4)))
void __dynamic_netdev_dbg(struct _ddebug *descriptor,
     const struct net_device *dev,
     const char *fmt, ...);

struct ib_device;

extern __attribute__((__format__(printf, 3, 4)))
void __dynamic_ibdev_dbg(struct _ddebug *descriptor,
    const struct ib_device *ibdev,
    const char *fmt, ...);
extern int ddebug_dyndbg_module_param_cb(char *param, char *val,
     const char *modname);
struct kernel_param;
int param_set_dyndbg_classes(const char *instr, const struct kernel_param *kp);
int param_get_dyndbg_classes(char *buffer, const struct kernel_param *kp);
extern const struct kernel_param_ops param_ops_dyndbg_classes;
extern const struct file_operations kmsg_fops;

enum {
 DUMP_PREFIX_NONE,
 DUMP_PREFIX_ADDRESS,
 DUMP_PREFIX_OFFSET
};
extern int hex_dump_to_buffer(const void *buf, size_t len, int rowsize,
         int groupsize, char *linebuf, size_t linebuflen,
         bool ascii);

extern void print_hex_dump(const char *level, const char *prefix_str,
      int prefix_type, int rowsize, int groupsize,
      const void *buf, size_t len, bool ascii);

struct static_call_site {
 s32 addr;
 s32 key;
};
struct static_call_key {
 void *func;
};
struct completion;
struct user;
extern void __might_resched(const char *file, int line, unsigned int offsets);
extern void __might_sleep(const char *file, int line);
extern void __cant_sleep(const char *file, int line, int preempt_offset);
extern void __cant_migrate(const char *file, int line);
void __might_fault(const char *file, int line);




void do_exit(long error_code) __attribute__((__noreturn__));

extern int num_to_str(char *buf, int size,
        unsigned long long num, unsigned int width);



extern __attribute__((__format__(printf, 2, 3))) int sprintf(char *buf, const char * fmt, ...);
extern __attribute__((__format__(printf, 2, 0))) int vsprintf(char *buf, const char *, va_list);
extern __attribute__((__format__(printf, 3, 4)))
int snprintf(char *buf, size_t size, const char *fmt, ...);
extern __attribute__((__format__(printf, 3, 0)))
int vsnprintf(char *buf, size_t size, const char *fmt, va_list args);
extern __attribute__((__format__(printf, 3, 4)))
int scnprintf(char *buf, size_t size, const char *fmt, ...);
extern __attribute__((__format__(printf, 3, 0)))
int vscnprintf(char *buf, size_t size, const char *fmt, va_list args);
extern __attribute__((__format__(printf, 2, 3))) __attribute__((__malloc__))
char *kasprintf(gfp_t gfp, const char *fmt, ...);
extern __attribute__((__format__(printf, 2, 0))) __attribute__((__malloc__))
char *kvasprintf(gfp_t gfp, const char *fmt, va_list args);
extern __attribute__((__format__(printf, 2, 0)))
const char *kvasprintf_const(gfp_t gfp, const char *fmt, va_list args);

extern __attribute__((__format__(scanf, 2, 3)))
int sscanf(const char *, const char *, ...);
extern __attribute__((__format__(scanf, 2, 0)))
int vsscanf(const char *, const char *, va_list);

extern int no_hash_pointers_enable(char *str);

extern int get_option(char **str, int *pint);
extern char *get_options(const char *str, int nints, int *ints);
extern unsigned long long memparse(const char *ptr, char **retptr);
extern bool parse_option_str(const char *str, const char *option);
extern char *next_arg(char *args, char **param, char **val);

extern int core_kernel_text(unsigned long addr);
extern int __kernel_text_address(unsigned long addr);
extern int kernel_text_address(unsigned long addr);
extern int func_ptr_is_kernel_text(void *ptr);

extern void bust_spinlocks(int yes);

extern int root_mountflags;

extern bool early_boot_irqs_disabled;





extern enum system_states {
 SYSTEM_BOOTING,
 SYSTEM_SCHEDULING,
 SYSTEM_FREEING_INITMEM,
 SYSTEM_RUNNING,
 SYSTEM_HALT,
 SYSTEM_POWER_OFF,
 SYSTEM_RESTART,
 SYSTEM_SUSPEND,
} system_state;
enum ftrace_dump_mode {
 DUMP_NONE,
 DUMP_ALL,
 DUMP_ORIG,
};


void tracing_on(void);
void tracing_off(void);
int tracing_is_on(void);
void tracing_snapshot(void);
void tracing_snapshot_alloc(void);

extern void tracing_start(void);
extern void tracing_stop(void);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__format__(printf, 1, 2)))
void ____trace_printk_check_format(const char *fmt, ...)
{
}
extern __attribute__((__format__(printf, 2, 3)))
int __trace_bprintk(unsigned long ip, const char *fmt, ...);

extern __attribute__((__format__(printf, 2, 3)))
int __trace_printk(unsigned long ip, const char *fmt, ...);
extern int __trace_bputs(unsigned long ip, const char *str);
extern int __trace_puts(unsigned long ip, const char *str, int size);

extern void trace_dump_stack(int skip);
extern __attribute__((__format__(printf, 2, 0))) int
__ftrace_vbprintk(unsigned long ip, const char *fmt, va_list ap);

extern __attribute__((__format__(printf, 2, 0))) int
__ftrace_vprintk(unsigned long ip, const char *fmt, va_list ap);

extern void ftrace_dump(enum ftrace_dump_mode oops_dump_mode);






struct cacheline_padding {
 char x[0];
} __attribute__((__aligned__(1 << (6))));



















static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void INIT_LIST_HEAD(struct list_head *list)
{
 do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_14(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(list->next) == sizeof(char) || sizeof(list->next) == sizeof(short) || sizeof(list->next) == sizeof(int) || sizeof(list->next) == sizeof(long)) || sizeof(list->next) == sizeof(long long))) __compiletime_assert_14(); } while (0); do { *(volatile typeof(list->next) *)&(list->next) = (list); } while (0); } while (0);
 do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_15(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(list->prev) == sizeof(char) || sizeof(list->prev) == sizeof(short) || sizeof(list->prev) == sizeof(int) || sizeof(list->prev) == sizeof(long)) || sizeof(list->prev) == sizeof(long long))) __compiletime_assert_15(); } while (0); do { *(volatile typeof(list->prev) *)&(list->prev) = (list); } while (0); } while (0);
}


extern bool __list_add_valid(struct list_head *new,
         struct list_head *prev,
         struct list_head *next);
extern bool __list_del_entry_valid(struct list_head *entry);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __list_add(struct list_head *new,
         struct list_head *prev,
         struct list_head *next)
{
 if (!__list_add_valid(new, prev, next))
  return;

 next->prev = new;
 new->next = next;
 new->prev = prev;
 do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_16(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(prev->next) == sizeof(char) || sizeof(prev->next) == sizeof(short) || sizeof(prev->next) == sizeof(int) || sizeof(prev->next) == sizeof(long)) || sizeof(prev->next) == sizeof(long long))) __compiletime_assert_16(); } while (0); do { *(volatile typeof(prev->next) *)&(prev->next) = (new); } while (0); } while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void list_add(struct list_head *new, struct list_head *head)
{
 __list_add(new, head, head->next);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void list_add_tail(struct list_head *new, struct list_head *head)
{
 __list_add(new, head->prev, head);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __list_del(struct list_head * prev, struct list_head * next)
{
 next->prev = prev;
 do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_17(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(prev->next) == sizeof(char) || sizeof(prev->next) == sizeof(short) || sizeof(prev->next) == sizeof(int) || sizeof(prev->next) == sizeof(long)) || sizeof(prev->next) == sizeof(long long))) __compiletime_assert_17(); } while (0); do { *(volatile typeof(prev->next) *)&(prev->next) = (next); } while (0); } while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __list_del_clearprev(struct list_head *entry)
{
 __list_del(entry->prev, entry->next);
 entry->prev = ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __list_del_entry(struct list_head *entry)
{
 if (!__list_del_entry_valid(entry))
  return;

 __list_del(entry->prev, entry->next);
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void list_del(struct list_head *entry)
{
 __list_del_entry(entry);
 entry->next = ((void *) 0x100 + 0);
 entry->prev = ((void *) 0x122 + 0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void list_replace(struct list_head *old,
    struct list_head *new)
{
 new->next = old->next;
 new->next->prev = new;
 new->prev = old->prev;
 new->prev->next = new;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void list_replace_init(struct list_head *old,
         struct list_head *new)
{
 list_replace(old, new);
 INIT_LIST_HEAD(old);
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void list_swap(struct list_head *entry1,
        struct list_head *entry2)
{
 struct list_head *pos = entry2->prev;

 list_del(entry2);
 list_replace(entry1, entry2);
 if (pos == entry1)
  pos = entry2;
 list_add(entry1, pos);
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void list_del_init(struct list_head *entry)
{
 __list_del_entry(entry);
 INIT_LIST_HEAD(entry);
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void list_move(struct list_head *list, struct list_head *head)
{
 __list_del_entry(list);
 list_add(list, head);
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void list_move_tail(struct list_head *list,
      struct list_head *head)
{
 __list_del_entry(list);
 list_add_tail(list, head);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void list_bulk_move_tail(struct list_head *head,
           struct list_head *first,
           struct list_head *last)
{
 first->prev->next = last->next;
 last->next->prev = first->prev;

 head->prev->next = first;
 first->prev = head->prev;

 last->next = head;
 head->prev = last;
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int list_is_first(const struct list_head *list, const struct list_head *head)
{
 return list->prev == head;
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int list_is_last(const struct list_head *list, const struct list_head *head)
{
 return list->next == head;
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int list_is_head(const struct list_head *list, const struct list_head *head)
{
 return list == head;
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int list_empty(const struct list_head *head)
{
 return ({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_18(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(head->next) == sizeof(char) || sizeof(head->next) == sizeof(short) || sizeof(head->next) == sizeof(int) || sizeof(head->next) == sizeof(long)) || sizeof(head->next) == sizeof(long long))) __compiletime_assert_18(); } while (0); (*(const volatile typeof( _Generic((head->next), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (head->next))) *)&(head->next)); }) == head;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void list_del_init_careful(struct list_head *entry)
{
 __list_del_entry(entry);
 do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_19(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(entry->prev) == sizeof(char) || sizeof(entry->prev) == sizeof(short) || sizeof(entry->prev) == sizeof(int) || sizeof(entry->prev) == sizeof(long)) || sizeof(entry->prev) == sizeof(long long))) __compiletime_assert_19(); } while (0); do { *(volatile typeof(entry->prev) *)&(entry->prev) = (entry); } while (0); } while (0);
 do { do { } while (0); do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_20(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&entry->next) == sizeof(char) || sizeof(*&entry->next) == sizeof(short) || sizeof(*&entry->next) == sizeof(int) || sizeof(*&entry->next) == sizeof(long)))) __compiletime_assert_20(); } while (0); __asm__ __volatile__("dbar %0 " : : "I"(0b10010) : "memory"); do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_21(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*&entry->next) == sizeof(char) || sizeof(*&entry->next) == sizeof(short) || sizeof(*&entry->next) == sizeof(int) || sizeof(*&entry->next) == sizeof(long)) || sizeof(*&entry->next) == sizeof(long long))) __compiletime_assert_21(); } while (0); do { *(volatile typeof(*&entry->next) *)&(*&entry->next) = (entry); } while (0); } while (0); } while (0); } while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int list_empty_careful(const struct list_head *head)
{
 struct list_head *next = ({ typeof(*&head->next) ___p1 = ({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_22(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*&head->next) == sizeof(char) || sizeof(*&head->next) == sizeof(short) || sizeof(*&head->next) == sizeof(int) || sizeof(*&head->next) == sizeof(long)) || sizeof(*&head->next) == sizeof(long long))) __compiletime_assert_22(); } while (0); (*(const volatile typeof( _Generic((*&head->next), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (*&head->next))) *)&(*&head->next)); }); do { __attribute__((__noreturn__)) extern void __compiletime_assert_23(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&head->next) == sizeof(char) || sizeof(*&head->next) == sizeof(short) || sizeof(*&head->next) == sizeof(int) || sizeof(*&head->next) == sizeof(long)))) __compiletime_assert_23(); } while (0); __asm__ __volatile__("dbar %0 " : : "I"(0b10100) : "memory"); ___p1; });
 return list_is_head(next, head) && (next == ({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_24(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(head->prev) == sizeof(char) || sizeof(head->prev) == sizeof(short) || sizeof(head->prev) == sizeof(int) || sizeof(head->prev) == sizeof(long)) || sizeof(head->prev) == sizeof(long long))) __compiletime_assert_24(); } while (0); (*(const volatile typeof( _Generic((head->prev), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (head->prev))) *)&(head->prev)); }));
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void list_rotate_left(struct list_head *head)
{
 struct list_head *first;

 if (!list_empty(head)) {
  first = head->next;
  list_move_tail(first, head);
 }
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void list_rotate_to_front(struct list_head *list,
     struct list_head *head)
{





 list_move_tail(head, list);
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int list_is_singular(const struct list_head *head)
{
 return !list_empty(head) && (head->next == head->prev);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __list_cut_position(struct list_head *list,
  struct list_head *head, struct list_head *entry)
{
 struct list_head *new_first = entry->next;
 list->next = head->next;
 list->next->prev = list;
 list->prev = entry;
 entry->next = list;
 head->next = new_first;
 new_first->prev = head;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void list_cut_position(struct list_head *list,
  struct list_head *head, struct list_head *entry)
{
 if (list_empty(head))
  return;
 if (list_is_singular(head) && !list_is_head(entry, head) && (entry != head->next))
  return;
 if (list_is_head(entry, head))
  INIT_LIST_HEAD(list);
 else
  __list_cut_position(list, head, entry);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void list_cut_before(struct list_head *list,
       struct list_head *head,
       struct list_head *entry)
{
 if (head->next == entry) {
  INIT_LIST_HEAD(list);
  return;
 }
 list->next = head->next;
 list->next->prev = list;
 list->prev = entry->prev;
 list->prev->next = list;
 head->next = entry;
 entry->prev = head;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __list_splice(const struct list_head *list,
     struct list_head *prev,
     struct list_head *next)
{
 struct list_head *first = list->next;
 struct list_head *last = list->prev;

 first->prev = prev;
 prev->next = first;

 last->next = next;
 next->prev = last;
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void list_splice(const struct list_head *list,
    struct list_head *head)
{
 if (!list_empty(list))
  __list_splice(list, head, head->next);
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void list_splice_tail(struct list_head *list,
    struct list_head *head)
{
 if (!list_empty(list))
  __list_splice(list, head->prev, head);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void list_splice_init(struct list_head *list,
        struct list_head *head)
{
 if (!list_empty(list)) {
  __list_splice(list, head, head->next);
  INIT_LIST_HEAD(list);
 }
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void list_splice_tail_init(struct list_head *list,
      struct list_head *head)
{
 if (!list_empty(list)) {
  __list_splice(list, head->prev, head);
  INIT_LIST_HEAD(list);
 }
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) size_t list_count_nodes(struct list_head *head)
{
 struct list_head *pos;
 size_t count = 0;

 for (pos = (head)->next; !list_is_head(pos, (head)); pos = pos->next)
  count++;

 return count;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void INIT_HLIST_NODE(struct hlist_node *h)
{
 h->next = ((void *)0);
 h->pprev = ((void *)0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int hlist_unhashed(const struct hlist_node *h)
{
 return !h->pprev;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int hlist_unhashed_lockless(const struct hlist_node *h)
{
 return !({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_25(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(h->pprev) == sizeof(char) || sizeof(h->pprev) == sizeof(short) || sizeof(h->pprev) == sizeof(int) || sizeof(h->pprev) == sizeof(long)) || sizeof(h->pprev) == sizeof(long long))) __compiletime_assert_25(); } while (0); (*(const volatile typeof( _Generic((h->pprev), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (h->pprev))) *)&(h->pprev)); });
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int hlist_empty(const struct hlist_head *h)
{
 return !({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_26(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(h->first) == sizeof(char) || sizeof(h->first) == sizeof(short) || sizeof(h->first) == sizeof(int) || sizeof(h->first) == sizeof(long)) || sizeof(h->first) == sizeof(long long))) __compiletime_assert_26(); } while (0); (*(const volatile typeof( _Generic((h->first), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (h->first))) *)&(h->first)); });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __hlist_del(struct hlist_node *n)
{
 struct hlist_node *next = n->next;
 struct hlist_node **pprev = n->pprev;

 do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_27(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*pprev) == sizeof(char) || sizeof(*pprev) == sizeof(short) || sizeof(*pprev) == sizeof(int) || sizeof(*pprev) == sizeof(long)) || sizeof(*pprev) == sizeof(long long))) __compiletime_assert_27(); } while (0); do { *(volatile typeof(*pprev) *)&(*pprev) = (next); } while (0); } while (0);
 if (next)
  do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_28(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(next->pprev) == sizeof(char) || sizeof(next->pprev) == sizeof(short) || sizeof(next->pprev) == sizeof(int) || sizeof(next->pprev) == sizeof(long)) || sizeof(next->pprev) == sizeof(long long))) __compiletime_assert_28(); } while (0); do { *(volatile typeof(next->pprev) *)&(next->pprev) = (pprev); } while (0); } while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void hlist_del(struct hlist_node *n)
{
 __hlist_del(n);
 n->next = ((void *) 0x100 + 0);
 n->pprev = ((void *) 0x122 + 0);
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void hlist_del_init(struct hlist_node *n)
{
 if (!hlist_unhashed(n)) {
  __hlist_del(n);
  INIT_HLIST_NODE(n);
 }
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void hlist_add_head(struct hlist_node *n, struct hlist_head *h)
{
 struct hlist_node *first = h->first;
 do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_29(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(n->next) == sizeof(char) || sizeof(n->next) == sizeof(short) || sizeof(n->next) == sizeof(int) || sizeof(n->next) == sizeof(long)) || sizeof(n->next) == sizeof(long long))) __compiletime_assert_29(); } while (0); do { *(volatile typeof(n->next) *)&(n->next) = (first); } while (0); } while (0);
 if (first)
  do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_30(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(first->pprev) == sizeof(char) || sizeof(first->pprev) == sizeof(short) || sizeof(first->pprev) == sizeof(int) || sizeof(first->pprev) == sizeof(long)) || sizeof(first->pprev) == sizeof(long long))) __compiletime_assert_30(); } while (0); do { *(volatile typeof(first->pprev) *)&(first->pprev) = (&n->next); } while (0); } while (0);
 do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_31(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(h->first) == sizeof(char) || sizeof(h->first) == sizeof(short) || sizeof(h->first) == sizeof(int) || sizeof(h->first) == sizeof(long)) || sizeof(h->first) == sizeof(long long))) __compiletime_assert_31(); } while (0); do { *(volatile typeof(h->first) *)&(h->first) = (n); } while (0); } while (0);
 do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_32(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(n->pprev) == sizeof(char) || sizeof(n->pprev) == sizeof(short) || sizeof(n->pprev) == sizeof(int) || sizeof(n->pprev) == sizeof(long)) || sizeof(n->pprev) == sizeof(long long))) __compiletime_assert_32(); } while (0); do { *(volatile typeof(n->pprev) *)&(n->pprev) = (&h->first); } while (0); } while (0);
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void hlist_add_before(struct hlist_node *n,
        struct hlist_node *next)
{
 do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_33(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(n->pprev) == sizeof(char) || sizeof(n->pprev) == sizeof(short) || sizeof(n->pprev) == sizeof(int) || sizeof(n->pprev) == sizeof(long)) || sizeof(n->pprev) == sizeof(long long))) __compiletime_assert_33(); } while (0); do { *(volatile typeof(n->pprev) *)&(n->pprev) = (next->pprev); } while (0); } while (0);
 do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_34(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(n->next) == sizeof(char) || sizeof(n->next) == sizeof(short) || sizeof(n->next) == sizeof(int) || sizeof(n->next) == sizeof(long)) || sizeof(n->next) == sizeof(long long))) __compiletime_assert_34(); } while (0); do { *(volatile typeof(n->next) *)&(n->next) = (next); } while (0); } while (0);
 do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_35(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(next->pprev) == sizeof(char) || sizeof(next->pprev) == sizeof(short) || sizeof(next->pprev) == sizeof(int) || sizeof(next->pprev) == sizeof(long)) || sizeof(next->pprev) == sizeof(long long))) __compiletime_assert_35(); } while (0); do { *(volatile typeof(next->pprev) *)&(next->pprev) = (&n->next); } while (0); } while (0);
 do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_36(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*(n->pprev)) == sizeof(char) || sizeof(*(n->pprev)) == sizeof(short) || sizeof(*(n->pprev)) == sizeof(int) || sizeof(*(n->pprev)) == sizeof(long)) || sizeof(*(n->pprev)) == sizeof(long long))) __compiletime_assert_36(); } while (0); do { *(volatile typeof(*(n->pprev)) *)&(*(n->pprev)) = (n); } while (0); } while (0);
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void hlist_add_behind(struct hlist_node *n,
        struct hlist_node *prev)
{
 do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_37(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(n->next) == sizeof(char) || sizeof(n->next) == sizeof(short) || sizeof(n->next) == sizeof(int) || sizeof(n->next) == sizeof(long)) || sizeof(n->next) == sizeof(long long))) __compiletime_assert_37(); } while (0); do { *(volatile typeof(n->next) *)&(n->next) = (prev->next); } while (0); } while (0);
 do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_38(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(prev->next) == sizeof(char) || sizeof(prev->next) == sizeof(short) || sizeof(prev->next) == sizeof(int) || sizeof(prev->next) == sizeof(long)) || sizeof(prev->next) == sizeof(long long))) __compiletime_assert_38(); } while (0); do { *(volatile typeof(prev->next) *)&(prev->next) = (n); } while (0); } while (0);
 do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_39(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(n->pprev) == sizeof(char) || sizeof(n->pprev) == sizeof(short) || sizeof(n->pprev) == sizeof(int) || sizeof(n->pprev) == sizeof(long)) || sizeof(n->pprev) == sizeof(long long))) __compiletime_assert_39(); } while (0); do { *(volatile typeof(n->pprev) *)&(n->pprev) = (&prev->next); } while (0); } while (0);

 if (n->next)
  do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_40(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(n->next->pprev) == sizeof(char) || sizeof(n->next->pprev) == sizeof(short) || sizeof(n->next->pprev) == sizeof(int) || sizeof(n->next->pprev) == sizeof(long)) || sizeof(n->next->pprev) == sizeof(long long))) __compiletime_assert_40(); } while (0); do { *(volatile typeof(n->next->pprev) *)&(n->next->pprev) = (&n->next); } while (0); } while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void hlist_add_fake(struct hlist_node *n)
{
 n->pprev = &n->next;
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool hlist_fake(struct hlist_node *h)
{
 return h->pprev == &h->next;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool
hlist_is_singular_node(struct hlist_node *n, struct hlist_head *h)
{
 return !n->next && n->pprev == &h->first;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void hlist_move_list(struct hlist_head *old,
       struct hlist_head *new)
{
 new->first = old->first;
 if (new->first)
  new->first->pprev = &new->first;
 old->first = ((void *)0);
}

















struct warn_args;
struct pt_regs;

void __warn(const char *file, int line, void *caller, unsigned taint,
     struct pt_regs *regs, struct warn_args *args);




struct bug_entry {



 signed int bug_addr_disp;





 signed int file_disp;

 unsigned short line;

 unsigned short flags;
};
extern __attribute__((__format__(printf, 4, 5)))
void warn_slowpath_fmt(const char *file, const int line, unsigned taint,
         const char *fmt, ...);
extern __attribute__((__format__(printf, 1, 2))) void __warn_printk(const char *fmt, ...);



enum bug_trap_type {
 BUG_TRAP_TYPE_NONE = 0,
 BUG_TRAP_TYPE_WARN = 1,
 BUG_TRAP_TYPE_BUG = 2,
};

struct pt_regs;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int is_warning_bug(const struct bug_entry *bug)
{
 return bug->flags & (1 << 0);
}

void bug_get_file_line(struct bug_entry *bug, const char **file,
         unsigned int *line);

struct bug_entry *find_bug(unsigned long bugaddr);

enum bug_trap_type report_bug(unsigned long bug_addr, struct pt_regs *regs);


int is_valid_bugaddr(unsigned long addr);

void generic_bug_clear_once(void);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__warn_unused_result__)) bool check_data_corruption(bool v) { return v; }














static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) u32
__iter_div_u64_rem(u64 dividend, u32 divisor, u64 *remainder)
{
 u32 ret = 0;

 while (dividend >= divisor) {


  asm("" : "+rm"(dividend));

  dividend -= divisor;
  ret++;
 }

 *remainder = dividend;

 return ret;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u64 div_u64_rem(u64 dividend, u32 divisor, u32 *remainder)
{
 *remainder = dividend % divisor;
 return dividend / divisor;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) s64 div_s64_rem(s64 dividend, s32 divisor, s32 *remainder)
{
 *remainder = dividend % divisor;
 return dividend / divisor;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u64 div64_u64_rem(u64 dividend, u64 divisor, u64 *remainder)
{
 *remainder = dividend % divisor;
 return dividend / divisor;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u64 div64_u64(u64 dividend, u64 divisor)
{
 return dividend / divisor;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) s64 div64_s64(s64 dividend, s64 divisor)
{
 return dividend / divisor;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u64 div_u64(u64 dividend, u32 divisor)
{
 u32 remainder;
 return div_u64_rem(dividend, divisor, &remainder);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) s64 div_s64(s64 dividend, s32 divisor)
{
 s32 remainder;
 return div_s64_rem(dividend, divisor, &remainder);
}


u32 iter_div_u64_rem(u64 dividend, u32 divisor, u64 *remainder);





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u64 mul_u32_u32(u32 a, u32 b)
{
 return (u64)a * b;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) u64 mul_u64_u32_shr(u64 a, u32 mul, unsigned int shift)
{
 u32 ah, al;
 u64 ret;

 al = a;
 ah = a >> 32;

 ret = mul_u32_u32(al, mul) >> shift;
 if (ah)
  ret += mul_u32_u32(ah, mul) << (32 - shift);

 return ret;
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u64 mul_u64_u64_shr(u64 a, u64 b, unsigned int shift)
{
 union {
  u64 ll;
  struct {



   u32 low, high;

  } l;
 } rl, rm, rn, rh, a0, b0;
 u64 c;

 a0.ll = a;
 b0.ll = b;

 rl.ll = mul_u32_u32(a0.l.low, b0.l.low);
 rm.ll = mul_u32_u32(a0.l.low, b0.l.high);
 rn.ll = mul_u32_u32(a0.l.high, b0.l.low);
 rh.ll = mul_u32_u32(a0.l.high, b0.l.high);






 rl.l.high = c = (u64)rl.l.high + rm.l.low + rn.l.low;
 rh.l.low = c = (c >> 32) + rm.l.high + rn.l.high + rh.l.low;
 rh.l.high = (c >> 32) + rh.l.high;





 if (shift == 0)
  return rl.ll;
 if (shift < 64)
  return (rl.ll >> shift) | (rh.ll << (64 - shift));
 return rh.ll >> (shift & 63);
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u64 mul_s64_u64_shr(s64 a, u64 b, unsigned int shift)
{
 u64 ret;





 ret = mul_u64_u64_shr(__builtin_choose_expr( __builtin_types_compatible_p(typeof(a), signed long long) || __builtin_types_compatible_p(typeof(a), unsigned long long), ({ signed long long __x = (a); __x < 0 ? -__x : __x; }), __builtin_choose_expr( __builtin_types_compatible_p(typeof(a), signed long) || __builtin_types_compatible_p(typeof(a), unsigned long), ({ signed long __x = (a); __x < 0 ? -__x : __x; }), __builtin_choose_expr( __builtin_types_compatible_p(typeof(a), signed int) || __builtin_types_compatible_p(typeof(a), unsigned int), ({ signed int __x = (a); __x < 0 ? -__x : __x; }), __builtin_choose_expr( __builtin_types_compatible_p(typeof(a), signed short) || __builtin_types_compatible_p(typeof(a), unsigned short), ({ signed short __x = (a); __x < 0 ? -__x : __x; }), __builtin_choose_expr( __builtin_types_compatible_p(typeof(a), signed char) || __builtin_types_compatible_p(typeof(a), unsigned char), ({ signed char __x = (a); __x < 0 ? -__x : __x; }), __builtin_choose_expr( __builtin_types_compatible_p(typeof(a), char), (char)({ signed char __x = (a); __x<0?-__x:__x; }), ((void)0))))))), b, shift);

 if (a < 0)
  ret = -((s64) ret);

 return ret;
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u64 mul_u64_u32_div(u64 a, u32 mul, u32 divisor)
{
 union {
  u64 ll;
  struct {



   u32 low, high;

  } l;
 } u, rl, rh;

 u.ll = a;
 rl.ll = mul_u32_u32(u.l.low, mul);
 rh.ll = mul_u32_u32(u.l.high, mul) + rl.l.high;


 rl.l.high = ({ uint32_t __base = (divisor); uint32_t __rem; __rem = ((uint64_t)(rh.ll)) % __base; (rh.ll) = ((uint64_t)(rh.ll)) / __base; __rem; });


 ({ uint32_t __base = (divisor); uint32_t __rem; __rem = ((uint64_t)(rl.ll)) % __base; (rl.ll) = ((uint64_t)(rl.ll)) / __base; __rem; });

 rl.l.high = rh.l.low;
 return rl.ll;
}


u64 mul_u64_u64_div_u64(u64 a, u64 mul, u64 div);

typedef __s64 time64_t;
typedef __u64 timeu64_t;












struct __kernel_timespec {
 __kernel_time64_t tv_sec;
 long long tv_nsec;
};

struct __kernel_itimerspec {
 struct __kernel_timespec it_interval;
 struct __kernel_timespec it_value;
};
struct __kernel_old_timeval {
 __kernel_long_t tv_sec;
 __kernel_long_t tv_usec;
};


struct __kernel_old_timespec {
 __kernel_old_time_t tv_sec;
 long tv_nsec;
};

struct __kernel_old_itimerval {
 struct __kernel_old_timeval it_interval;
 struct __kernel_old_timeval it_value;
};

struct __kernel_sock_timeval {
 __s64 tv_sec;
 __s64 tv_usec;
};
struct timezone {
 int tz_minuteswest;
 int tz_dsttime;
};

struct timespec64 {
 time64_t tv_sec;
 long tv_nsec;
};

struct itimerspec64 {
 struct timespec64 it_interval;
 struct timespec64 it_value;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int timespec64_equal(const struct timespec64 *a,
       const struct timespec64 *b)
{
 return (a->tv_sec == b->tv_sec) && (a->tv_nsec == b->tv_nsec);
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int timespec64_compare(const struct timespec64 *lhs, const struct timespec64 *rhs)
{
 if (lhs->tv_sec < rhs->tv_sec)
  return -1;
 if (lhs->tv_sec > rhs->tv_sec)
  return 1;
 return lhs->tv_nsec - rhs->tv_nsec;
}

extern void set_normalized_timespec64(struct timespec64 *ts, time64_t sec, s64 nsec);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) struct timespec64 timespec64_add(struct timespec64 lhs,
      struct timespec64 rhs)
{
 struct timespec64 ts_delta;
 set_normalized_timespec64(&ts_delta, lhs.tv_sec + rhs.tv_sec,
    lhs.tv_nsec + rhs.tv_nsec);
 return ts_delta;
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) struct timespec64 timespec64_sub(struct timespec64 lhs,
      struct timespec64 rhs)
{
 struct timespec64 ts_delta;
 set_normalized_timespec64(&ts_delta, lhs.tv_sec - rhs.tv_sec,
    lhs.tv_nsec - rhs.tv_nsec);
 return ts_delta;
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool timespec64_valid(const struct timespec64 *ts)
{

 if (ts->tv_sec < 0)
  return false;

 if ((unsigned long)ts->tv_nsec >= 1000000000L)
  return false;
 return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool timespec64_valid_strict(const struct timespec64 *ts)
{
 if (!timespec64_valid(ts))
  return false;

 if ((unsigned long long)ts->tv_sec >= (((s64)~((u64)1 << 63)) / 1000000000L))
  return false;
 return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool timespec64_valid_settod(const struct timespec64 *ts)
{
 if (!timespec64_valid(ts))
  return false;

 if ((unsigned long long)ts->tv_sec >= ((((s64)~((u64)1 << 63)) / 1000000000L) - (30LL * 365 * 24 *3600)))
  return false;
 return true;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) s64 timespec64_to_ns(const struct timespec64 *ts)
{

 if (ts->tv_sec >= (((s64)~((u64)1 << 63)) / 1000000000L))
  return ((s64)~((u64)1 << 63));

 if (ts->tv_sec <= ((-((s64)~((u64)1 << 63)) - 1) / 1000000000L))
  return (-((s64)~((u64)1 << 63)) - 1);

 return ((s64) ts->tv_sec * 1000000000L) + ts->tv_nsec;
}







extern struct timespec64 ns_to_timespec64(s64 nsec);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void timespec64_add_ns(struct timespec64 *a, u64 ns)
{
 a->tv_sec += __iter_div_u64_rem(a->tv_nsec + ns, 1000000000L, &ns);
 a->tv_nsec = ns;
}





extern struct timespec64 timespec64_add_safe(const struct timespec64 lhs,
      const struct timespec64 rhs);

struct timespec;
struct old_timespec32;
struct pollfd;

enum timespec_type {
 TT_NONE = 0,
 TT_NATIVE = 1,
 TT_COMPAT = 2,
};




struct restart_block {
 unsigned long arch_data;
 long (*fn)(struct restart_block *);
 union {

  struct {
   u32 *uaddr;
   u32 val;
   u32 flags;
   u32 bitset;
   u64 time;
   u32 *uaddr2;
  } futex;

  struct {
   clockid_t clockid;
   enum timespec_type type;
   union {
    struct __kernel_timespec *rmtp;
    struct old_timespec32 *compat_rmtp;
   };
   u64 expires;
  } nanosleep;

  struct {
   struct pollfd *ufds;
   int nfds;
   int has_timeout;
   unsigned long tv_sec;
   unsigned long tv_nsec;
  } poll;
 };
};

extern long do_no_restart_syscall(struct restart_block *parm);








enum {
 BAD_STACK = -1,
 NOT_STACK = 0,
 GOOD_FRAME,
 GOOD_STACK,
};


enum syscall_work_bit {
 SYSCALL_WORK_BIT_SECCOMP,
 SYSCALL_WORK_BIT_SYSCALL_TRACEPOINT,
 SYSCALL_WORK_BIT_SYSCALL_TRACE,
 SYSCALL_WORK_BIT_SYSCALL_EMU,
 SYSCALL_WORK_BIT_SYSCALL_AUDIT,
 SYSCALL_WORK_BIT_SYSCALL_USER_DISPATCH,
 SYSCALL_WORK_BIT_SYSCALL_EXIT_TRAP,
};
















unsigned long _find_next_bit(const unsigned long *addr1, unsigned long nbits,
    unsigned long start);
unsigned long _find_next_and_bit(const unsigned long *addr1, const unsigned long *addr2,
     unsigned long nbits, unsigned long start);
unsigned long _find_next_andnot_bit(const unsigned long *addr1, const unsigned long *addr2,
     unsigned long nbits, unsigned long start);
unsigned long _find_next_or_bit(const unsigned long *addr1, const unsigned long *addr2,
     unsigned long nbits, unsigned long start);
unsigned long _find_next_and_andnot_bit(const unsigned long *addr1, const unsigned long *addr2,
     const unsigned long *addr3, unsigned long nbits,
     unsigned long start);
unsigned long _find_next_zero_bit(const unsigned long *addr, unsigned long nbits,
      unsigned long start);
extern unsigned long _find_first_bit(const unsigned long *addr, unsigned long size);
unsigned long __find_nth_bit(const unsigned long *addr, unsigned long size, unsigned long n);
unsigned long __find_nth_and_bit(const unsigned long *addr1, const unsigned long *addr2,
    unsigned long size, unsigned long n);
unsigned long __find_nth_andnot_bit(const unsigned long *addr1, const unsigned long *addr2,
     unsigned long size, unsigned long n);
unsigned long __find_nth_and_andnot_bit(const unsigned long *addr1, const unsigned long *addr2,
     const unsigned long *addr3, unsigned long size,
     unsigned long n);
extern unsigned long _find_first_and_bit(const unsigned long *addr1,
      const unsigned long *addr2, unsigned long size);
extern unsigned long _find_first_zero_bit(const unsigned long *addr, unsigned long size);
extern unsigned long _find_last_bit(const unsigned long *addr, unsigned long size);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0)))
unsigned long find_next_bit(const unsigned long *addr, unsigned long size,
       unsigned long offset)
{
 if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
  unsigned long val;

  if (__builtin_expect(!!(offset >= size), 0))
   return size;

  val = *addr & ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((offset) > (size - 1)) * 0l)) : (int *)8))), (offset) > (size - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (offset)) + 1) & (~(((0UL))) >> (64 - 1 - (size - 1)))));
  return val ? __ffs(val) : size;
 }

 return _find_next_bit(addr, size, offset);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0)))
unsigned long find_next_and_bit(const unsigned long *addr1,
  const unsigned long *addr2, unsigned long size,
  unsigned long offset)
{
 if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
  unsigned long val;

  if (__builtin_expect(!!(offset >= size), 0))
   return size;

  val = *addr1 & *addr2 & ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((offset) > (size - 1)) * 0l)) : (int *)8))), (offset) > (size - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (offset)) + 1) & (~(((0UL))) >> (64 - 1 - (size - 1)))));
  return val ? __ffs(val) : size;
 }

 return _find_next_and_bit(addr1, addr2, size, offset);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0)))
unsigned long find_next_andnot_bit(const unsigned long *addr1,
  const unsigned long *addr2, unsigned long size,
  unsigned long offset)
{
 if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
  unsigned long val;

  if (__builtin_expect(!!(offset >= size), 0))
   return size;

  val = *addr1 & ~*addr2 & ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((offset) > (size - 1)) * 0l)) : (int *)8))), (offset) > (size - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (offset)) + 1) & (~(((0UL))) >> (64 - 1 - (size - 1)))));
  return val ? __ffs(val) : size;
 }

 return _find_next_andnot_bit(addr1, addr2, size, offset);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0)))
unsigned long find_next_or_bit(const unsigned long *addr1,
  const unsigned long *addr2, unsigned long size,
  unsigned long offset)
{
 if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
  unsigned long val;

  if (__builtin_expect(!!(offset >= size), 0))
   return size;

  val = (*addr1 | *addr2) & ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((offset) > (size - 1)) * 0l)) : (int *)8))), (offset) > (size - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (offset)) + 1) & (~(((0UL))) >> (64 - 1 - (size - 1)))));
  return val ? __ffs(val) : size;
 }

 return _find_next_or_bit(addr1, addr2, size, offset);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__))
unsigned long find_next_and_andnot_bit(const unsigned long *addr1,
       const unsigned long *addr2,
       const unsigned long *addr3,
       unsigned long size,
       unsigned long offset)
{
 if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
  unsigned long val;

  if (__builtin_expect(!!(offset >= size), 0))
   return size;

  val = *addr1 & *addr2 & ~*addr3 & ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((offset) > (size - 1)) * 0l)) : (int *)8))), (offset) > (size - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (offset)) + 1) & (~(((0UL))) >> (64 - 1 - (size - 1)))));
  return val ? __ffs(val) : size;
 }

 return _find_next_and_andnot_bit(addr1, addr2, addr3, size, offset);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0)))
unsigned long find_next_zero_bit(const unsigned long *addr, unsigned long size,
     unsigned long offset)
{
 if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
  unsigned long val;

  if (__builtin_expect(!!(offset >= size), 0))
   return size;

  val = *addr | ~((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((offset) > (size - 1)) * 0l)) : (int *)8))), (offset) > (size - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (offset)) + 1) & (~(((0UL))) >> (64 - 1 - (size - 1)))));
  return val == ~0UL ? size : __ffs(~(val));
 }

 return _find_next_zero_bit(addr, size, offset);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0)))
unsigned long find_first_bit(const unsigned long *addr, unsigned long size)
{
 if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
  unsigned long val = *addr & ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((0) > (size - 1)) * 0l)) : (int *)8))), (0) > (size - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (0)) + 1) & (~(((0UL))) >> (64 - 1 - (size - 1)))));

  return val ? __ffs(val) : size;
 }

 return _find_first_bit(addr, size);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0)))
unsigned long find_nth_bit(const unsigned long *addr, unsigned long size, unsigned long n)
{
 if (n >= size)
  return size;

 if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
  unsigned long val = *addr & ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((0) > (size - 1)) * 0l)) : (int *)8))), (0) > (size - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (0)) + 1) & (~(((0UL))) >> (64 - 1 - (size - 1)))));

  return val ? fns(val, n) : size;
 }

 return __find_nth_bit(addr, size, n);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0)))
unsigned long find_nth_and_bit(const unsigned long *addr1, const unsigned long *addr2,
    unsigned long size, unsigned long n)
{
 if (n >= size)
  return size;

 if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
  unsigned long val = *addr1 & *addr2 & ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((0) > (size - 1)) * 0l)) : (int *)8))), (0) > (size - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (0)) + 1) & (~(((0UL))) >> (64 - 1 - (size - 1)))));

  return val ? fns(val, n) : size;
 }

 return __find_nth_and_bit(addr1, addr2, size, n);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0)))
unsigned long find_nth_andnot_bit(const unsigned long *addr1, const unsigned long *addr2,
    unsigned long size, unsigned long n)
{
 if (n >= size)
  return size;

 if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
  unsigned long val = *addr1 & (~*addr2) & ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((0) > (size - 1)) * 0l)) : (int *)8))), (0) > (size - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (0)) + 1) & (~(((0UL))) >> (64 - 1 - (size - 1)))));

  return val ? fns(val, n) : size;
 }

 return __find_nth_andnot_bit(addr1, addr2, size, n);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__))
unsigned long find_nth_and_andnot_bit(const unsigned long *addr1,
     const unsigned long *addr2,
     const unsigned long *addr3,
     unsigned long size, unsigned long n)
{
 if (n >= size)
  return size;

 if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
  unsigned long val = *addr1 & *addr2 & (~*addr3) & ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((0) > (size - 1)) * 0l)) : (int *)8))), (0) > (size - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (0)) + 1) & (~(((0UL))) >> (64 - 1 - (size - 1)))));

  return val ? fns(val, n) : size;
 }

 return __find_nth_and_andnot_bit(addr1, addr2, addr3, size, n);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0)))
unsigned long find_first_and_bit(const unsigned long *addr1,
     const unsigned long *addr2,
     unsigned long size)
{
 if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
  unsigned long val = *addr1 & *addr2 & ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((0) > (size - 1)) * 0l)) : (int *)8))), (0) > (size - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (0)) + 1) & (~(((0UL))) >> (64 - 1 - (size - 1)))));

  return val ? __ffs(val) : size;
 }

 return _find_first_and_bit(addr1, addr2, size);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0)))
unsigned long find_first_zero_bit(const unsigned long *addr, unsigned long size)
{
 if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
  unsigned long val = *addr | ~((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((0) > (size - 1)) * 0l)) : (int *)8))), (0) > (size - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (0)) + 1) & (~(((0UL))) >> (64 - 1 - (size - 1)))));

  return val == ~0UL ? size : __ffs(~(val));
 }

 return _find_first_zero_bit(addr, size);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0)))
unsigned long find_last_bit(const unsigned long *addr, unsigned long size)
{
 if ((__builtin_constant_p(size) && (size) <= 64 && (size) > 0)) {
  unsigned long val = *addr & ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((0) > (size - 1)) * 0l)) : (int *)8))), (0) > (size - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (0)) + 1) & (~(((0UL))) >> (64 - 1 - (size - 1)))));

  return val ? __fls(val) : size;
 }

 return _find_last_bit(addr, size);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0)))
unsigned long find_next_and_bit_wrap(const unsigned long *addr1,
     const unsigned long *addr2,
     unsigned long size, unsigned long offset)
{
 unsigned long bit = find_next_and_bit(addr1, addr2, size, offset);

 if (bit < size)
  return bit;

 bit = find_first_and_bit(addr1, addr2, offset);
 return bit < offset ? bit : size;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0)))
unsigned long find_next_bit_wrap(const unsigned long *addr,
     unsigned long size, unsigned long offset)
{
 unsigned long bit = find_next_bit(addr, size, offset);

 if (bit < size)
  return bit;

 bit = find_first_bit(addr, offset);
 return bit < offset ? bit : size;
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0)))
unsigned long __for_each_wrap(const unsigned long *bitmap, unsigned long size,
     unsigned long start, unsigned long n)
{
 unsigned long bit;


 if (n > start) {

  bit = find_next_bit(bitmap, size, n);
  if (bit < size)
   return bit;


  n = 0;
 }


 bit = find_next_bit(bitmap, start, n);
 return bit < start ? bit : size;
}
extern unsigned long find_next_clump8(unsigned long *clump,
          const unsigned long *addr,
          unsigned long size, unsigned long offset);






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long find_next_zero_bit_le(const void *addr,
  unsigned long size, unsigned long offset)
{
 return find_next_zero_bit(addr, size, offset);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long find_next_bit_le(const void *addr,
  unsigned long size, unsigned long offset)
{
 return find_next_bit(addr, size, offset);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long find_first_zero_bit_le(const void *addr,
  unsigned long size)
{
 return find_first_zero_bit(addr, size);
}


extern char *strndup_user(const char *, long);
extern void *memdup_user(const void *, size_t);
extern void *vmemdup_user(const void *, size_t);
extern void *memdup_user_nul(const void *, size_t);












extern void *memset(void *__s, int __c, size_t __count);


extern void *memcpy(void *__to, __const__ void *__from, size_t __n);


extern void *memmove(void *__dest, __const__ void *__src, size_t __n);


extern char * strcpy(char *,const char *);


extern char * strncpy(char *,const char *, __kernel_size_t);


size_t strlcpy(char *, const char *, size_t);


ssize_t strscpy(char *, const char *, size_t);



ssize_t strscpy_pad(char *dest, const char *src, size_t count);


extern char * strcat(char *, const char *);


extern char * strncat(char *, const char *, __kernel_size_t);


extern size_t strlcat(char *, const char *, __kernel_size_t);


extern int strcmp(const char *,const char *);


extern int strncmp(const char *,const char *,__kernel_size_t);


extern int strcasecmp(const char *s1, const char *s2);


extern int strncasecmp(const char *s1, const char *s2, size_t n);


extern char * strchr(const char *,int);


extern char * strchrnul(const char *,int);

extern char * strnchrnul(const char *, size_t, int);

extern char * strnchr(const char *, size_t, int);


extern char * strrchr(const char *,int);

extern char * __attribute__((__warn_unused_result__)) skip_spaces(const char *);

extern char *strim(char *);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__warn_unused_result__)) char *strstrip(char *str)
{
 return strim(str);
}


extern char * strstr(const char *, const char *);


extern char * strnstr(const char *, const char *, size_t);


extern __kernel_size_t strlen(const char *);


extern __kernel_size_t strnlen(const char *,__kernel_size_t);


extern char * strpbrk(const char *,const char *);


extern char * strsep(char **,const char *);


extern __kernel_size_t strspn(const char *,const char *);


extern __kernel_size_t strcspn(const char *,const char *);







extern void *memset16(uint16_t *, uint16_t, __kernel_size_t);



extern void *memset32(uint32_t *, uint32_t, __kernel_size_t);



extern void *memset64(uint64_t *, uint64_t, __kernel_size_t);


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void *memset_l(unsigned long *p, unsigned long v,
  __kernel_size_t n)
{
 if (64 == 32)
  return memset32((uint32_t *)p, v, n);
 else
  return memset64((uint64_t *)p, v, n);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void *memset_p(void **p, void *v, __kernel_size_t n)
{
 if (64 == 32)
  return memset32((uint32_t *)p, (uintptr_t)v, n);
 else
  return memset64((uint64_t *)p, (uintptr_t)v, n);
}

extern void **__memcat_p(void **a, void **b);
extern void * memscan(void *,int,__kernel_size_t);


extern int memcmp(const void *,const void *,__kernel_size_t);


extern int bcmp(const void *,const void *,__kernel_size_t);


extern void * memchr(const void *,int,__kernel_size_t);


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void memcpy_flushcache(void *dst, const void *src, size_t cnt)
{
 memcpy(dst, src, cnt);
}


void *memchr_inv(const void *s, int c, size_t n);
char *strreplace(char *str, char old, char new);

extern void kfree_const(const void *x);

extern char *kstrdup(const char *s, gfp_t gfp) __attribute__((__malloc__));
extern const char *kstrdup_const(const char *s, gfp_t gfp);
extern char *kstrndup(const char *s, size_t len, gfp_t gfp);
extern void *kmemdup(const void *src, size_t len, gfp_t gfp) __attribute__((__alloc_size__(2)));
extern void *kvmemdup(const void *src, size_t len, gfp_t gfp) __attribute__((__alloc_size__(2)));
extern char *kmemdup_nul(const char *s, size_t len, gfp_t gfp);

extern char **argv_split(gfp_t gfp, const char *str, int *argcp);
extern void argv_free(char **argv);

extern bool sysfs_streq(const char *s1, const char *s2);
int match_string(const char * const *array, size_t n, const char *string);
int __sysfs_match_string(const char * const *array, size_t n, const char *s);
int vbin_printf(u32 *bin_buf, size_t size, const char *fmt, va_list args);
int bstr_printf(char *buf, size_t size, const char *fmt, const u32 *bin_buf);
int bprintf(u32 *bin_buf, size_t size, const char *fmt, ...) __attribute__((__format__(printf, 3, 4)));


extern ssize_t memory_read_from_buffer(void *to, size_t count, loff_t *ppos,
           const void *from, size_t available);

int ptr_to_hashval(const void *ptr, unsigned long *hashval_out);






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool strstarts(const char *str, const char *prefix)
{
 return strncmp(str, prefix, strlen(prefix)) == 0;
}

size_t memweight(const void *ptr, size_t bytes);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void memzero_explicit(void *s, size_t count)
{
 memset(s, 0, count);
 __asm__ __volatile__("": :"r"(s) :"memory");
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) const char *kbasename(const char *path)
{
 const char *tail = strrchr(path, '/');
 return tail ? tail + 1 : path;
}


void fortify_panic(const char *name) __attribute__((__noreturn__)) __attribute__((__cold__));
void __read_overflow(void) __attribute__((__error__("detected read beyond size of object (1st parameter)")));
void __read_overflow2(void) __attribute__((__error__("detected read beyond size of object (2nd parameter)")));
void __read_overflow2_field(size_t avail, size_t wanted) __attribute__((__warning__("detected read beyond size of field (2nd parameter); maybe use struct_group()?")));
void __write_overflow(void) __attribute__((__error__("detected write beyond size of object (1st parameter)")));
void __write_overflow_field(size_t avail, size_t wanted) __attribute__((__warning__("detected write beyond size of field (1st parameter); maybe use struct_group()?")));
extern inline __attribute__((__gnu_inline__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __attribute__((__gnu_inline__)) __attribute__((__overloadable__)) __attribute__((__diagnose_as_builtin__(__builtin_strncpy, 1, 2, 3)))
char *strncpy(char * const __attribute__((__pass_dynamic_object_size__(1))) p, const char *q, __kernel_size_t size)
{
 const size_t p_size = __builtin_dynamic_object_size(p, 1);

 if (( __builtin_constant_p((p_size) < (size)) && (p_size) < (size) ))
  __write_overflow();
 if (p_size < size)
  fortify_panic(__func__);
 return __builtin_strncpy(p, q, size);
}

extern __kernel_size_t __real_strnlen(const char *, __kernel_size_t) __asm__("strnlen");
extern inline __attribute__((__gnu_inline__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __attribute__((__gnu_inline__)) __attribute__((__overloadable__)) __kernel_size_t strnlen(const char * const __attribute__((__pass_dynamic_object_size__(1))) p, __kernel_size_t maxlen)
{
 const size_t p_size = __builtin_dynamic_object_size(p, 1);
 const size_t p_len = ({ char *__p = (char *)(p); size_t __ret = (~(size_t)0); const size_t __p_size = __builtin_dynamic_object_size(p, 1); if (__p_size != (~(size_t)0) && __builtin_constant_p(*__p)) { size_t __p_len = __p_size - 1; if (__builtin_constant_p(__p[__p_len]) && __p[__p_len] == '\0') __ret = __builtin_strlen(__p); } __ret; });
 size_t ret;


 if (__builtin_constant_p(maxlen) && p_len != (~(size_t)0)) {

  if (maxlen >= p_size)
   return p_len;
 }


 ret = __real_strnlen(p, maxlen < p_size ? maxlen : p_size);
 if (p_size <= ret && maxlen != ret)
  fortify_panic(__func__);
 return ret;
}
extern inline __attribute__((__gnu_inline__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __attribute__((__gnu_inline__)) __attribute__((__overloadable__)) __attribute__((__diagnose_as_builtin__(__builtin_strlen, 1)))
__kernel_size_t __fortify_strlen(const char * const __attribute__((__pass_dynamic_object_size__(1))) p)
{
 const size_t p_size = __builtin_dynamic_object_size(p, 1);
 __kernel_size_t ret;


 if (p_size == (~(size_t)0))
  return __builtin_strlen(p);
 ret = strnlen(p, p_size);
 if (p_size <= ret)
  fortify_panic(__func__);
 return ret;
}


#include "strlcpy.c"

extern ssize_t __real_strscpy(char *, const char *, size_t) __asm__("strscpy");
extern inline __attribute__((__gnu_inline__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __attribute__((__gnu_inline__)) __attribute__((__overloadable__)) ssize_t strscpy(char * const __attribute__((__pass_dynamic_object_size__(1))) p, const char * const __attribute__((__pass_dynamic_object_size__(1))) q, size_t size)
{

 const size_t p_size = __builtin_dynamic_object_size(p, 1);
 const size_t q_size = __builtin_dynamic_object_size(q, 1);
 size_t len;


 if (p_size == (~(size_t)0) && q_size == (~(size_t)0))
  return __real_strscpy(p, q, size);





 if (( __builtin_constant_p((p_size) < (size)) && (p_size) < (size) ))
  __write_overflow();


 if (( __builtin_constant_p((p_size) < ((~(size_t)0))) && (p_size) < ((~(size_t)0)) )) {
  len = ({ char *__p = (char *)(q); size_t __ret = (~(size_t)0); const size_t __p_size = __builtin_dynamic_object_size(q, 1); if (__p_size != (~(size_t)0) && __builtin_constant_p(*__p)) { size_t __p_len = __p_size - 1; if (__builtin_constant_p(__p[__p_len]) && __p[__p_len] == '\0') __ret = __builtin_strlen(__p); } __ret; });

  if (len < (~(size_t)0) && ( __builtin_constant_p((len) < (size)) && (len) < (size) )) {
   __builtin_memcpy(p, q, len + 1);
   return len;
  }
 }





 len = strnlen(q, size);





 len = len == size ? size : len + 1;





 if (len > p_size)
  fortify_panic(__func__);






 return __real_strscpy(p, q, len);
}


extern size_t __real_strlcat(char *p, const char *q, size_t avail) __asm__("strlcat");
extern inline __attribute__((__gnu_inline__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __attribute__((__gnu_inline__)) __attribute__((__overloadable__))
size_t strlcat(char * const __attribute__((__pass_dynamic_object_size__(1))) p, const char * const __attribute__((__pass_dynamic_object_size__(1))) q, size_t avail)
{
 const size_t p_size = __builtin_dynamic_object_size(p, 1);
 const size_t q_size = __builtin_dynamic_object_size(q, 1);
 size_t p_len, copy_len;
 size_t actual, wanted;


 if (p_size == (~(size_t)0) && q_size == (~(size_t)0))
  return __real_strlcat(p, q, avail);

 p_len = strnlen(p, avail);
 copy_len = __builtin_choose_expr((sizeof(int) == sizeof(*(8 ? ((void *)((long)(__builtin_strlen(q)) * 0l)) : (int *)8))), __builtin_strlen(q), __fortify_strlen(q));
 wanted = actual = p_len + copy_len;


 if (avail <= p_len)
  return wanted;


 if (p_size <= p_len)
  fortify_panic(__func__);

 if (actual >= avail) {
  copy_len = avail - p_len - 1;
  actual = p_len + copy_len;
 }


 if (p_size <= actual)
  fortify_panic(__func__);
 __builtin_memcpy(p + p_len, q, copy_len);
 p[actual] = '\0';

 return wanted;
}
extern inline __attribute__((__gnu_inline__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __attribute__((__gnu_inline__)) __attribute__((__overloadable__)) __attribute__((__diagnose_as_builtin__(__builtin_strcat, 1, 2)))
char *strcat(char * const __attribute__((__pass_dynamic_object_size__(1))) p, const char *q)
{
 const size_t p_size = __builtin_dynamic_object_size(p, 1);

 if (strlcat(p, q, p_size) >= p_size)
  fortify_panic(__func__);
 return p;
}
extern inline __attribute__((__gnu_inline__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __attribute__((__gnu_inline__)) __attribute__((__overloadable__)) __attribute__((__diagnose_as_builtin__(__builtin_strncat, 1, 2, 3)))
char *strncat(char * const __attribute__((__pass_dynamic_object_size__(1))) p, const char * const __attribute__((__pass_dynamic_object_size__(1))) q, __kernel_size_t count)
{
 const size_t p_size = __builtin_dynamic_object_size(p, 1);
 const size_t q_size = __builtin_dynamic_object_size(q, 1);
 size_t p_len, copy_len;

 if (p_size == (~(size_t)0) && q_size == (~(size_t)0))
  return __builtin_strncat(p, q, count);
 p_len = __builtin_choose_expr((sizeof(int) == sizeof(*(8 ? ((void *)((long)(__builtin_strlen(p)) * 0l)) : (int *)8))), __builtin_strlen(p), __fortify_strlen(p));
 copy_len = strnlen(q, count);
 if (p_size < p_len + copy_len + 1)
  fortify_panic(__func__);
 __builtin_memcpy(p + p_len, q, copy_len);
 p[p_len + copy_len] = '\0';
 return p;
}

extern inline __attribute__((__gnu_inline__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __attribute__((__gnu_inline__)) __attribute__((__overloadable__)) void fortify_memset_chk(__kernel_size_t size,
      const size_t p_size,
      const size_t p_size_field)
{
 if (__builtin_constant_p(size)) {







  if (( __builtin_constant_p((p_size_field) < (p_size)) && (p_size_field) < (p_size) ) &&
      ( __builtin_constant_p((p_size) < (size)) && (p_size) < (size) ))
   __write_overflow();


  if (( __builtin_constant_p((p_size_field) < (size)) && (p_size_field) < (size) ))
   __write_overflow_field(p_size_field, size);
 }
 if (p_size != (~(size_t)0) && p_size < size)
  fortify_panic("memset");
}
extern inline __attribute__((__gnu_inline__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __attribute__((__gnu_inline__)) __attribute__((__overloadable__)) bool fortify_memcpy_chk(__kernel_size_t size,
      const size_t p_size,
      const size_t q_size,
      const size_t p_size_field,
      const size_t q_size_field,
      const char *func)
{
 if (__builtin_constant_p(size)) {







  if (( __builtin_constant_p((p_size_field) < (p_size)) && (p_size_field) < (p_size) ) &&
      ( __builtin_constant_p((p_size) < (size)) && (p_size) < (size) ))
   __write_overflow();
  if (( __builtin_constant_p((q_size_field) < (q_size)) && (q_size_field) < (q_size) ) &&
      ( __builtin_constant_p((q_size) < (size)) && (q_size) < (size) ))
   __read_overflow2();


  if (( __builtin_constant_p((p_size_field) < (size)) && (p_size_field) < (size) ))
   __write_overflow_field(p_size_field, size);





  if ((1 ||
       ( __builtin_constant_p((p_size_field) < (size)) && (p_size_field) < (size) )) &&
      ( __builtin_constant_p((q_size_field) < (size)) && (q_size_field) < (size) ))
   __read_overflow2_field(q_size_field, size);
 }
 if ((p_size != (~(size_t)0) && p_size < size) ||
     (q_size != (~(size_t)0) && q_size < size))
  fortify_panic(func);
 if (p_size_field != 0 && p_size_field != (~(size_t)0) &&
     p_size != p_size_field && p_size_field < size)
  return true;

 return false;
}
extern void *__real_memscan(void *, int, __kernel_size_t) __asm__("memscan");
extern inline __attribute__((__gnu_inline__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __attribute__((__gnu_inline__)) __attribute__((__overloadable__)) void *memscan(void * const __attribute__((__pass_dynamic_object_size__(0))) p, int c, __kernel_size_t size)
{
 const size_t p_size = __builtin_dynamic_object_size(p, 0);

 if (( __builtin_constant_p((p_size) < (size)) && (p_size) < (size) ))
  __read_overflow();
 if (p_size < size)
  fortify_panic(__func__);
 return __real_memscan(p, c, size);
}

extern inline __attribute__((__gnu_inline__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __attribute__((__gnu_inline__)) __attribute__((__overloadable__)) __attribute__((__diagnose_as_builtin__(__builtin_memcmp, 1, 2, 3)))
int memcmp(const void * const __attribute__((__pass_dynamic_object_size__(0))) p, const void * const __attribute__((__pass_dynamic_object_size__(0))) q, __kernel_size_t size)
{
 const size_t p_size = __builtin_dynamic_object_size(p, 0);
 const size_t q_size = __builtin_dynamic_object_size(q, 0);

 if (__builtin_constant_p(size)) {
  if (( __builtin_constant_p((p_size) < (size)) && (p_size) < (size) ))
   __read_overflow();
  if (( __builtin_constant_p((q_size) < (size)) && (q_size) < (size) ))
   __read_overflow2();
 }
 if (p_size < size || q_size < size)
  fortify_panic(__func__);
 return __builtin_memcmp(p, q, size);
}

extern inline __attribute__((__gnu_inline__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __attribute__((__gnu_inline__)) __attribute__((__overloadable__)) __attribute__((__diagnose_as_builtin__(__builtin_memchr, 1, 2, 3)))
void *memchr(const void * const __attribute__((__pass_dynamic_object_size__(0))) p, int c, __kernel_size_t size)
{
 const size_t p_size = __builtin_dynamic_object_size(p, 0);

 if (( __builtin_constant_p((p_size) < (size)) && (p_size) < (size) ))
  __read_overflow();
 if (p_size < size)
  fortify_panic(__func__);
 return __builtin_memchr(p, c, size);
}

void *__real_memchr_inv(const void *s, int c, size_t n) __asm__("memchr_inv");
extern inline __attribute__((__gnu_inline__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __attribute__((__gnu_inline__)) __attribute__((__overloadable__)) void *memchr_inv(const void * const __attribute__((__pass_dynamic_object_size__(0))) p, int c, size_t size)
{
 const size_t p_size = __builtin_dynamic_object_size(p, 0);

 if (( __builtin_constant_p((p_size) < (size)) && (p_size) < (size) ))
  __read_overflow();
 if (p_size < size)
  fortify_panic(__func__);
 return __real_memchr_inv(p, c, size);
}

extern void *__real_kmemdup(const void *src, size_t len, gfp_t gfp) __asm__("kmemdup")
            __attribute__((__alloc_size__(2)));
extern inline __attribute__((__gnu_inline__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __attribute__((__gnu_inline__)) __attribute__((__overloadable__)) void *kmemdup(const void * const __attribute__((__pass_dynamic_object_size__(0))) p, size_t size, gfp_t gfp)
{
 const size_t p_size = __builtin_dynamic_object_size(p, 0);

 if (( __builtin_constant_p((p_size) < (size)) && (p_size) < (size) ))
  __read_overflow();
 if (p_size < size)
  fortify_panic(__func__);
 return __real_kmemdup(p, size, gfp);
}
extern inline __attribute__((__gnu_inline__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __attribute__((__gnu_inline__)) __attribute__((__overloadable__)) __attribute__((__diagnose_as_builtin__(__builtin_strcpy, 1, 2)))
char *strcpy(char * const __attribute__((__pass_dynamic_object_size__(1))) p, const char * const __attribute__((__pass_dynamic_object_size__(1))) q)
{
 const size_t p_size = __builtin_dynamic_object_size(p, 1);
 const size_t q_size = __builtin_dynamic_object_size(q, 1);
 size_t size;


 if (__builtin_constant_p(p_size) &&
     __builtin_constant_p(q_size) &&
     p_size == (~(size_t)0) && q_size == (~(size_t)0))
  return __builtin_strcpy(p, q);
 size = __builtin_choose_expr((sizeof(int) == sizeof(*(8 ? ((void *)((long)(__builtin_strlen(q)) * 0l)) : (int *)8))), __builtin_strlen(q), __fortify_strlen(q)) + 1;

 if (( __builtin_constant_p((p_size) < (size)) && (p_size) < (size) ))
  __write_overflow();

 if (p_size < size)
  fortify_panic(__func__);
 __builtin_memcpy(p, q, size);
 return p;
}






void memcpy_and_pad(void *dest, size_t dest_len, const void *src, size_t count,
      int pad);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) size_t str_has_prefix(const char *str, const char *prefix)
{
 size_t len = __builtin_choose_expr((sizeof(int) == sizeof(*(8 ? ((void *)((long)(__builtin_strlen(prefix)) * 0l)) : (int *)8))), __builtin_strlen(prefix), __fortify_strlen(prefix));
 return strncmp(str, prefix, len) == 0 ? len : 0;
}


struct device;
unsigned long *bitmap_alloc(unsigned int nbits, gfp_t flags);
unsigned long *bitmap_zalloc(unsigned int nbits, gfp_t flags);
unsigned long *bitmap_alloc_node(unsigned int nbits, gfp_t flags, int node);
unsigned long *bitmap_zalloc_node(unsigned int nbits, gfp_t flags, int node);
void bitmap_free(const unsigned long *bitmap);


unsigned long *devm_bitmap_alloc(struct device *dev,
     unsigned int nbits, gfp_t flags);
unsigned long *devm_bitmap_zalloc(struct device *dev,
      unsigned int nbits, gfp_t flags);





bool __bitmap_equal(const unsigned long *bitmap1,
      const unsigned long *bitmap2, unsigned int nbits);
bool __attribute__((__pure__)) __bitmap_or_equal(const unsigned long *src1,
         const unsigned long *src2,
         const unsigned long *src3,
         unsigned int nbits);
void __bitmap_complement(unsigned long *dst, const unsigned long *src,
    unsigned int nbits);
void __bitmap_shift_right(unsigned long *dst, const unsigned long *src,
     unsigned int shift, unsigned int nbits);
void __bitmap_shift_left(unsigned long *dst, const unsigned long *src,
    unsigned int shift, unsigned int nbits);
void bitmap_cut(unsigned long *dst, const unsigned long *src,
  unsigned int first, unsigned int cut, unsigned int nbits);
bool __bitmap_and(unsigned long *dst, const unsigned long *bitmap1,
   const unsigned long *bitmap2, unsigned int nbits);
void __bitmap_or(unsigned long *dst, const unsigned long *bitmap1,
   const unsigned long *bitmap2, unsigned int nbits);
void __bitmap_xor(unsigned long *dst, const unsigned long *bitmap1,
    const unsigned long *bitmap2, unsigned int nbits);
bool __bitmap_andnot(unsigned long *dst, const unsigned long *bitmap1,
      const unsigned long *bitmap2, unsigned int nbits);
void __bitmap_replace(unsigned long *dst,
        const unsigned long *old, const unsigned long *new,
        const unsigned long *mask, unsigned int nbits);
bool __bitmap_intersects(const unsigned long *bitmap1,
    const unsigned long *bitmap2, unsigned int nbits);
bool __bitmap_subset(const unsigned long *bitmap1,
       const unsigned long *bitmap2, unsigned int nbits);
unsigned int __bitmap_weight(const unsigned long *bitmap, unsigned int nbits);
unsigned int __bitmap_weight_and(const unsigned long *bitmap1,
     const unsigned long *bitmap2, unsigned int nbits);
void __bitmap_set(unsigned long *map, unsigned int start, int len);
void __bitmap_clear(unsigned long *map, unsigned int start, int len);

unsigned long bitmap_find_next_zero_area_off(unsigned long *map,
          unsigned long size,
          unsigned long start,
          unsigned int nr,
          unsigned long align_mask,
          unsigned long align_offset);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long
bitmap_find_next_zero_area(unsigned long *map,
      unsigned long size,
      unsigned long start,
      unsigned int nr,
      unsigned long align_mask)
{
 return bitmap_find_next_zero_area_off(map, size, start, nr,
           align_mask, 0);
}

int bitmap_parse(const char *buf, unsigned int buflen,
   unsigned long *dst, int nbits);
int bitmap_parse_user(const char *ubuf, unsigned int ulen,
   unsigned long *dst, int nbits);
int bitmap_parselist(const char *buf, unsigned long *maskp,
   int nmaskbits);
int bitmap_parselist_user(const char *ubuf, unsigned int ulen,
   unsigned long *dst, int nbits);
void bitmap_remap(unsigned long *dst, const unsigned long *src,
  const unsigned long *old, const unsigned long *new, unsigned int nbits);
int bitmap_bitremap(int oldbit,
  const unsigned long *old, const unsigned long *new, int bits);
void bitmap_onto(unsigned long *dst, const unsigned long *orig,
  const unsigned long *relmap, unsigned int bits);
void bitmap_fold(unsigned long *dst, const unsigned long *orig,
  unsigned int sz, unsigned int nbits);
int bitmap_find_free_region(unsigned long *bitmap, unsigned int bits, int order);
void bitmap_release_region(unsigned long *bitmap, unsigned int pos, int order);
int bitmap_allocate_region(unsigned long *bitmap, unsigned int pos, int order);






int bitmap_print_to_pagebuf(bool list, char *buf,
       const unsigned long *maskp, int nmaskbits);

extern int bitmap_print_bitmask_to_buf(char *buf, const unsigned long *maskp,
          int nmaskbits, loff_t off, size_t count);

extern int bitmap_print_list_to_buf(char *buf, const unsigned long *maskp,
          int nmaskbits, loff_t off, size_t count);




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void bitmap_zero(unsigned long *dst, unsigned int nbits)
{
 unsigned int len = (((nbits) + ((sizeof(long) * 8)) - 1) / ((sizeof(long) * 8))) * sizeof(unsigned long);

 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  *dst = 0;
 else
  ({ size_t __fortify_size = (size_t)(len); fortify_memset_chk(__fortify_size, __builtin_dynamic_object_size(dst, 0), __builtin_dynamic_object_size(dst, 1)), __builtin_memset(dst, 0, __fortify_size); });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void bitmap_fill(unsigned long *dst, unsigned int nbits)
{
 unsigned int len = (((nbits) + ((sizeof(long) * 8)) - 1) / ((sizeof(long) * 8))) * sizeof(unsigned long);

 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  *dst = ~0UL;
 else
  ({ size_t __fortify_size = (size_t)(len); fortify_memset_chk(__fortify_size, __builtin_dynamic_object_size(dst, 0), __builtin_dynamic_object_size(dst, 1)), __builtin_memset(dst, 0xff, __fortify_size); });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void bitmap_copy(unsigned long *dst, const unsigned long *src,
   unsigned int nbits)
{
 unsigned int len = (((nbits) + ((sizeof(long) * 8)) - 1) / ((sizeof(long) * 8))) * sizeof(unsigned long);

 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  *dst = *src;
 else
  ({ const size_t __fortify_size = (size_t)(len); const size_t __p_size = (__builtin_dynamic_object_size(dst, 0)); const size_t __q_size = (__builtin_dynamic_object_size(src, 0)); const size_t __p_size_field = (__builtin_dynamic_object_size(dst, 1)); const size_t __q_size_field = (__builtin_dynamic_object_size(src, 1)); ({ bool __ret_do_once = !!(fortify_memcpy_chk(__fortify_size, __p_size, __q_size, __p_size_field, __q_size_field, "memcpy")); if (({ static bool __attribute__((__section__(".data.once"))) __already_done; bool __ret_cond = !!(__ret_do_once); bool __ret_once = false; if (__builtin_expect(!!(__ret_cond && !__already_done), 0)) { __already_done = true; __ret_once = true; } __builtin_expect(!!(__ret_once), 0); })) ({ int __ret_warn_on = !!(1); if (__builtin_expect(!!(__ret_warn_on), 0)) do { do { } while(0); __warn_printk("memcpy" ": detected field-spanning write (size %zu) of single %s (size %zu)\n", __fortify_size, "field \"" "dst" "\" at " "include/linux/bitmap.h" ":" "268", __p_size_field); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/bitmap.h\"; .popsection; .long 10002b - .; .short 268; .short (1 << 0)|((1 << 3) | ((9) << 8)); .popsection; 10001: break 1");; do { } while(0); } while (0); do { } while(0); } while (0); __builtin_expect(!!(__ret_warn_on), 0); }); __builtin_expect(!!(__ret_do_once), 0); }); __builtin_memcpy(dst, src, __fortify_size); });
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void bitmap_copy_clear_tail(unsigned long *dst,
  const unsigned long *src, unsigned int nbits)
{
 bitmap_copy(dst, src, nbits);
 if (nbits % 64)
  dst[nbits / 64] &= (~0UL >> (-(nbits) & (64 - 1)));
}
void bitmap_from_arr32(unsigned long *bitmap, const u32 *buf,
       unsigned int nbits);
void bitmap_to_arr32(u32 *buf, const unsigned long *bitmap,
       unsigned int nbits);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool bitmap_and(unsigned long *dst, const unsigned long *src1,
   const unsigned long *src2, unsigned int nbits)
{
 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  return (*dst = *src1 & *src2 & (~0UL >> (-(nbits) & (64 - 1)))) != 0;
 return __bitmap_and(dst, src1, src2, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void bitmap_or(unsigned long *dst, const unsigned long *src1,
   const unsigned long *src2, unsigned int nbits)
{
 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  *dst = *src1 | *src2;
 else
  __bitmap_or(dst, src1, src2, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void bitmap_xor(unsigned long *dst, const unsigned long *src1,
   const unsigned long *src2, unsigned int nbits)
{
 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  *dst = *src1 ^ *src2;
 else
  __bitmap_xor(dst, src1, src2, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool bitmap_andnot(unsigned long *dst, const unsigned long *src1,
   const unsigned long *src2, unsigned int nbits)
{
 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  return (*dst = *src1 & ~(*src2) & (~0UL >> (-(nbits) & (64 - 1)))) != 0;
 return __bitmap_andnot(dst, src1, src2, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void bitmap_complement(unsigned long *dst, const unsigned long *src,
   unsigned int nbits)
{
 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  *dst = ~(*src);
 else
  __bitmap_complement(dst, src, nbits);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool bitmap_equal(const unsigned long *src1,
    const unsigned long *src2, unsigned int nbits)
{
 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  return !((*src1 ^ *src2) & (~0UL >> (-(nbits) & (64 - 1))));
 if (__builtin_constant_p(nbits & (8 - 1)) &&
     (((nbits) & ((typeof(nbits))(8) - 1)) == 0))
  return !memcmp(src1, src2, nbits / 8);
 return __bitmap_equal(src1, src2, nbits);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool bitmap_or_equal(const unsigned long *src1,
       const unsigned long *src2,
       const unsigned long *src3,
       unsigned int nbits)
{
 if (!(__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  return __bitmap_or_equal(src1, src2, src3, nbits);

 return !(((*src1 | *src2) ^ *src3) & (~0UL >> (-(nbits) & (64 - 1))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool bitmap_intersects(const unsigned long *src1,
         const unsigned long *src2,
         unsigned int nbits)
{
 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  return ((*src1 & *src2) & (~0UL >> (-(nbits) & (64 - 1)))) != 0;
 else
  return __bitmap_intersects(src1, src2, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool bitmap_subset(const unsigned long *src1,
     const unsigned long *src2, unsigned int nbits)
{
 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  return ! ((*src1 & ~(*src2)) & (~0UL >> (-(nbits) & (64 - 1))));
 else
  return __bitmap_subset(src1, src2, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool bitmap_empty(const unsigned long *src, unsigned nbits)
{
 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  return ! (*src & (~0UL >> (-(nbits) & (64 - 1))));

 return find_first_bit(src, nbits) == nbits;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool bitmap_full(const unsigned long *src, unsigned int nbits)
{
 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  return ! (~(*src) & (~0UL >> (-(nbits) & (64 - 1))));

 return find_first_zero_bit(src, nbits) == nbits;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__))
unsigned int bitmap_weight(const unsigned long *src, unsigned int nbits)
{
 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  return hweight_long(*src & (~0UL >> (-(nbits) & (64 - 1))));
 return __bitmap_weight(src, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__))
unsigned long bitmap_weight_and(const unsigned long *src1,
    const unsigned long *src2, unsigned int nbits)
{
 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  return hweight_long(*src1 & *src2 & (~0UL >> (-(nbits) & (64 - 1))));
 return __bitmap_weight_and(src1, src2, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void bitmap_set(unsigned long *map, unsigned int start,
  unsigned int nbits)
{
 if (__builtin_constant_p(nbits) && nbits == 1)
  ((__builtin_constant_p(start) && __builtin_constant_p((uintptr_t)(map) != (uintptr_t)((void *)0)) && (uintptr_t)(map) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(map))) ? generic___set_bit(start, map) : generic___set_bit(start, map));
 else if ((__builtin_constant_p(start + nbits) && (start + nbits) <= 64 && (start + nbits) > 0))
  *map |= ((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((start) > (start + nbits - 1)) * 0l)) : (int *)8))), (start) > (start + nbits - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (start)) + 1) & (~(((0UL))) >> (64 - 1 - (start + nbits - 1)))));
 else if (__builtin_constant_p(start & (8 - 1)) &&
   (((start) & ((typeof(start))(8) - 1)) == 0) &&
   __builtin_constant_p(nbits & (8 - 1)) &&
   (((nbits) & ((typeof(nbits))(8) - 1)) == 0))
  ({ size_t __fortify_size = (size_t)(nbits / 8); fortify_memset_chk(__fortify_size, __builtin_dynamic_object_size((char *)map + start / 8, 0), __builtin_dynamic_object_size((char *)map + start / 8, 1)), __builtin_memset((char *)map + start / 8, 0xff, __fortify_size); });
 else
  __bitmap_set(map, start, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void bitmap_clear(unsigned long *map, unsigned int start,
  unsigned int nbits)
{
 if (__builtin_constant_p(nbits) && nbits == 1)
  ((__builtin_constant_p(start) && __builtin_constant_p((uintptr_t)(map) != (uintptr_t)((void *)0)) && (uintptr_t)(map) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(map))) ? generic___clear_bit(start, map) : generic___clear_bit(start, map));
 else if ((__builtin_constant_p(start + nbits) && (start + nbits) <= 64 && (start + nbits) > 0))
  *map &= ~((((int)(sizeof(struct { int:(-!!(__builtin_choose_expr( (sizeof(int) == sizeof(*(8 ? ((void *)((long)((start) > (start + nbits - 1)) * 0l)) : (int *)8))), (start) > (start + nbits - 1), 0))); })))) + (((~(((0UL)))) - ((((1UL))) << (start)) + 1) & (~(((0UL))) >> (64 - 1 - (start + nbits - 1)))));
 else if (__builtin_constant_p(start & (8 - 1)) &&
   (((start) & ((typeof(start))(8) - 1)) == 0) &&
   __builtin_constant_p(nbits & (8 - 1)) &&
   (((nbits) & ((typeof(nbits))(8) - 1)) == 0))
  ({ size_t __fortify_size = (size_t)(nbits / 8); fortify_memset_chk(__fortify_size, __builtin_dynamic_object_size((char *)map + start / 8, 0), __builtin_dynamic_object_size((char *)map + start / 8, 1)), __builtin_memset((char *)map + start / 8, 0, __fortify_size); });
 else
  __bitmap_clear(map, start, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void bitmap_shift_right(unsigned long *dst, const unsigned long *src,
    unsigned int shift, unsigned int nbits)
{
 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  *dst = (*src & (~0UL >> (-(nbits) & (64 - 1)))) >> shift;
 else
  __bitmap_shift_right(dst, src, shift, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void bitmap_shift_left(unsigned long *dst, const unsigned long *src,
    unsigned int shift, unsigned int nbits)
{
 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  *dst = (*src << shift) & (~0UL >> (-(nbits) & (64 - 1)));
 else
  __bitmap_shift_left(dst, src, shift, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void bitmap_replace(unsigned long *dst,
      const unsigned long *old,
      const unsigned long *new,
      const unsigned long *mask,
      unsigned int nbits)
{
 if ((__builtin_constant_p(nbits) && (nbits) <= 64 && (nbits) > 0))
  *dst = (*old & ~(*mask)) | (*new & *mask);
 else
  __bitmap_replace(dst, old, new, mask, nbits);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void bitmap_next_set_region(unsigned long *bitmap,
       unsigned int *rs, unsigned int *re,
       unsigned int end)
{
 *rs = find_next_bit(bitmap, end, *rs);
 *re = find_next_zero_bit(bitmap, end, *rs + 1);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void bitmap_from_u64(unsigned long *dst, u64 mask)
{
 bitmap_copy_clear_tail((unsigned long *)(dst), (const unsigned long *)(&mask), (64));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long bitmap_get_value8(const unsigned long *map,
           unsigned long start)
{
 const size_t index = ((start) / 64);
 const unsigned long offset = start % 64;

 return (map[index] >> offset) & 0xFF;
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void bitmap_set_value8(unsigned long *map, unsigned long value,
         unsigned long start)
{
 const size_t index = ((start) / 64);
 const unsigned long offset = start % 64;

 map[index] &= ~(0xFFUL << offset);
 map[index] |= value << offset;
}



int memory_add_physaddr_to_nid(u64 addr);


int numa_map_to_online_node(int node);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int phys_to_target_node(u64 start)
{
 ({ bool __ret_do_once = !!(true); if (({ static bool __attribute__((__section__(".data.once"))) __already_done; bool __ret_cond = !!(__ret_do_once); bool __ret_once = false; if (__builtin_expect(!!(__ret_cond && !__already_done), 0)) { __already_done = true; __ret_once = true; } __builtin_expect(!!(__ret_once), 0); })) ({ do { if (__builtin_constant_p("\001" "6" "Unknown target node for memory at 0x%llx, assuming node 0\n") && __builtin_constant_p(((void *)0))) { static const struct pi_entry _entry __attribute__((__used__)) = { .fmt = __builtin_constant_p("\001" "6" "Unknown target node for memory at 0x%llx, assuming node 0\n") ? ("\001" "6" "Unknown target node for memory at 0x%llx, assuming node 0\n") : ((void *)0), .func = __func__, .file = "include/linux/numa.h", .line = 42, .level = __builtin_constant_p(((void *)0)) ? (((void *)0)) : ((void *)0), .subsys_fmt_prefix = ((void *)0), }; static const struct pi_entry *_entry_ptr __attribute__((__used__)) __attribute__((__section__(".printk_index"))) = &_entry; } } while (0); _printk("\001" "6" "Unknown target node for memory at 0x%llx, assuming node 0\n", start); }); __builtin_expect(!!(__ret_do_once), 0); });

 return 0;
}


typedef struct cpumask { unsigned long bits[(((64) + ((sizeof(long) * 8)) - 1) / ((sizeof(long) * 8)))]; } cpumask_t;
extern unsigned int nr_cpu_ids;


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void set_nr_cpu_ids(unsigned int nr)
{



 nr_cpu_ids = nr;

}
extern struct cpumask __cpu_possible_mask;
extern struct cpumask __cpu_online_mask;
extern struct cpumask __cpu_present_mask;
extern struct cpumask __cpu_active_mask;
extern struct cpumask __cpu_dying_mask;






extern atomic_t __num_online_cpus;

extern cpumask_t cpus_booted_once_mask;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void cpu_max_bits_warn(unsigned int cpu, unsigned int bits)
{

 ({ int __ret_warn_on = !!(cpu >= bits); if (__builtin_expect(!!(__ret_warn_on), 0)) do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/cpumask.h\"; .popsection; .long 10002b - .; .short 143; .short (1 << 0)|((1 << 1) | ((9) << 8)); .popsection; 10001: break 1");; do { } while(0); } while (0); __builtin_expect(!!(__ret_warn_on), 0); });

}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) unsigned int cpumask_check(unsigned int cpu)
{
 cpu_max_bits_warn(cpu, ((unsigned int)64));
 return cpu;
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned int cpumask_first(const struct cpumask *srcp)
{
 return find_first_bit(((srcp)->bits), ((unsigned int)64));
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned int cpumask_first_zero(const struct cpumask *srcp)
{
 return find_first_zero_bit(((srcp)->bits), ((unsigned int)64));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0)))
unsigned int cpumask_first_and(const struct cpumask *srcp1, const struct cpumask *srcp2)
{
 return find_first_and_bit(((srcp1)->bits), ((srcp2)->bits), ((unsigned int)64));
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned int cpumask_last(const struct cpumask *srcp)
{
 return find_last_bit(((srcp)->bits), ((unsigned int)64));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0)))
unsigned int cpumask_next(int n, const struct cpumask *srcp)
{

 if (n != -1)
  cpumask_check(n);
 return find_next_bit(((srcp)->bits), ((unsigned int)64), n + 1);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned int cpumask_next_zero(int n, const struct cpumask *srcp)
{

 if (n != -1)
  cpumask_check(n);
 return find_next_zero_bit(((srcp)->bits), ((unsigned int)64), n+1);
}
unsigned int cpumask_local_spread(unsigned int i, int node);
unsigned int cpumask_any_and_distribute(const struct cpumask *src1p,
          const struct cpumask *src2p);
unsigned int cpumask_any_distribute(const struct cpumask *srcp);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0)))
unsigned int cpumask_next_and(int n, const struct cpumask *src1p,
       const struct cpumask *src2p)
{

 if (n != -1)
  cpumask_check(n);
 return find_next_and_bit(((src1p)->bits), ((src2p)->bits),
  ((unsigned int)64), n + 1);
}
unsigned int __attribute__((__pure__)) cpumask_next_wrap(int n, const struct cpumask *mask, int start, bool wrap);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0)))
unsigned int cpumask_any_but(const struct cpumask *mask, unsigned int cpu)
{
 unsigned int i;

 cpumask_check(cpu);
 for ((i) = 0; (i) = find_next_bit((((mask)->bits)), (((unsigned int)64)), (i)), (i) < (((unsigned int)64)); (i)++)
  if (i != cpu)
   break;
 return i;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned int cpumask_nth(unsigned int cpu, const struct cpumask *srcp)
{
 return find_nth_bit(((srcp)->bits), ((unsigned int)64), cpumask_check(cpu));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0)))
unsigned int cpumask_nth_and(unsigned int cpu, const struct cpumask *srcp1,
       const struct cpumask *srcp2)
{
 return find_nth_and_bit(((srcp1)->bits), ((srcp2)->bits),
    ((unsigned int)64), cpumask_check(cpu));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0)))
unsigned int cpumask_nth_andnot(unsigned int cpu, const struct cpumask *srcp1,
       const struct cpumask *srcp2)
{
 return find_nth_andnot_bit(((srcp1)->bits), ((srcp2)->bits),
    ((unsigned int)64), cpumask_check(cpu));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__))
unsigned int cpumask_nth_and_andnot(unsigned int cpu, const struct cpumask *srcp1,
       const struct cpumask *srcp2,
       const struct cpumask *srcp3)
{
 return find_nth_and_andnot_bit(((srcp1)->bits),
     ((srcp2)->bits),
     ((srcp3)->bits),
     ((unsigned int)64), cpumask_check(cpu));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void cpumask_set_cpu(unsigned int cpu, struct cpumask *dstp)
{
 set_bit(cpumask_check(cpu), ((dstp)->bits));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __cpumask_set_cpu(unsigned int cpu, struct cpumask *dstp)
{
 ((__builtin_constant_p(cpumask_check(cpu)) && __builtin_constant_p((uintptr_t)(((dstp)->bits)) != (uintptr_t)((void *)0)) && (uintptr_t)(((dstp)->bits)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(((dstp)->bits)))) ? generic___set_bit(cpumask_check(cpu), ((dstp)->bits)) : generic___set_bit(cpumask_check(cpu), ((dstp)->bits)));
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void cpumask_clear_cpu(int cpu, struct cpumask *dstp)
{
 clear_bit(cpumask_check(cpu), ((dstp)->bits));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __cpumask_clear_cpu(int cpu, struct cpumask *dstp)
{
 ((__builtin_constant_p(cpumask_check(cpu)) && __builtin_constant_p((uintptr_t)(((dstp)->bits)) != (uintptr_t)((void *)0)) && (uintptr_t)(((dstp)->bits)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(((dstp)->bits)))) ? generic___clear_bit(cpumask_check(cpu), ((dstp)->bits)) : generic___clear_bit(cpumask_check(cpu), ((dstp)->bits)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool cpumask_test_cpu(int cpu, const struct cpumask *cpumask)
{
 return ((__builtin_constant_p(cpumask_check(cpu)) && __builtin_constant_p((uintptr_t)((((cpumask))->bits)) != (uintptr_t)((void *)0)) && (uintptr_t)((((cpumask))->bits)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)((((cpumask))->bits)))) ? const_test_bit(cpumask_check(cpu), (((cpumask))->bits)) : generic_test_bit(cpumask_check(cpu), (((cpumask))->bits)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool cpumask_test_and_set_cpu(int cpu, struct cpumask *cpumask)
{
 return test_and_set_bit(cpumask_check(cpu), ((cpumask)->bits));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool cpumask_test_and_clear_cpu(int cpu, struct cpumask *cpumask)
{
 return test_and_clear_bit(cpumask_check(cpu), ((cpumask)->bits));
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void cpumask_setall(struct cpumask *dstp)
{
 if ((__builtin_constant_p(((unsigned int)64)) && (((unsigned int)64)) <= 64 && (((unsigned int)64)) > 0)) {
  ((dstp)->bits)[0] = (~0UL >> (-(nr_cpu_ids) & (64 - 1)));
  return;
 }
 bitmap_fill(((dstp)->bits), nr_cpu_ids);
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void cpumask_clear(struct cpumask *dstp)
{
 bitmap_zero(((dstp)->bits), ((unsigned int)64));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool cpumask_and(struct cpumask *dstp,
          const struct cpumask *src1p,
          const struct cpumask *src2p)
{
 return bitmap_and(((dstp)->bits), ((src1p)->bits),
           ((src2p)->bits), ((unsigned int)64));
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void cpumask_or(struct cpumask *dstp, const struct cpumask *src1p,
         const struct cpumask *src2p)
{
 bitmap_or(((dstp)->bits), ((src1p)->bits),
          ((src2p)->bits), ((unsigned int)64));
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void cpumask_xor(struct cpumask *dstp,
          const struct cpumask *src1p,
          const struct cpumask *src2p)
{
 bitmap_xor(((dstp)->bits), ((src1p)->bits),
           ((src2p)->bits), ((unsigned int)64));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool cpumask_andnot(struct cpumask *dstp,
      const struct cpumask *src1p,
      const struct cpumask *src2p)
{
 return bitmap_andnot(((dstp)->bits), ((src1p)->bits),
       ((src2p)->bits), ((unsigned int)64));
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool cpumask_equal(const struct cpumask *src1p,
    const struct cpumask *src2p)
{
 return bitmap_equal(((src1p)->bits), ((src2p)->bits),
       ((unsigned int)64));
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool cpumask_or_equal(const struct cpumask *src1p,
        const struct cpumask *src2p,
        const struct cpumask *src3p)
{
 return bitmap_or_equal(((src1p)->bits), ((src2p)->bits),
          ((src3p)->bits), ((unsigned int)64));
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool cpumask_intersects(const struct cpumask *src1p,
         const struct cpumask *src2p)
{
 return bitmap_intersects(((src1p)->bits), ((src2p)->bits),
            ((unsigned int)64));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool cpumask_subset(const struct cpumask *src1p,
     const struct cpumask *src2p)
{
 return bitmap_subset(((src1p)->bits), ((src2p)->bits),
        ((unsigned int)64));
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool cpumask_empty(const struct cpumask *srcp)
{
 return bitmap_empty(((srcp)->bits), ((unsigned int)64));
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool cpumask_full(const struct cpumask *srcp)
{
 return bitmap_full(((srcp)->bits), nr_cpu_ids);
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned int cpumask_weight(const struct cpumask *srcp)
{
 return bitmap_weight(((srcp)->bits), ((unsigned int)64));
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned int cpumask_weight_and(const struct cpumask *srcp1,
      const struct cpumask *srcp2)
{
 return bitmap_weight_and(((srcp1)->bits), ((srcp2)->bits), ((unsigned int)64));
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void cpumask_shift_right(struct cpumask *dstp,
           const struct cpumask *srcp, int n)
{
 bitmap_shift_right(((dstp)->bits), ((srcp)->bits), n,
            ((unsigned int)64));
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void cpumask_shift_left(struct cpumask *dstp,
          const struct cpumask *srcp, int n)
{
 bitmap_shift_left(((dstp)->bits), ((srcp)->bits), n,
           nr_cpu_ids);
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void cpumask_copy(struct cpumask *dstp,
    const struct cpumask *srcp)
{
 bitmap_copy(((dstp)->bits), ((srcp)->bits), ((unsigned int)64));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int cpumask_parse_user(const char *buf, int len,
         struct cpumask *dstp)
{
 return bitmap_parse_user(buf, len, ((dstp)->bits), nr_cpu_ids);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int cpumask_parselist_user(const char *buf, int len,
         struct cpumask *dstp)
{
 return bitmap_parselist_user(buf, len, ((dstp)->bits),
         nr_cpu_ids);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int cpumask_parse(const char *buf, struct cpumask *dstp)
{
 return bitmap_parse(buf, (~0U), ((dstp)->bits), nr_cpu_ids);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int cpulist_parse(const char *buf, struct cpumask *dstp)
{
 return bitmap_parselist(buf, ((dstp)->bits), nr_cpu_ids);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned int cpumask_size(void)
{
 return (((((unsigned int)64)) + ((sizeof(long) * 8)) - 1) / ((sizeof(long) * 8))) * sizeof(long);
}
typedef struct cpumask *cpumask_var_t;




bool alloc_cpumask_var_node(cpumask_var_t *mask, gfp_t flags, int node);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0)))
bool zalloc_cpumask_var_node(cpumask_var_t *mask, gfp_t flags, int node)
{
 return alloc_cpumask_var_node(mask, flags | (( gfp_t)0x100u), node);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0)))
bool alloc_cpumask_var(cpumask_var_t *mask, gfp_t flags)
{
 return alloc_cpumask_var_node(mask, flags, (-1));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0)))
bool zalloc_cpumask_var(cpumask_var_t *mask, gfp_t flags)
{
 return alloc_cpumask_var(mask, flags | (( gfp_t)0x100u));
}

void alloc_bootmem_cpumask_var(cpumask_var_t *mask);
void free_cpumask_var(cpumask_var_t mask);
void free_bootmem_cpumask_var(cpumask_var_t mask);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool cpumask_available(cpumask_var_t mask)
{
 return mask != ((void *)0);
}
extern const unsigned long cpu_all_bits[(((64) + ((sizeof(long) * 8)) - 1) / ((sizeof(long) * 8)))];
void init_cpu_present(const struct cpumask *src);
void init_cpu_possible(const struct cpumask *src);
void init_cpu_online(const struct cpumask *src);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void reset_cpu_possible_mask(void)
{
 bitmap_zero(((&__cpu_possible_mask)->bits), 64);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void
set_cpu_possible(unsigned int cpu, bool possible)
{
 if (possible)
  cpumask_set_cpu(cpu, &__cpu_possible_mask);
 else
  cpumask_clear_cpu(cpu, &__cpu_possible_mask);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void
set_cpu_present(unsigned int cpu, bool present)
{
 if (present)
  cpumask_set_cpu(cpu, &__cpu_present_mask);
 else
  cpumask_clear_cpu(cpu, &__cpu_present_mask);
}

void set_cpu_online(unsigned int cpu, bool online);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void
set_cpu_active(unsigned int cpu, bool active)
{
 if (active)
  cpumask_set_cpu(cpu, &__cpu_active_mask);
 else
  cpumask_clear_cpu(cpu, &__cpu_active_mask);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void
set_cpu_dying(unsigned int cpu, bool dying)
{
 if (dying)
  cpumask_set_cpu(cpu, &__cpu_dying_mask);
 else
  cpumask_clear_cpu(cpu, &__cpu_dying_mask);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int __check_is_bitmap(const unsigned long *bitmap)
{
 return 1;
}
extern const unsigned long
 cpu_bit_bitmap[64 +1][(((64) + ((sizeof(long) * 8)) - 1) / ((sizeof(long) * 8)))];

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) const struct cpumask *get_cpu_mask(unsigned int cpu)
{
 const unsigned long *p = cpu_bit_bitmap[1 + cpu % 64];
 p -= cpu / 64;
 return ((struct cpumask *)(1 ? (p) : (void *)sizeof(__check_is_bitmap(p))));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) unsigned int num_online_cpus(void)
{
 return raw_atomic_read(&__num_online_cpus);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool cpu_online(unsigned int cpu)
{
 return cpumask_test_cpu(cpu, ((const struct cpumask *)&__cpu_online_mask));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool cpu_possible(unsigned int cpu)
{
 return cpumask_test_cpu(cpu, ((const struct cpumask *)&__cpu_possible_mask));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool cpu_present(unsigned int cpu)
{
 return cpumask_test_cpu(cpu, ((const struct cpumask *)&__cpu_present_mask));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool cpu_active(unsigned int cpu)
{
 return cpumask_test_cpu(cpu, ((const struct cpumask *)&__cpu_active_mask));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool cpu_dying(unsigned int cpu)
{
 return cpumask_test_cpu(cpu, ((const struct cpumask *)&__cpu_dying_mask));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) ssize_t
cpumap_print_to_pagebuf(bool list, char *buf, const struct cpumask *mask)
{
 return bitmap_print_to_pagebuf(list, buf, ((mask)->bits),
          nr_cpu_ids);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) ssize_t
cpumap_print_bitmask_to_buf(char *buf, const struct cpumask *mask,
  loff_t off, size_t count)
{
 return bitmap_print_bitmask_to_buf(buf, ((mask)->bits),
       nr_cpu_ids, off, count) - 1;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) ssize_t
cpumap_print_list_to_buf(char *buf, const struct cpumask *mask,
  loff_t off, size_t count)
{
 return bitmap_print_list_to_buf(buf, ((mask)->bits),
       nr_cpu_ids, off, count) - 1;
}

enum cpu_type_enum {
 CPU_UNKNOWN,
 CPU_LOONGSON32,
 CPU_LOONGSON64,
 CPU_LAST
};
typedef struct rdtime {
  unsigned int value;
  unsigned int timeid;
} __rdtime_t;


typedef struct drdtime {
  unsigned long dvalue;
  unsigned long dtimeid;
} __drdtime_t;

extern __inline __drdtime_t
    __attribute__((__gnu_inline__, __always_inline__, __artificial__))
    __rdtime_d(void) {
  __drdtime_t __drdtime;
  __asm__ volatile(
      "rdtime.d %[val], %[tid]\n\t"
      : [val] "=&r"(__drdtime.dvalue), [tid] "=&r"(__drdtime.dtimeid));
  return __drdtime;
}


extern __inline __rdtime_t
    __attribute__((__gnu_inline__, __always_inline__, __artificial__))
    __rdtimeh_w(void) {
  __rdtime_t __rdtime;
  __asm__ volatile("rdtimeh.w %[val], %[tid]\n\t"
                   : [val] "=&r"(__rdtime.value), [tid] "=&r"(__rdtime.timeid));
  return __rdtime;
}

extern __inline __rdtime_t
    __attribute__((__gnu_inline__, __always_inline__, __artificial__))
    __rdtimel_w(void) {
  __rdtime_t __rdtime;
  __asm__ volatile("rdtimel.w %[val], %[tid]\n\t"
                   : [val] "=&r"(__rdtime.value), [tid] "=&r"(__rdtime.timeid));
  return __rdtime;
}


extern __inline int
    __attribute__((__gnu_inline__, __always_inline__, __artificial__))
    __crc_w_b_w(char _1, int _2) {
  return (int)__builtin_loongarch_crc_w_b_w((char)_1, (int)_2);
}

extern __inline int
    __attribute__((__gnu_inline__, __always_inline__, __artificial__))
    __crc_w_h_w(short _1, int _2) {
  return (int)__builtin_loongarch_crc_w_h_w((short)_1, (int)_2);
}

extern __inline int
    __attribute__((__gnu_inline__, __always_inline__, __artificial__))
    __crc_w_w_w(int _1, int _2) {
  return (int)__builtin_loongarch_crc_w_w_w((int)_1, (int)_2);
}

extern __inline int
    __attribute__((__gnu_inline__, __always_inline__, __artificial__))
    __crc_w_d_w(long int _1, int _2) {
  return (int)__builtin_loongarch_crc_w_d_w((long int)_1, (int)_2);
}

extern __inline int
    __attribute__((__gnu_inline__, __always_inline__, __artificial__))
    __crcc_w_b_w(char _1, int _2) {
  return (int)__builtin_loongarch_crcc_w_b_w((char)_1, (int)_2);
}

extern __inline int
    __attribute__((__gnu_inline__, __always_inline__, __artificial__))
    __crcc_w_h_w(short _1, int _2) {
  return (int)__builtin_loongarch_crcc_w_h_w((short)_1, (int)_2);
}

extern __inline int
    __attribute__((__gnu_inline__, __always_inline__, __artificial__))
    __crcc_w_w_w(int _1, int _2) {
  return (int)__builtin_loongarch_crcc_w_w_w((int)_1, (int)_2);
}

extern __inline int
    __attribute__((__gnu_inline__, __always_inline__, __artificial__))
    __crcc_w_d_w(long int _1, int _2) {
  return (int)__builtin_loongarch_crcc_w_d_w((long int)_1, (int)_2);
}
extern __inline unsigned char
    __attribute__((__gnu_inline__, __always_inline__, __artificial__))
    __iocsrrd_b(unsigned int _1) {
  return (unsigned char)__builtin_loongarch_iocsrrd_b((unsigned int)_1);
}

extern __inline unsigned char
    __attribute__((__gnu_inline__, __always_inline__, __artificial__))
    __iocsrrd_h(unsigned int _1) {
  return (unsigned short)__builtin_loongarch_iocsrrd_h((unsigned int)_1);
}

extern __inline unsigned int
    __attribute__((__gnu_inline__, __always_inline__, __artificial__))
    __iocsrrd_w(unsigned int _1) {
  return (unsigned int)__builtin_loongarch_iocsrrd_w((unsigned int)_1);
}


extern __inline unsigned long int
    __attribute__((__gnu_inline__, __always_inline__, __artificial__))
    __iocsrrd_d(unsigned int _1) {
  return (unsigned long int)__builtin_loongarch_iocsrrd_d((unsigned int)_1);
}


extern __inline void
    __attribute__((__gnu_inline__, __always_inline__, __artificial__))
    __iocsrwr_b(unsigned char _1, unsigned int _2) {
  __builtin_loongarch_iocsrwr_b((unsigned char)_1, (unsigned int)_2);
}

extern __inline void
    __attribute__((__gnu_inline__, __always_inline__, __artificial__))
    __iocsrwr_h(unsigned short _1, unsigned int _2) {
  __builtin_loongarch_iocsrwr_h((unsigned short)_1, (unsigned int)_2);
}

extern __inline void
    __attribute__((__gnu_inline__, __always_inline__, __artificial__))
    __iocsrwr_w(unsigned int _1, unsigned int _2) {
  __builtin_loongarch_iocsrwr_w((unsigned int)_1, (unsigned int)_2);
}

extern __inline unsigned int
    __attribute__((__gnu_inline__, __always_inline__, __artificial__))
    __cpucfg(unsigned int _1) {
  return (unsigned int)__builtin_loongarch_cpucfg((unsigned int)_1);
}


extern __inline void
    __attribute__((__gnu_inline__, __always_inline__, __artificial__))
    __iocsrwr_d(unsigned long int _1, unsigned int _2) {
  __builtin_loongarch_iocsrwr_d((unsigned long int)_1, (unsigned int)_2);
}

extern __inline void
    __attribute__((__gnu_inline__, __always_inline__, __artificial__))
    __asrtgt_d(long int _1, long int _2) {
  __builtin_loongarch_asrtgt_d((long int)_1, (long int)_2);
}

extern __inline void
    __attribute__((__gnu_inline__, __always_inline__, __artificial__))
    __asrtle_d(long int _1, long int _2) {
  __builtin_loongarch_asrtle_d((long int)_1, (long int)_2);
}
__asm__(".macro	parse_r var r\n\t"
 "\\var	= -1\n\t"
 ".ifc	\\r, $r" "0" "\n\t" "\\var	= " "0" "\n\t" ".endif\n\t" ".ifc	\\r, $r" "1" "\n\t" "\\var	= " "1" "\n\t" ".endif\n\t" ".ifc	\\r, $r" "2" "\n\t" "\\var	= " "2" "\n\t" ".endif\n\t" ".ifc	\\r, $r" "3" "\n\t" "\\var	= " "3" "\n\t" ".endif\n\t"
 ".ifc	\\r, $r" "4" "\n\t" "\\var	= " "4" "\n\t" ".endif\n\t" ".ifc	\\r, $r" "5" "\n\t" "\\var	= " "5" "\n\t" ".endif\n\t" ".ifc	\\r, $r" "6" "\n\t" "\\var	= " "6" "\n\t" ".endif\n\t" ".ifc	\\r, $r" "7" "\n\t" "\\var	= " "7" "\n\t" ".endif\n\t"
 ".ifc	\\r, $r" "8" "\n\t" "\\var	= " "8" "\n\t" ".endif\n\t" ".ifc	\\r, $r" "9" "\n\t" "\\var	= " "9" "\n\t" ".endif\n\t" ".ifc	\\r, $r" "10" "\n\t" "\\var	= " "10" "\n\t" ".endif\n\t" ".ifc	\\r, $r" "11" "\n\t" "\\var	= " "11" "\n\t" ".endif\n\t"
 ".ifc	\\r, $r" "12" "\n\t" "\\var	= " "12" "\n\t" ".endif\n\t" ".ifc	\\r, $r" "13" "\n\t" "\\var	= " "13" "\n\t" ".endif\n\t" ".ifc	\\r, $r" "14" "\n\t" "\\var	= " "14" "\n\t" ".endif\n\t" ".ifc	\\r, $r" "15" "\n\t" "\\var	= " "15" "\n\t" ".endif\n\t"
 ".ifc	\\r, $r" "16" "\n\t" "\\var	= " "16" "\n\t" ".endif\n\t" ".ifc	\\r, $r" "17" "\n\t" "\\var	= " "17" "\n\t" ".endif\n\t" ".ifc	\\r, $r" "18" "\n\t" "\\var	= " "18" "\n\t" ".endif\n\t" ".ifc	\\r, $r" "19" "\n\t" "\\var	= " "19" "\n\t" ".endif\n\t"
 ".ifc	\\r, $r" "20" "\n\t" "\\var	= " "20" "\n\t" ".endif\n\t" ".ifc	\\r, $r" "21" "\n\t" "\\var	= " "21" "\n\t" ".endif\n\t" ".ifc	\\r, $r" "22" "\n\t" "\\var	= " "22" "\n\t" ".endif\n\t" ".ifc	\\r, $r" "23" "\n\t" "\\var	= " "23" "\n\t" ".endif\n\t"
 ".ifc	\\r, $r" "24" "\n\t" "\\var	= " "24" "\n\t" ".endif\n\t" ".ifc	\\r, $r" "25" "\n\t" "\\var	= " "25" "\n\t" ".endif\n\t" ".ifc	\\r, $r" "26" "\n\t" "\\var	= " "26" "\n\t" ".endif\n\t" ".ifc	\\r, $r" "27" "\n\t" "\\var	= " "27" "\n\t" ".endif\n\t"
 ".ifc	\\r, $r" "28" "\n\t" "\\var	= " "28" "\n\t" ".endif\n\t" ".ifc	\\r, $r" "29" "\n\t" "\\var	= " "29" "\n\t" ".endif\n\t" ".ifc	\\r, $r" "30" "\n\t" "\\var	= " "30" "\n\t" ".endif\n\t" ".ifc	\\r, $r" "31" "\n\t" "\\var	= " "31" "\n\t" ".endif\n\t"
 ".iflt	\\var\n\t"
 ".error	\"Unable to parse register name \\r\"\n\t"
 ".endif\n\t"
 ".endm");
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) u64 drdtime(void)
{
 int rID = 0;
 u64 val = 0;

 __asm__ __volatile__(
  "rdtime.d %0, %1 \n\t"
  : "=r"(val), "=r"(rID)
  :
  );
 return val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned int get_csr_cpuid(void)
{
 return ((unsigned int)__builtin_loongarch_csrrd_w((0x20)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void csr_any_send(unsigned int addr, unsigned int data,
    unsigned int data_mask, unsigned int cpu)
{
 uint64_t val = 0;

 val = ((((1ULL))) << (31)) | addr;
 val |= (cpu << 16);
 val |= (data_mask << 27);
 val |= ((uint64_t)data << 32);
 __iocsrwr_d(val, 0x1158);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned int read_csr_excode(void)
{
 return (((unsigned int)__builtin_loongarch_csrrd_w((0x5))) & ((unsigned long)(0x3f) << 16)) >> 16;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void write_csr_index(unsigned int idx)
{
 ((unsigned int)__builtin_loongarch_csrxchg_w((unsigned int)(idx), (unsigned int)(0xfff), (0x10)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned int read_csr_pagesize(void)
{
 return (((unsigned int)__builtin_loongarch_csrrd_w((0x10))) & 0x3f000000) >> 24;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void write_csr_pagesize(unsigned int size)
{
 ((unsigned int)__builtin_loongarch_csrxchg_w((unsigned int)(size << 24), (unsigned int)(0x3f000000), (0x10)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned int read_csr_tlbrefill_pagesize(void)
{
 return (((unsigned long int)__builtin_loongarch_csrrd_d((0x8e))) & ((unsigned long)(0x3f) << 0)) >> 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void write_csr_tlbrefill_pagesize(unsigned int size)
{
 ((unsigned long int)__builtin_loongarch_csrxchg_d( (unsigned long int)(size << 0), (unsigned long int)(((unsigned long)(0x3f) << 0)), (0x8e)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long set_csr_euen(unsigned long set) { unsigned long res, new; res = ((unsigned int)__builtin_loongarch_csrrd_w((0x2))); new = res | set; ((unsigned int)__builtin_loongarch_csrwr_w((unsigned int)(new), (0x2))); return res; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long clear_csr_euen(unsigned long clear) { unsigned long res, new; res = ((unsigned int)__builtin_loongarch_csrrd_w((0x2))); new = res & ~clear; ((unsigned int)__builtin_loongarch_csrwr_w((unsigned int)(new), (0x2))); return res; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long change_csr_euen(unsigned long change, unsigned long val) { unsigned long res, new; res = ((unsigned int)__builtin_loongarch_csrrd_w((0x2))); new = res & ~change; new |= (val & change); ((unsigned int)__builtin_loongarch_csrwr_w((unsigned int)(new), (0x2))); return res; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long set_csr_ecfg(unsigned long set) { unsigned long res, new; res = ((unsigned int)__builtin_loongarch_csrrd_w((0x4))); new = res | set; ((unsigned int)__builtin_loongarch_csrwr_w((unsigned int)(new), (0x4))); return res; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long clear_csr_ecfg(unsigned long clear) { unsigned long res, new; res = ((unsigned int)__builtin_loongarch_csrrd_w((0x4))); new = res & ~clear; ((unsigned int)__builtin_loongarch_csrwr_w((unsigned int)(new), (0x4))); return res; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long change_csr_ecfg(unsigned long change, unsigned long val) { unsigned long res, new; res = ((unsigned int)__builtin_loongarch_csrrd_w((0x4))); new = res & ~change; new |= (val & change); ((unsigned int)__builtin_loongarch_csrwr_w((unsigned int)(new), (0x4))); return res; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long set_csr_tlbidx(unsigned long set) { unsigned long res, new; res = ((unsigned int)__builtin_loongarch_csrrd_w((0x10))); new = res | set; ((unsigned int)__builtin_loongarch_csrwr_w((unsigned int)(new), (0x10))); return res; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long clear_csr_tlbidx(unsigned long clear) { unsigned long res, new; res = ((unsigned int)__builtin_loongarch_csrrd_w((0x10))); new = res & ~clear; ((unsigned int)__builtin_loongarch_csrwr_w((unsigned int)(new), (0x10))); return res; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long change_csr_tlbidx(unsigned long change, unsigned long val) { unsigned long res, new; res = ((unsigned int)__builtin_loongarch_csrrd_w((0x10))); new = res & ~change; new |= (val & change); ((unsigned int)__builtin_loongarch_csrwr_w((unsigned int)(new), (0x10))); return res; }


enum {
 CACHE_PRESENT = (1 << 0),
 CACHE_PRIVATE = (1 << 1),
 CACHE_INCLUSIVE = (1 << 2),
};




struct cache_desc {
 unsigned char type;
 unsigned char level;
 unsigned short sets;
 unsigned char ways;
 unsigned char linesz;
 unsigned char flags;
};




struct cpuinfo_loongarch {
 u64 asid_cache;
 unsigned long asid_mask;




 unsigned long long options;
 unsigned int processor_id;
 unsigned int fpu_vers;
 unsigned int fpu_csr0;
 unsigned int fpu_mask;
 unsigned int cputype;
 int isa_level;
 int tlbsize;
 int tlbsizemtlb;
 int tlbsizestlbsets;
 int tlbsizestlbways;
 int cache_leaves_present;
 struct cache_desc cache_leaves[6];
 int core;
 int package;
 int global_id;
 int vabits;
 int pabits;
 unsigned int ksave_mask;
 unsigned int watch_dreg_count;
 unsigned int watch_ireg_count;
 unsigned int watch_reg_use_cnt;
} __attribute__((__aligned__((1 << 6))));

extern struct cpuinfo_loongarch cpu_data[];




extern void cpu_probe(void);

extern const char *__cpu_family[];
extern const char *__cpu_full_name[];



struct seq_file;
struct notifier_block;

extern int register_proc_cpuinfo_notifier(struct notifier_block *nb);
extern int proc_cpuinfo_notifier_call_chain(unsigned long val, void *v);
struct proc_cpuinfo_notifier_args {
 struct seq_file *m;
 unsigned long n;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool cpus_are_siblings(int cpua, int cpub)
{
 struct cpuinfo_loongarch *infoa = &cpu_data[cpua];
 struct cpuinfo_loongarch *infob = &cpu_data[cpub];

 if (infoa->package != infob->package)
  return false;

 if (infoa->core != infob->core)
  return false;

 return true;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long cpu_asid_mask(struct cpuinfo_loongarch *cpuinfo)
{
 return cpuinfo->asid_mask;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void set_cpu_asid_mask(struct cpuinfo_loongarch *cpuinfo,
         unsigned long asid_mask)
{
 cpuinfo->asid_mask = asid_mask;
}
struct arch_hw_breakpoint_ctrl {
 u32 __reserved : 28,
 len : 2,
 type : 2;
};

struct arch_hw_breakpoint {
 u64 address;
 u64 mask;
 struct arch_hw_breakpoint_ctrl ctrl;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u32 encode_ctrl_reg(struct arch_hw_breakpoint_ctrl ctrl)
{
 return (ctrl.len << 10) | (ctrl.type << 8);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void decode_ctrl_reg(u32 reg, struct arch_hw_breakpoint_ctrl *ctrl)
{
 reg >>= 8;
 ctrl->type = reg & 0x3;
 reg >>= 2;
 ctrl->len = reg & 0x3;
}

struct task_struct;
struct notifier_block;
struct perf_event;
struct perf_event_attr;

extern int arch_bp_generic_fields(struct arch_hw_breakpoint_ctrl ctrl,
      int *gen_len, int *gen_type, int *offset);
extern int arch_check_bp_in_kernelspace(struct arch_hw_breakpoint *hw);
extern int hw_breakpoint_arch_parse(struct perf_event *bp,
        const struct perf_event_attr *attr,
        struct arch_hw_breakpoint *hw);
extern int hw_breakpoint_exceptions_notify(struct notifier_block *unused,
        unsigned long val, void *data);

extern int arch_install_hw_breakpoint(struct perf_event *bp);
extern void arch_uninstall_hw_breakpoint(struct perf_event *bp);
extern int hw_breakpoint_slots(int type);
extern void hw_breakpoint_pmu_read(struct perf_event *bp);

void breakpoint_handler(struct pt_regs *regs);
void watchpoint_handler(struct pt_regs *regs);


extern void ptrace_hw_copy_thread(struct task_struct *task);
extern void hw_breakpoint_thread_switch(struct task_struct *next);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int get_num_brps(void)
{
 return ((unsigned long int)__builtin_loongarch_csrrd_d((0x380))) & 0x3f;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int get_num_wrps(void)
{
 return ((unsigned long int)__builtin_loongarch_csrrd_d((0x300))) & 0x3f;
}

struct user_pt_regs {

 unsigned long regs[32];


 unsigned long orig_a0;


 unsigned long csr_era;
 unsigned long csr_badv;
 unsigned long reserved[10];
} __attribute__((aligned(8)));

struct user_fp_state {
 uint64_t fpr[32];
 uint64_t fcc;
 uint32_t fcsr;
};

struct user_lsx_state {

 uint64_t vregs[32*2];
};

struct user_lasx_state {

 uint64_t vregs[32*4];
};

struct user_lbt_state {
 uint64_t scr[4];
 uint32_t eflags;
 uint32_t ftop;
};

struct user_watch_state {
 uint64_t dbg_info;
 struct {
  uint64_t addr;
  uint64_t mask;
  uint32_t ctrl;
  uint32_t pad;
 } dbg_regs[8];
};
struct sigcontext {
 __u64 sc_pc;
 __u64 sc_regs[32];
 __u32 sc_flags;
 __u64 sc_extcontext[0] __attribute__((__aligned__(16)));
};


struct sctx_info {
 __u32 magic;
 __u32 size;
 __u64 padding;
};




struct fpu_context {
 __u64 regs[32];
 __u64 fcc;
 __u32 fcsr;
};




struct lsx_context {
 __u64 regs[2*32];
 __u64 fcc;
 __u32 fcsr;
};




struct lasx_context {
 __u64 regs[4*32];
 __u64 fcc;
 __u32 fcsr;
};




struct lbt_context {
 __u64 regs[4];
 __u32 eflags;
 __u32 ftop;
};
unsigned long stack_top(void);
union fpureg {
 __u32 val32[256 / 32];
 __u64 val64[256 / 64];
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u32 get_fpr32(union fpureg *fpr, unsigned idx) { return fpr->val32[(idx)]; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void set_fpr32(union fpureg *fpr, unsigned int idx, u32 val) { fpr->val32[(idx)] = val; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u64 get_fpr64(union fpureg *fpr, unsigned idx) { return fpr->val64[(idx)]; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void set_fpr64(union fpureg *fpr, unsigned int idx, u64 val) { fpr->val64[(idx)] = val; }

struct loongarch_fpu {
 uint64_t fcc;
 uint32_t fcsr;
 uint32_t ftop;
 union fpureg fpr[32];
};

struct loongarch_lbt {

 unsigned long scr0;
 unsigned long scr1;
 unsigned long scr2;
 unsigned long scr3;

 unsigned long eflags;
};







struct loongarch_vdso_info;




struct thread_struct {

 unsigned long reg01, reg03, reg22;
 unsigned long reg23, reg24, reg25, reg26;
 unsigned long reg27, reg28, reg29, reg30, reg31;


 unsigned long sched_ra;
 unsigned long sched_cfa;


 unsigned long csr_prmd;
 unsigned long csr_crmd;
 unsigned long csr_euen;
 unsigned long csr_ecfg;
 unsigned long csr_badvaddr;


 unsigned long trap_nr;
 unsigned long error_code;
 unsigned long single_step;
 struct loongarch_vdso_info *vdso;





 struct loongarch_fpu fpu __attribute__((aligned(32)));
 struct loongarch_lbt lbt;


 struct perf_event *hbp_break[8];
 struct perf_event *hbp_watch[8];
};
struct task_struct;

enum idle_boot_override {IDLE_NO_OVERRIDE = 0, IDLE_HALT, IDLE_NOMWAIT, IDLE_POLL};

extern unsigned long boot_option_idle_override;



extern void start_thread(struct pt_regs *regs, unsigned long pc, unsigned long sp);

unsigned long __get_wchan(struct task_struct *p);








struct thread_info {
 struct task_struct *task;
 unsigned long flags;
 unsigned long tp_value;
 __u32 cpu;
 int preempt_count;
 struct pt_regs *regs;
 unsigned long syscall;
 unsigned long syscall_work;
};
register struct thread_info *__current_thread_info __asm__("$tp");

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) struct thread_info *current_thread_info(void)
{
 return __current_thread_info;
}

register unsigned long current_stack_pointer __asm__("$sp");







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) long set_restart_fn(struct restart_block *restart,
     long (*fn)(struct restart_block *))
{
 restart->fn = fn;
 do { } while (0);
 return -516;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void set_ti_thread_flag(struct thread_info *ti, int flag)
{
 set_bit(flag, (unsigned long *)&ti->flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void clear_ti_thread_flag(struct thread_info *ti, int flag)
{
 clear_bit(flag, (unsigned long *)&ti->flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void update_ti_thread_flag(struct thread_info *ti, int flag,
      bool value)
{
 if (value)
  set_ti_thread_flag(ti, flag);
 else
  clear_ti_thread_flag(ti, flag);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int test_and_set_ti_thread_flag(struct thread_info *ti, int flag)
{
 return test_and_set_bit(flag, (unsigned long *)&ti->flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int test_and_clear_ti_thread_flag(struct thread_info *ti, int flag)
{
 return test_and_clear_bit(flag, (unsigned long *)&ti->flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int test_ti_thread_flag(struct thread_info *ti, int flag)
{
 return ((__builtin_constant_p(flag) && __builtin_constant_p((uintptr_t)((unsigned long *)&ti->flags) != (uintptr_t)((void *)0)) && (uintptr_t)((unsigned long *)&ti->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)((unsigned long *)&ti->flags))) ? const_test_bit(flag, (unsigned long *)&ti->flags) : generic_test_bit(flag, (unsigned long *)&ti->flags));
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) unsigned long read_ti_thread_flags(struct thread_info *ti)
{
 return ({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_41(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(ti->flags) == sizeof(char) || sizeof(ti->flags) == sizeof(short) || sizeof(ti->flags) == sizeof(int) || sizeof(ti->flags) == sizeof(long)) || sizeof(ti->flags) == sizeof(long long))) __compiletime_assert_41(); } while (0); (*(const volatile typeof( _Generic((ti->flags), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (ti->flags))) *)&(ti->flags)); });
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool tif_need_resched(void)
{
 return ((__builtin_constant_p(2) && __builtin_constant_p((uintptr_t)((unsigned long *)(&current_thread_info()->flags)) != (uintptr_t)((void *)0)) && (uintptr_t)((unsigned long *)(&current_thread_info()->flags)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)((unsigned long *)(&current_thread_info()->flags)))) ? const_test_bit(2, (unsigned long *)(&current_thread_info()->flags)) : generic_test_bit(2, (unsigned long *)(&current_thread_info()->flags)));

}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int arch_within_stack_frames(const void * const stack,
        const void * const stackend,
        const void *obj, unsigned long len)
{
 return 0;
}



extern void __check_object_size(const void *ptr, unsigned long n,
     bool to_user);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void check_object_size(const void *ptr, unsigned long n,
           bool to_user)
{
 if (!__builtin_constant_p(n))
  __check_object_size(ptr, n, to_user);
}






extern void __attribute__((__error__("copy source size is too small")))
__bad_copy_from(void);
extern void __attribute__((__error__("copy destination size is too small")))
__bad_copy_to(void);

void __copy_overflow(int size, unsigned long count);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void copy_overflow(int size, unsigned long count)
{
 if (1)
  __copy_overflow(size, count);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __attribute__((__warn_unused_result__)) bool
check_copy_size(const void *addr, size_t bytes, bool is_source)
{
 int sz = __builtin_object_size(addr, 0);
 if (__builtin_expect(!!(sz >= 0 && sz < bytes), 0)) {
  if (!__builtin_constant_p(bytes))
   copy_overflow(sz, bytes);
  else if (is_source)
   __bad_copy_from();
  else
   __bad_copy_to();
  return false;
 }
 if (({ int __ret_warn_on = !!(bytes > ((int)(~0U >> 1))); if (__builtin_expect(!!(__ret_warn_on), 0)) do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/thread_info.h\"; .popsection; .long 10002b - .; .short 249; .short (1 << 0)|((1 << 1) | ((9) << 8)); .popsection; 10001: break 1");; do { } while(0); } while (0); __builtin_expect(!!(__ret_warn_on), 0); }))
  return false;
 check_object_size(addr, bytes, is_source);
 return true;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void arch_setup_new_exec(void) { }


void arch_task_cache_init(void);
void arch_release_task_struct(struct task_struct *tsk);
int arch_dup_task_struct(struct task_struct *dst,
    struct task_struct *src);



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int preempt_count(void)
{
 return ({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_42(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(current_thread_info()->preempt_count) == sizeof(char) || sizeof(current_thread_info()->preempt_count) == sizeof(short) || sizeof(current_thread_info()->preempt_count) == sizeof(int) || sizeof(current_thread_info()->preempt_count) == sizeof(long)) || sizeof(current_thread_info()->preempt_count) == sizeof(long long))) __compiletime_assert_42(); } while (0); (*(const volatile typeof( _Generic((current_thread_info()->preempt_count), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (current_thread_info()->preempt_count))) *)&(current_thread_info()->preempt_count)); });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) volatile int *preempt_count_ptr(void)
{
 return &current_thread_info()->preempt_count;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void preempt_count_set(int pc)
{
 *preempt_count_ptr() = pc;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void set_preempt_need_resched(void)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void clear_preempt_need_resched(void)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool test_preempt_need_resched(void)
{
 return false;
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __preempt_count_add(int val)
{
 *preempt_count_ptr() += val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __preempt_count_sub(int val)
{
 *preempt_count_ptr() -= val;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool __preempt_count_dec_and_test(void)
{





 return !--*preempt_count_ptr() && tif_need_resched();
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool should_resched(int preempt_offset)
{
 return __builtin_expect(!!(preempt_count() == preempt_offset && tif_need_resched()), 0);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) unsigned char interrupt_context_level(void)
{
 unsigned long pc = preempt_count();
 unsigned char level = 0;

 level += !!(pc & ((((1UL << (4))-1) << (((0 + 8) + 8) + 4))));
 level += !!(pc & ((((1UL << (4))-1) << (((0 + 8) + 8) + 4)) | (((1UL << (4))-1) << ((0 + 8) + 8))));
 level += !!(pc & ((((1UL << (4))-1) << (((0 + 8) + 8) + 4)) | (((1UL << (4))-1) << ((0 + 8) + 8)) | (1UL << (0 + 8))));

 return level;
}
extern void migrate_disable(void);
extern void migrate_enable(void);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void preempt_enable_nested(void)
{
 if (0)
  do { __asm__ __volatile__("": : :"memory"); __preempt_count_sub(1); } while (0);
}

typedef struct { void *lock; ; } class_preempt_t; static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void class_preempt_destructor(class_preempt_t *_T) { if (_T->lock) { do { __asm__ __volatile__("": : :"memory"); __preempt_count_sub(1); } while (0); } } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) class_preempt_t class_preempt_constructor(void) { class_preempt_t _t = { .lock = (void*)1 }, *_T __attribute__((__unused__)) = &_t; do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); return _t; }
typedef struct { void *lock; ; } class_preempt_notrace_t; static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void class_preempt_notrace_destructor(class_preempt_notrace_t *_T) { if (_T->lock) { do { __asm__ __volatile__("": : :"memory"); __preempt_count_sub(1); } while (0); } } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) class_preempt_notrace_t class_preempt_notrace_constructor(void) { class_preempt_notrace_t _t = { .lock = (void*)1 }, *_T __attribute__((__unused__)) = &_t; do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); return _t; }
typedef struct { void *lock; ; } class_migrate_t; static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void class_migrate_destructor(class_migrate_t *_T) { if (_T->lock) { migrate_enable(); } } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) class_migrate_t class_migrate_constructor(void) { class_migrate_t _t = { .lock = (void*)1 }, *_T __attribute__((__unused__)) = &_t; migrate_disable(); return _t; }


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void arch_local_irq_enable(void)
{
 u32 flags = ((unsigned long)(0x1) << 2);
 __asm__ __volatile__(
  "csrxchg %[val], %[mask], %[reg]\n\t"
  : [val] "+r" (flags)
  : [mask] "r" (((unsigned long)(0x1) << 2)), [reg] "i" (0x0)
  : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void arch_local_irq_disable(void)
{
 u32 flags = 0;
 __asm__ __volatile__(
  "csrxchg %[val], %[mask], %[reg]\n\t"
  : [val] "+r" (flags)
  : [mask] "r" (((unsigned long)(0x1) << 2)), [reg] "i" (0x0)
  : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long arch_local_irq_save(void)
{
 u32 flags = 0;
 __asm__ __volatile__(
  "csrxchg %[val], %[mask], %[reg]\n\t"
  : [val] "+r" (flags)
  : [mask] "r" (((unsigned long)(0x1) << 2)), [reg] "i" (0x0)
  : "memory");
 return flags;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void arch_local_irq_restore(unsigned long flags)
{
 __asm__ __volatile__(
  "csrxchg %[val], %[mask], %[reg]\n\t"
  : [val] "+r" (flags)
  : [mask] "r" (((unsigned long)(0x1) << 2)), [reg] "i" (0x0)
  : "memory");
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long arch_local_save_flags(void)
{
 u32 flags;
 __asm__ __volatile__(
  "csrrd %[val], %[reg]\n\t"
  : [val] "=r" (flags)
  : [reg] "i" (0x0)
  : "memory");
 return flags;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int arch_irqs_disabled_flags(unsigned long flags)
{
 return !(flags & ((unsigned long)(0x1) << 2));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int arch_irqs_disabled(void)
{
 return arch_irqs_disabled_flags(arch_local_save_flags());
}
register unsigned long __my_cpu_offset __asm__("$r21");

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void set_my_cpu_offset(unsigned long off)
{
 __my_cpu_offset = off;
 ((unsigned long int)__builtin_loongarch_csrwr_d((unsigned long int)(off), (0x33)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long __percpu_add(void *ptr, unsigned long val, int size) { unsigned long ret; switch (size) { case 4: __asm__ __volatile__( "am""add"".w" " %[ret], %[val], %[ptr]	\n" : [ret] "=&r" (ret), [ptr] "+ZB"(*(u32 *)ptr) : [val] "r" (val)); break; case 8: __asm__ __volatile__( "am""add"".d" " %[ret], %[val], %[ptr]	\n" : [ret] "=&r" (ret), [ptr] "+ZB"(*(u64 *)ptr) : [val] "r" (val)); break; default: ret = 0; do { __attribute__((__noreturn__)) extern void __compiletime_assert_43(void) __attribute__((__error__("BUILD_BUG failed"))); if (!(!(1))) __compiletime_assert_43(); } while (0); } return ret + val; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long __percpu_and(void *ptr, unsigned long val, int size) { unsigned long ret; switch (size) { case 4: __asm__ __volatile__( "am""and"".w" " %[ret], %[val], %[ptr]	\n" : [ret] "=&r" (ret), [ptr] "+ZB"(*(u32 *)ptr) : [val] "r" (val)); break; case 8: __asm__ __volatile__( "am""and"".d" " %[ret], %[val], %[ptr]	\n" : [ret] "=&r" (ret), [ptr] "+ZB"(*(u64 *)ptr) : [val] "r" (val)); break; default: ret = 0; do { __attribute__((__noreturn__)) extern void __compiletime_assert_44(void) __attribute__((__error__("BUILD_BUG failed"))); if (!(!(1))) __compiletime_assert_44(); } while (0); } return ret & val; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long __percpu_or(void *ptr, unsigned long val, int size) { unsigned long ret; switch (size) { case 4: __asm__ __volatile__( "am""or"".w" " %[ret], %[val], %[ptr]	\n" : [ret] "=&r" (ret), [ptr] "+ZB"(*(u32 *)ptr) : [val] "r" (val)); break; case 8: __asm__ __volatile__( "am""or"".d" " %[ret], %[val], %[ptr]	\n" : [ret] "=&r" (ret), [ptr] "+ZB"(*(u64 *)ptr) : [val] "r" (val)); break; default: ret = 0; do { __attribute__((__noreturn__)) extern void __compiletime_assert_45(void) __attribute__((__error__("BUILD_BUG failed"))); if (!(!(1))) __compiletime_assert_45(); } while (0); } return ret | val; }


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long __percpu_read(void *ptr, int size)
{
 unsigned long ret;

 switch (size) {
 case 1:
  __asm__ __volatile__ ("ldx.b %[ret], $r21, %[ptr]	\n"
  : [ret] "=&r"(ret)
  : [ptr] "r"(ptr)
  : "memory");
  break;
 case 2:
  __asm__ __volatile__ ("ldx.h %[ret], $r21, %[ptr]	\n"
  : [ret] "=&r"(ret)
  : [ptr] "r"(ptr)
  : "memory");
  break;
 case 4:
  __asm__ __volatile__ ("ldx.w %[ret], $r21, %[ptr]	\n"
  : [ret] "=&r"(ret)
  : [ptr] "r"(ptr)
  : "memory");
  break;
 case 8:
  __asm__ __volatile__ ("ldx.d %[ret], $r21, %[ptr]	\n"
  : [ret] "=&r"(ret)
  : [ptr] "r"(ptr)
  : "memory");
  break;
 default:
  ret = 0;
  do { __attribute__((__noreturn__)) extern void __compiletime_assert_46(void) __attribute__((__error__("BUILD_BUG failed"))); if (!(!(1))) __compiletime_assert_46(); } while (0);
 }

 return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __percpu_write(void *ptr, unsigned long val, int size)
{
 switch (size) {
 case 1:
  __asm__ __volatile__("stx.b %[val], $r21, %[ptr]	\n"
  :
  : [val] "r" (val), [ptr] "r" (ptr)
  : "memory");
  break;
 case 2:
  __asm__ __volatile__("stx.h %[val], $r21, %[ptr]	\n"
  :
  : [val] "r" (val), [ptr] "r" (ptr)
  : "memory");
  break;
 case 4:
  __asm__ __volatile__("stx.w %[val], $r21, %[ptr]	\n"
  :
  : [val] "r" (val), [ptr] "r" (ptr)
  : "memory");
  break;
 case 8:
  __asm__ __volatile__("stx.d %[val], $r21, %[ptr]	\n"
  :
  : [val] "r" (val), [ptr] "r" (ptr)
  : "memory");
  break;
 default:
  do { __attribute__((__noreturn__)) extern void __compiletime_assert_47(void) __attribute__((__error__("BUILD_BUG failed"))); if (!(!(1))) __compiletime_assert_47(); } while (0);
 }
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long __percpu_xchg(void *ptr, unsigned long val,
      int size)
{
 switch (size) {
 case 1:
 case 2:
  return __xchg_small((volatile void *)ptr, val, size);

 case 4:
  return ({ __typeof((u32)val) __ret; __asm__ __volatile__ ( " ""amswap.w"" %1, %z2, %0 \n" : "+ZB" (*(volatile u32 *)ptr), "=&r" (__ret) : "Jr" ((u32)val) : "memory"); __ret; });

 case 8:
  return ({ __typeof((u64)val) __ret; __asm__ __volatile__ ( " ""amswap.d"" %1, %z2, %0 \n" : "+ZB" (*(volatile u64 *)ptr), "=&r" (__ret) : "Jr" ((u64)val) : "memory"); __ret; });

 default:
  do { __attribute__((__noreturn__)) extern void __compiletime_assert_48(void) __attribute__((__error__("BUILD_BUG failed"))); if (!(!(1))) __compiletime_assert_48(); } while (0);
 }

 return 0;
}






extern void __bad_size_call_parameter(void);




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __this_cpu_preempt_check(const char *op) { }
extern unsigned long __per_cpu_offset[64];
extern void setup_per_cpu_areas(void);



  extern void lockdep_softirqs_on(unsigned long ip);
  extern void lockdep_softirqs_off(unsigned long ip);
  extern void lockdep_hardirqs_on_prepare(void);
  extern void lockdep_hardirqs_on(unsigned long ip);
  extern void lockdep_hardirqs_off(unsigned long ip);
struct irqtrace_events {
 unsigned int irq_events;
 unsigned long hardirq_enable_ip;
 unsigned long hardirq_disable_ip;
 unsigned int hardirq_enable_event;
 unsigned int hardirq_disable_event;
 unsigned long softirq_disable_ip;
 unsigned long softirq_enable_ip;
 unsigned int softirq_disable_event;
 unsigned int softirq_enable_event;
};

extern __attribute__((__section__(".discard"))) __attribute__((unused)) char __pcpu_scope_hardirqs_enabled; extern __attribute__((section(".data..percpu" ""))) __typeof__(int) hardirqs_enabled;
extern __attribute__((__section__(".discard"))) __attribute__((unused)) char __pcpu_scope_hardirq_context; extern __attribute__((section(".data..percpu" ""))) __typeof__(int) hardirq_context;

extern void trace_hardirqs_on_prepare(void);
extern void trace_hardirqs_off_finish(void);
extern void trace_hardirqs_on(void);
extern void trace_hardirqs_off(void);
 extern void stop_critical_timings(void);
 extern void start_critical_timings(void);






extern void warn_bogus_irq_restore(void);
typedef struct { void *lock; ; } class_irq_t; static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void class_irq_destructor(class_irq_t *_T) { if (_T->lock) { do { trace_hardirqs_on(); arch_local_irq_enable(); } while (0); } } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) class_irq_t class_irq_constructor(void) { class_irq_t _t = { .lock = (void*)1 }, *_T __attribute__((__unused__)) = &_t; do { bool was_disabled = (arch_irqs_disabled()); arch_local_irq_disable(); if (!was_disabled) trace_hardirqs_off(); } while (0); return _t; }
typedef struct { void *lock; unsigned long flags; } class_irqsave_t; static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void class_irqsave_destructor(class_irqsave_t *_T) { if (_T->lock) { do { if (!({ ({ unsigned long __dummy; typeof(_T->flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); arch_irqs_disabled_flags(_T->flags); })) trace_hardirqs_on(); do { ({ unsigned long __dummy; typeof(_T->flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); do { if (__builtin_expect(!!(!arch_irqs_disabled()), 0)) warn_bogus_irq_restore(); } while (0); arch_local_irq_restore(_T->flags); } while (0); } while (0); } } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) class_irqsave_t class_irqsave_constructor(void) { class_irqsave_t _t = { .lock = (void*)1 }, *_T __attribute__((__unused__)) = &_t; do { do { ({ unsigned long __dummy; typeof(_T->flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); _T->flags = arch_local_irq_save(); } while (0); if (!({ ({ unsigned long __dummy; typeof(_T->flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); arch_irqs_disabled_flags(_T->flags); })) trace_hardirqs_off(); } while (0); return _t; }










extern void __local_bh_disable_ip(unsigned long ip, unsigned int cnt);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void local_bh_disable(void)
{
 __local_bh_disable_ip(({ __label__ __here; __here: (unsigned long)&&__here; }), (2 * (1UL << (0 + 8))));
}

extern void _local_bh_enable(void);
extern void __local_bh_enable_ip(unsigned long ip, unsigned int cnt);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void local_bh_enable_ip(unsigned long ip)
{
 __local_bh_enable_ip(ip, (2 * (1UL << (0 + 8))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void local_bh_enable(void)
{
 __local_bh_enable_ip(({ __label__ __here; __here: (unsigned long)&&__here; }), (2 * (1UL << (0 + 8))));
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool local_bh_blocked(void) { return false; }




struct llist_head {
 struct llist_node *first;
};

struct llist_node {
 struct llist_node *next;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void init_llist_head(struct llist_head *list)
{
 list->first = ((void *)0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool llist_empty(const struct llist_head *head)
{
 return ({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_49(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(head->first) == sizeof(char) || sizeof(head->first) == sizeof(short) || sizeof(head->first) == sizeof(int) || sizeof(head->first) == sizeof(long)) || sizeof(head->first) == sizeof(long long))) __compiletime_assert_49(); } while (0); (*(const volatile typeof( _Generic((head->first), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (head->first))) *)&(head->first)); }) == ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) struct llist_node *llist_next(struct llist_node *node)
{
 return node->next;
}

extern bool llist_add_batch(struct llist_node *new_first,
       struct llist_node *new_last,
       struct llist_head *head);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool __llist_add_batch(struct llist_node *new_first,
         struct llist_node *new_last,
         struct llist_head *head)
{
 new_last->next = head->first;
 head->first = new_first;
 return new_last->next == ((void *)0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool llist_add(struct llist_node *new, struct llist_head *head)
{
 return llist_add_batch(new, new, head);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool __llist_add(struct llist_node *new, struct llist_head *head)
{
 return __llist_add_batch(new, new, head);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) struct llist_node *llist_del_all(struct llist_head *head)
{
 return ({ typeof(&head->first) __ai_ptr = (&head->first); do { } while (0); instrument_atomic_read_write(__ai_ptr, sizeof(*__ai_ptr)); ({ __typeof__(*(__ai_ptr)) __res; __res = (__typeof__(*(__ai_ptr))) __arch_xchg((__ai_ptr), (unsigned long)(((void *)0)), sizeof(*(__ai_ptr))); __res; }); });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) struct llist_node *__llist_del_all(struct llist_head *head)
{
 struct llist_node *first = head->first;

 head->first = ((void *)0);
 return first;
}

extern struct llist_node *llist_del_first(struct llist_head *head);

struct llist_node *llist_reverse_order(struct llist_node *head);

enum {
 CSD_FLAG_LOCK = 0x01,

 IRQ_WORK_PENDING = 0x01,
 IRQ_WORK_BUSY = 0x02,
 IRQ_WORK_LAZY = 0x04,
 IRQ_WORK_HARD_IRQ = 0x08,

 IRQ_WORK_CLAIMED = (IRQ_WORK_PENDING | IRQ_WORK_BUSY),

 CSD_TYPE_ASYNC = 0x00,
 CSD_TYPE_SYNC = 0x10,
 CSD_TYPE_IRQ_WORK = 0x20,
 CSD_TYPE_TTWU = 0x30,

 CSD_FLAG_TYPE_MASK = 0xF0,
};
struct __call_single_node {
 struct llist_node llist;
 union {
  unsigned int u_flags;
  atomic_t a_flags;
 };

 u16 src, dst;

};

typedef void (*smp_call_func_t)(void *info);
typedef bool (*smp_cond_func_t)(int cpu, void *info);




struct __call_single_data {
 struct __call_single_node node;
 smp_call_func_t func;
 void *info;
};





typedef struct __call_single_data call_single_data_t
 __attribute__((__aligned__(sizeof(struct __call_single_data))));
extern void __smp_call_single_queue(int cpu, struct llist_node *node);


extern unsigned int total_cpus;

int smp_call_function_single(int cpuid, smp_call_func_t func, void *info,
        int wait);

void on_each_cpu_cond_mask(smp_cond_func_t cond_func, smp_call_func_t func,
      void *info, bool wait, const struct cpumask *mask);

int smp_call_function_single_async(int cpu, struct __call_single_data *csd);





void __attribute__((__noreturn__)) panic_smp_self_stop(void);
void __attribute__((__noreturn__)) nmi_panic_self_stop(struct pt_regs *regs);
void crash_smp_send_stop(void);




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void on_each_cpu(smp_call_func_t func, void *info, int wait)
{
 on_each_cpu_cond_mask(((void *)0), func, info, wait, ((const struct cpumask *)&__cpu_online_mask));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void on_each_cpu_mask(const struct cpumask *mask,
        smp_call_func_t func, void *info, bool wait)
{
 on_each_cpu_cond_mask(((void *)0), func, info, wait, mask);
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void on_each_cpu_cond(smp_cond_func_t cond_func,
        smp_call_func_t func, void *info, bool wait)
{
 on_each_cpu_cond_mask(cond_func, func, info, wait, ((const struct cpumask *)&__cpu_online_mask));
}






extern int smp_num_siblings;
extern int num_processors;
extern int disabled_cpus;
extern cpumask_t cpu_sibling_map[];
extern cpumask_t cpu_core_map[];
extern cpumask_t cpu_foreign_map[];

void loongson_smp_setup(void);
void loongson_prepare_cpus(unsigned int max_cpus);
void loongson_boot_secondary(int cpu, struct task_struct *idle);
void loongson_init_secondary(void);
void loongson_smp_finish(void);
void loongson_send_ipi_single(int cpu, unsigned int action);
void loongson_send_ipi_mask(const struct cpumask *mask, unsigned int action);

int loongson_cpu_disable(void);
void loongson_cpu_die(unsigned int cpu);


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void plat_smp_setup(void)
{
 loongson_smp_setup();
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int raw_smp_processor_id(void)
{





 return current_thread_info()->cpu;

}




extern int __cpu_number_map[64];



extern int __cpu_logical_map[64];
struct secondary_data {
 unsigned long stack;
 unsigned long thread_info;
};
extern struct secondary_data cpuboot_data;

extern void smpboot_entry(void);

extern void calculate_cpu_foreign_map(void);




extern void show_ipi_list(struct seq_file *p, int prec);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void arch_send_call_function_single_ipi(int cpu)
{
 loongson_send_ipi_single(cpu, 0x4);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void arch_send_call_function_ipi_mask(const struct cpumask *mask)
{
 loongson_send_ipi_mask(mask, 0x4);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int __cpu_disable(void)
{
 return loongson_cpu_disable();
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __cpu_die(unsigned int cpu)
{
 loongson_cpu_die(cpu);
}

extern void __attribute__((__noreturn__)) play_dead(void);
extern void smp_send_stop(void);




extern void arch_smp_send_reschedule(int cpu);
extern void smp_prepare_cpus(unsigned int max_cpus);




extern int __cpu_up(unsigned int cpunum, struct task_struct *tidle);




extern void smp_cpus_done(unsigned int max_cpus);




void smp_call_function(smp_call_func_t func, void *info, int wait);
void smp_call_function_many(const struct cpumask *mask,
       smp_call_func_t func, void *info, bool wait);

int smp_call_function_any(const struct cpumask *mask,
     smp_call_func_t func, void *info, int wait);

void kick_all_cpus_sync(void);
void wake_up_all_idle_cpus(void);




void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) call_function_init(void);
void generic_smp_call_function_single_interrupt(void);







void smp_prepare_boot_cpu(void);

extern unsigned int setup_max_cpus;
extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) setup_nr_cpu_ids(void);
extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) smp_init(void);

extern int __boot_cpu_id;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int get_boot_cpu_id(void)
{
 return __boot_cpu_id;
}
extern void arch_disable_smp_support(void);

extern void arch_thaw_secondary_cpus_begin(void);
extern void arch_thaw_secondary_cpus_end(void);

void smp_setup_processor_id(void);

int smp_call_on_cpu(unsigned int cpu, int (*func)(void *), void *par,
      bool phys);


int smpcfd_prepare_cpu(unsigned int cpu);
int smpcfd_dead_cpu(unsigned int cpu);
int smpcfd_dying_cpu(unsigned int cpu);


struct task_struct;












struct task_struct;

extern int debug_locks __attribute__((__section__(".data..read_mostly")));
extern int debug_locks_silent __attribute__((__section__(".data..read_mostly")));


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int __debug_locks_off(void)
{
 return ({ typeof(&debug_locks) __ai_ptr = (&debug_locks); do { } while (0); instrument_atomic_read_write(__ai_ptr, sizeof(*__ai_ptr)); ({ __typeof__(*(__ai_ptr)) __res; __res = (__typeof__(*(__ai_ptr))) __arch_xchg((__ai_ptr), (unsigned long)(0), sizeof(*(__ai_ptr))); __res; }); });
}




extern int debug_locks_off(void);
  extern void locking_selftest(void);





extern void debug_show_all_locks(void);
extern void debug_show_held_locks(struct task_struct *task);
extern void debug_check_no_locks_freed(const void *from, unsigned long len);
extern void debug_check_no_locks_held(void);






struct task_struct;
struct pt_regs;
typedef bool (*stack_trace_consume_fn)(void *cookie, unsigned long addr);
void arch_stack_walk(stack_trace_consume_fn consume_entry, void *cookie,
       struct task_struct *task, struct pt_regs *regs);
int arch_stack_walk_reliable(stack_trace_consume_fn consume_entry, void *cookie,
        struct task_struct *task);

void arch_stack_walk_user(stack_trace_consume_fn consume_entry, void *cookie,
     const struct pt_regs *regs);



void stack_trace_print(const unsigned long *trace, unsigned int nr_entries,
         int spaces);
int stack_trace_snprint(char *buf, size_t size, const unsigned long *entries,
   unsigned int nr_entries, int spaces);
unsigned int stack_trace_save(unsigned long *store, unsigned int size,
         unsigned int skipnr);
unsigned int stack_trace_save_tsk(struct task_struct *task,
      unsigned long *store, unsigned int size,
      unsigned int skipnr);
unsigned int stack_trace_save_regs(struct pt_regs *regs, unsigned long *store,
       unsigned int size, unsigned int skipnr);
unsigned int stack_trace_save_user(unsigned long *store, unsigned int size);
unsigned int filter_irq_stacks(unsigned long *entries, unsigned int nr_entries);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int stack_trace_save_tsk_reliable(struct task_struct *tsk,
      unsigned long *store,
      unsigned int size)
{
 return -38;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void lockdep_copy_map(struct lockdep_map *to,
        struct lockdep_map *from)
{
 int i;

 *to = *from;
 for (i = 0; i < 2; i++)
  to->class_cache[i] = ((void *)0);
}





struct lock_list {
 struct list_head entry;
 struct lock_class *class;
 struct lock_class *links_to;
 const struct lock_trace *trace;
 u16 distance;

 u8 dep;

 u8 only_xr;





 struct lock_list *parent;
};
struct lock_chain {

 unsigned int irq_context : 2,
     depth : 6,
     base : 24;

 struct hlist_node entry;
 u64 chain_key;
};





struct held_lock {
 u64 prev_chain_key;
 unsigned long acquire_ip;
 struct lockdep_map *instance;
 struct lockdep_map *nest_lock;

 u64 waittime_stamp;
 u64 holdtime_stamp;






 unsigned int class_idx:13;
 unsigned int irq_context:2;
 unsigned int trylock:1;

 unsigned int read:2;
 unsigned int check:1;
 unsigned int hardirqs_off:1;
 unsigned int sync:1;
 unsigned int references:11;
 unsigned int pin_count;
};




extern void lockdep_init(void);
extern void lockdep_reset(void);
extern void lockdep_reset_lock(struct lockdep_map *lock);
extern void lockdep_free_key_range(void *start, unsigned long size);
extern void lockdep_sys_exit(void);
extern void lockdep_set_selftest_task(struct task_struct *task);

extern void lockdep_init_task(struct task_struct *task);
extern void lockdep_register_key(struct lock_class_key *key);
extern void lockdep_unregister_key(struct lock_class_key *key);







extern void lockdep_init_map_type(struct lockdep_map *lock, const char *name,
 struct lock_class_key *key, int subclass, u8 inner, u8 outer, u8 lock_type);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void
lockdep_init_map_waits(struct lockdep_map *lock, const char *name,
         struct lock_class_key *key, int subclass, u8 inner, u8 outer)
{
 lockdep_init_map_type(lock, name, key, subclass, inner, outer, LD_LOCK_NORMAL);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void
lockdep_init_map_wait(struct lockdep_map *lock, const char *name,
        struct lock_class_key *key, int subclass, u8 inner)
{
 lockdep_init_map_waits(lock, name, key, subclass, inner, LD_WAIT_INV);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void lockdep_init_map(struct lockdep_map *lock, const char *name,
        struct lock_class_key *key, int subclass)
{
 lockdep_init_map_wait(lock, name, key, subclass, LD_WAIT_INV);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int lockdep_match_key(struct lockdep_map *lock,
        struct lock_class_key *key)
{
 return lock->key == key;
}
extern void lock_acquire(struct lockdep_map *lock, unsigned int subclass,
    int trylock, int read, int check,
    struct lockdep_map *nest_lock, unsigned long ip);

extern void lock_release(struct lockdep_map *lock, unsigned long ip);

extern void lock_sync(struct lockdep_map *lock, unsigned int subclass,
        int read, int check, struct lockdep_map *nest_lock,
        unsigned long ip);
extern int lock_is_held_type(const struct lockdep_map *lock, int read);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int lock_is_held(const struct lockdep_map *lock)
{
 return lock_is_held_type(lock, -1);
}




extern void lock_set_class(struct lockdep_map *lock, const char *name,
      struct lock_class_key *key, unsigned int subclass,
      unsigned long ip);




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void lock_set_subclass(struct lockdep_map *lock,
  unsigned int subclass, unsigned long ip)
{
 lock_set_class(lock, lock->name, lock->key, subclass, ip);
}

extern void lock_downgrade(struct lockdep_map *lock, unsigned long ip);



extern struct pin_cookie lock_pin_lock(struct lockdep_map *lock);
extern void lock_repin_lock(struct lockdep_map *lock, struct pin_cookie);
extern void lock_unpin_lock(struct lockdep_map *lock, struct pin_cookie);
void lockdep_set_lock_cmp_fn(struct lockdep_map *, lock_cmp_fn, lock_print_fn);






enum xhlock_context_t {
 XHLOCK_HARD,
 XHLOCK_SOFT,
 XHLOCK_CTX_NR,
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void lockdep_invariant_state(bool force) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void lockdep_free_task(struct task_struct *task) {}



extern void lock_contended(struct lockdep_map *lock, unsigned long ip);
extern void lock_acquired(struct lockdep_map *lock, unsigned long ip);
extern void print_irqtrace_events(struct task_struct *curr);
extern unsigned int force_read_lock_recursive;





extern bool read_lock_is_recursive(void);
extern __attribute__((__section__(".discard"))) __attribute__((unused)) char __pcpu_scope_hardirqs_enabled; extern __attribute__((section(".data..percpu" ""))) __typeof__(int) hardirqs_enabled;
extern __attribute__((__section__(".discard"))) __attribute__((unused)) char __pcpu_scope_hardirq_context; extern __attribute__((section(".data..percpu" ""))) __typeof__(int) hardirq_context;
extern __attribute__((__section__(".discard"))) __attribute__((unused)) char __pcpu_scope_lockdep_recursion; extern __attribute__((section(".data..percpu" ""))) __typeof__(unsigned int) lockdep_recursion;
void lockdep_rcu_suspicious(const char *file, const int line, const char *s);


typedef struct spinlock {
 union {
  struct raw_spinlock rlock;



  struct {
   u8 __padding[(__builtin_offsetof(struct raw_spinlock, dep_map))];
   struct lockdep_map dep_map;
  };

 };
} spinlock_t;
typedef struct {
 arch_rwlock_t raw_lock;

 unsigned int magic, owner_cpu;
 void *owner;


 struct lockdep_map dep_map;

} rwlock_t;





















static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void queued_spin_unlock(struct qspinlock *lock)
{
 do { __attribute__((__noreturn__)) extern void __compiletime_assert_50(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(lock->locked) == sizeof(char) || sizeof(lock->locked) == sizeof(short) || sizeof(lock->locked) == sizeof(int) || sizeof(lock->locked) == sizeof(long)))) __compiletime_assert_50(); } while (0);
 __asm__ __volatile__("dbar %0 " : : "I"(0b00000) : "memory");
 do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_51(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(lock->locked) == sizeof(char) || sizeof(lock->locked) == sizeof(short) || sizeof(lock->locked) == sizeof(int) || sizeof(lock->locked) == sizeof(long)) || sizeof(lock->locked) == sizeof(long long))) __compiletime_assert_51(); } while (0); do { *(volatile typeof(lock->locked) *)&(lock->locked) = (0); } while (0); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int queued_spin_is_locked(struct qspinlock *lock)
{




 return atomic_read(&lock->val);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int queued_spin_value_unlocked(struct qspinlock lock)
{
 return !atomic_read(&lock.val);
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int queued_spin_is_contended(struct qspinlock *lock)
{
 return atomic_read(&lock->val) & ~(((1U << 8) - 1) << 0);
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int queued_spin_trylock(struct qspinlock *lock)
{
 int val = atomic_read(&lock->val);

 if (__builtin_expect(!!(val), 0))
  return 0;

 return __builtin_expect(!!(atomic_try_cmpxchg_acquire(&lock->val, &val, (1U << 0))), 1);
}

extern void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val);






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void queued_spin_lock(struct qspinlock *lock)
{
 int val = 0;

 if (__builtin_expect(!!(atomic_try_cmpxchg_acquire(&lock->val, &val, (1U << 0))), 1))
  return;

 queued_spin_lock_slowpath(lock, val);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool virt_spin_lock(struct qspinlock *lock)
{
 return false;
}
extern void queued_read_lock_slowpath(struct qrwlock *lock);
extern void queued_write_lock_slowpath(struct qrwlock *lock);






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int queued_read_trylock(struct qrwlock *lock)
{
 int cnts;

 cnts = atomic_read(&lock->cnts);
 if (__builtin_expect(!!(!(cnts & 0x1ff)), 1)) {
  cnts = (u32)atomic_add_return_acquire((1U << 9), &lock->cnts);
  if (__builtin_expect(!!(!(cnts & 0x1ff)), 1))
   return 1;
  atomic_sub((1U << 9), &lock->cnts);
 }
 return 0;
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int queued_write_trylock(struct qrwlock *lock)
{
 int cnts;

 cnts = atomic_read(&lock->cnts);
 if (__builtin_expect(!!(cnts), 0))
  return 0;

 return __builtin_expect(!!(atomic_try_cmpxchg_acquire(&lock->cnts, &cnts, 0x0ff)), 1);

}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void queued_read_lock(struct qrwlock *lock)
{
 int cnts;

 cnts = atomic_add_return_acquire((1U << 9), &lock->cnts);
 if (__builtin_expect(!!(!(cnts & 0x1ff)), 1))
  return;


 queued_read_lock_slowpath(lock);
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void queued_write_lock(struct qrwlock *lock)
{
 int cnts = 0;

 if (__builtin_expect(!!(atomic_try_cmpxchg_acquire(&lock->cnts, &cnts, 0x0ff)), 1))
  return;

 queued_write_lock_slowpath(lock);
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void queued_read_unlock(struct qrwlock *lock)
{



 (void)atomic_sub_return_release((1U << 9), &lock->cnts);
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void queued_write_unlock(struct qrwlock *lock)
{
 do { do { } while (0); do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_52(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&lock->wlocked) == sizeof(char) || sizeof(*&lock->wlocked) == sizeof(short) || sizeof(*&lock->wlocked) == sizeof(int) || sizeof(*&lock->wlocked) == sizeof(long)))) __compiletime_assert_52(); } while (0); __asm__ __volatile__("dbar %0 " : : "I"(0b10010) : "memory"); do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_53(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*&lock->wlocked) == sizeof(char) || sizeof(*&lock->wlocked) == sizeof(short) || sizeof(*&lock->wlocked) == sizeof(int) || sizeof(*&lock->wlocked) == sizeof(long)) || sizeof(*&lock->wlocked) == sizeof(long long))) __compiletime_assert_53(); } while (0); do { *(volatile typeof(*&lock->wlocked) *)&(*&lock->wlocked) = (0); } while (0); } while (0); } while (0); } while (0);
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int queued_rwlock_is_contended(struct qrwlock *lock)
{
 return queued_spin_is_locked(&lock->wait_lock);
}





  extern void __raw_spin_lock_init(raw_spinlock_t *lock, const char *name,
       struct lock_class_key *key, short inner);
 extern void do_raw_spin_lock(raw_spinlock_t *lock) ;
 extern int do_raw_spin_trylock(raw_spinlock_t *lock);
 extern void do_raw_spin_unlock(raw_spinlock_t *lock) ;
  extern void __rwlock_init(rwlock_t *lock, const char *name,
       struct lock_class_key *key);
 extern void do_raw_read_lock(rwlock_t *lock) ;
 extern int do_raw_read_trylock(rwlock_t *lock);
 extern void do_raw_read_unlock(rwlock_t *lock) ;
 extern void do_raw_write_lock(rwlock_t *lock) ;
 extern int do_raw_write_trylock(rwlock_t *lock);
 extern void do_raw_write_unlock(rwlock_t *lock) ;






int in_lock_functions(unsigned long addr);



void __attribute__((__section__(".spinlock.text"))) _raw_spin_lock(raw_spinlock_t *lock) ;
void __attribute__((__section__(".spinlock.text"))) _raw_spin_lock_nested(raw_spinlock_t *lock, int subclass)
                        ;
void __attribute__((__section__(".spinlock.text")))
_raw_spin_lock_nest_lock(raw_spinlock_t *lock, struct lockdep_map *map)
                        ;
void __attribute__((__section__(".spinlock.text"))) _raw_spin_lock_bh(raw_spinlock_t *lock) ;
void __attribute__((__section__(".spinlock.text"))) _raw_spin_lock_irq(raw_spinlock_t *lock)
                        ;

unsigned long __attribute__((__section__(".spinlock.text"))) _raw_spin_lock_irqsave(raw_spinlock_t *lock)
                        ;
unsigned long __attribute__((__section__(".spinlock.text")))
_raw_spin_lock_irqsave_nested(raw_spinlock_t *lock, int subclass)
                        ;
int __attribute__((__section__(".spinlock.text"))) _raw_spin_trylock(raw_spinlock_t *lock);
int __attribute__((__section__(".spinlock.text"))) _raw_spin_trylock_bh(raw_spinlock_t *lock);
void __attribute__((__section__(".spinlock.text"))) _raw_spin_unlock(raw_spinlock_t *lock) ;
void __attribute__((__section__(".spinlock.text"))) _raw_spin_unlock_bh(raw_spinlock_t *lock) ;
void __attribute__((__section__(".spinlock.text"))) _raw_spin_unlock_irq(raw_spinlock_t *lock) ;
void __attribute__((__section__(".spinlock.text")))
_raw_spin_unlock_irqrestore(raw_spinlock_t *lock, unsigned long flags)
                        ;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int __raw_spin_trylock(raw_spinlock_t *lock)
{
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 if (do_raw_spin_trylock(lock)) {
  lock_acquire(&lock->dep_map, 0, 1, 0, 1, ((void *)0), (unsigned long)__builtin_return_address(0));
  return 1;
 }
 do { __asm__ __volatile__("": : :"memory"); __preempt_count_sub(1); } while (0);
 return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long __raw_spin_lock_irqsave(raw_spinlock_t *lock)
{
 unsigned long flags;

 do { do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); flags = arch_local_irq_save(); } while (0); if (!({ ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); arch_irqs_disabled_flags(flags); })) trace_hardirqs_off(); } while (0);
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 lock_acquire(&lock->dep_map, 0, 0, 0, 1, ((void *)0), (unsigned long)__builtin_return_address(0));
 do { if (!do_raw_spin_trylock(lock)) { lock_contended(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); do_raw_spin_lock(lock); } lock_acquired(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); } while (0);
 return flags;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __raw_spin_lock_irq(raw_spinlock_t *lock)
{
 do { bool was_disabled = (arch_irqs_disabled()); arch_local_irq_disable(); if (!was_disabled) trace_hardirqs_off(); } while (0);
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 lock_acquire(&lock->dep_map, 0, 0, 0, 1, ((void *)0), (unsigned long)__builtin_return_address(0));
 do { if (!do_raw_spin_trylock(lock)) { lock_contended(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); do_raw_spin_lock(lock); } lock_acquired(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __raw_spin_lock_bh(raw_spinlock_t *lock)
{
 __local_bh_disable_ip((unsigned long)__builtin_return_address(0), ((2 * (1UL << (0 + 8))) + (1UL << 0)));
 lock_acquire(&lock->dep_map, 0, 0, 0, 1, ((void *)0), (unsigned long)__builtin_return_address(0));
 do { if (!do_raw_spin_trylock(lock)) { lock_contended(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); do_raw_spin_lock(lock); } lock_acquired(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __raw_spin_lock(raw_spinlock_t *lock)
{
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 lock_acquire(&lock->dep_map, 0, 0, 0, 1, ((void *)0), (unsigned long)__builtin_return_address(0));
 do { if (!do_raw_spin_trylock(lock)) { lock_contended(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); do_raw_spin_lock(lock); } lock_acquired(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); } while (0);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __raw_spin_unlock(raw_spinlock_t *lock)
{
 lock_release(&lock->dep_map, (unsigned long)__builtin_return_address(0));
 do_raw_spin_unlock(lock);
 do { __asm__ __volatile__("": : :"memory"); __preempt_count_sub(1); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __raw_spin_unlock_irqrestore(raw_spinlock_t *lock,
         unsigned long flags)
{
 lock_release(&lock->dep_map, (unsigned long)__builtin_return_address(0));
 do_raw_spin_unlock(lock);
 do { if (!({ ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); arch_irqs_disabled_flags(flags); })) trace_hardirqs_on(); do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); do { if (__builtin_expect(!!(!arch_irqs_disabled()), 0)) warn_bogus_irq_restore(); } while (0); arch_local_irq_restore(flags); } while (0); } while (0);
 do { __asm__ __volatile__("": : :"memory"); __preempt_count_sub(1); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __raw_spin_unlock_irq(raw_spinlock_t *lock)
{
 lock_release(&lock->dep_map, (unsigned long)__builtin_return_address(0));
 do_raw_spin_unlock(lock);
 do { trace_hardirqs_on(); arch_local_irq_enable(); } while (0);
 do { __asm__ __volatile__("": : :"memory"); __preempt_count_sub(1); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __raw_spin_unlock_bh(raw_spinlock_t *lock)
{
 lock_release(&lock->dep_map, (unsigned long)__builtin_return_address(0));
 do_raw_spin_unlock(lock);
 __local_bh_enable_ip((unsigned long)__builtin_return_address(0), ((2 * (1UL << (0 + 8))) + (1UL << 0)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int __raw_spin_trylock_bh(raw_spinlock_t *lock)
{
 __local_bh_disable_ip((unsigned long)__builtin_return_address(0), ((2 * (1UL << (0 + 8))) + (1UL << 0)));
 if (do_raw_spin_trylock(lock)) {
  lock_acquire(&lock->dep_map, 0, 1, 0, 1, ((void *)0), (unsigned long)__builtin_return_address(0));
  return 1;
 }
 __local_bh_enable_ip((unsigned long)__builtin_return_address(0), ((2 * (1UL << (0 + 8))) + (1UL << 0)));
 return 0;
}



void __attribute__((__section__(".spinlock.text"))) _raw_read_lock(rwlock_t *lock) ;
void __attribute__((__section__(".spinlock.text"))) _raw_write_lock(rwlock_t *lock) ;
void __attribute__((__section__(".spinlock.text"))) _raw_write_lock_nested(rwlock_t *lock, int subclass) ;
void __attribute__((__section__(".spinlock.text"))) _raw_read_lock_bh(rwlock_t *lock) ;
void __attribute__((__section__(".spinlock.text"))) _raw_write_lock_bh(rwlock_t *lock) ;
void __attribute__((__section__(".spinlock.text"))) _raw_read_lock_irq(rwlock_t *lock) ;
void __attribute__((__section__(".spinlock.text"))) _raw_write_lock_irq(rwlock_t *lock) ;
unsigned long __attribute__((__section__(".spinlock.text"))) _raw_read_lock_irqsave(rwlock_t *lock)
                       ;
unsigned long __attribute__((__section__(".spinlock.text"))) _raw_write_lock_irqsave(rwlock_t *lock)
                       ;
int __attribute__((__section__(".spinlock.text"))) _raw_read_trylock(rwlock_t *lock);
int __attribute__((__section__(".spinlock.text"))) _raw_write_trylock(rwlock_t *lock);
void __attribute__((__section__(".spinlock.text"))) _raw_read_unlock(rwlock_t *lock) ;
void __attribute__((__section__(".spinlock.text"))) _raw_write_unlock(rwlock_t *lock) ;
void __attribute__((__section__(".spinlock.text"))) _raw_read_unlock_bh(rwlock_t *lock) ;
void __attribute__((__section__(".spinlock.text"))) _raw_write_unlock_bh(rwlock_t *lock) ;
void __attribute__((__section__(".spinlock.text"))) _raw_read_unlock_irq(rwlock_t *lock) ;
void __attribute__((__section__(".spinlock.text"))) _raw_write_unlock_irq(rwlock_t *lock) ;
void __attribute__((__section__(".spinlock.text")))
_raw_read_unlock_irqrestore(rwlock_t *lock, unsigned long flags)
                       ;
void __attribute__((__section__(".spinlock.text")))
_raw_write_unlock_irqrestore(rwlock_t *lock, unsigned long flags)
                       ;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int __raw_read_trylock(rwlock_t *lock)
{
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 if (do_raw_read_trylock(lock)) {
  do { if (read_lock_is_recursive()) lock_acquire(&lock->dep_map, 0, 1, 2, 1, ((void *)0), (unsigned long)__builtin_return_address(0)); else lock_acquire(&lock->dep_map, 0, 1, 1, 1, ((void *)0), (unsigned long)__builtin_return_address(0)); } while (0);
  return 1;
 }
 do { __asm__ __volatile__("": : :"memory"); __preempt_count_sub(1); } while (0);
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int __raw_write_trylock(rwlock_t *lock)
{
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 if (do_raw_write_trylock(lock)) {
  lock_acquire(&lock->dep_map, 0, 1, 0, 1, ((void *)0), (unsigned long)__builtin_return_address(0));
  return 1;
 }
 do { __asm__ __volatile__("": : :"memory"); __preempt_count_sub(1); } while (0);
 return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __raw_read_lock(rwlock_t *lock)
{
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 do { if (read_lock_is_recursive()) lock_acquire(&lock->dep_map, 0, 0, 2, 1, ((void *)0), (unsigned long)__builtin_return_address(0)); else lock_acquire(&lock->dep_map, 0, 0, 1, 1, ((void *)0), (unsigned long)__builtin_return_address(0)); } while (0);
 do { if (!do_raw_read_trylock(lock)) { lock_contended(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); do_raw_read_lock(lock); } lock_acquired(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long __raw_read_lock_irqsave(rwlock_t *lock)
{
 unsigned long flags;

 do { do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); flags = arch_local_irq_save(); } while (0); if (!({ ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); arch_irqs_disabled_flags(flags); })) trace_hardirqs_off(); } while (0);
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 do { if (read_lock_is_recursive()) lock_acquire(&lock->dep_map, 0, 0, 2, 1, ((void *)0), (unsigned long)__builtin_return_address(0)); else lock_acquire(&lock->dep_map, 0, 0, 1, 1, ((void *)0), (unsigned long)__builtin_return_address(0)); } while (0);
 do { if (!do_raw_read_trylock(lock)) { lock_contended(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); do_raw_read_lock(lock); } lock_acquired(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); } while (0);
 return flags;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __raw_read_lock_irq(rwlock_t *lock)
{
 do { bool was_disabled = (arch_irqs_disabled()); arch_local_irq_disable(); if (!was_disabled) trace_hardirqs_off(); } while (0);
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 do { if (read_lock_is_recursive()) lock_acquire(&lock->dep_map, 0, 0, 2, 1, ((void *)0), (unsigned long)__builtin_return_address(0)); else lock_acquire(&lock->dep_map, 0, 0, 1, 1, ((void *)0), (unsigned long)__builtin_return_address(0)); } while (0);
 do { if (!do_raw_read_trylock(lock)) { lock_contended(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); do_raw_read_lock(lock); } lock_acquired(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __raw_read_lock_bh(rwlock_t *lock)
{
 __local_bh_disable_ip((unsigned long)__builtin_return_address(0), ((2 * (1UL << (0 + 8))) + (1UL << 0)));
 do { if (read_lock_is_recursive()) lock_acquire(&lock->dep_map, 0, 0, 2, 1, ((void *)0), (unsigned long)__builtin_return_address(0)); else lock_acquire(&lock->dep_map, 0, 0, 1, 1, ((void *)0), (unsigned long)__builtin_return_address(0)); } while (0);
 do { if (!do_raw_read_trylock(lock)) { lock_contended(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); do_raw_read_lock(lock); } lock_acquired(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long __raw_write_lock_irqsave(rwlock_t *lock)
{
 unsigned long flags;

 do { do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); flags = arch_local_irq_save(); } while (0); if (!({ ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); arch_irqs_disabled_flags(flags); })) trace_hardirqs_off(); } while (0);
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 lock_acquire(&lock->dep_map, 0, 0, 0, 1, ((void *)0), (unsigned long)__builtin_return_address(0));
 do { if (!do_raw_write_trylock(lock)) { lock_contended(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); do_raw_write_lock(lock); } lock_acquired(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); } while (0);
 return flags;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __raw_write_lock_irq(rwlock_t *lock)
{
 do { bool was_disabled = (arch_irqs_disabled()); arch_local_irq_disable(); if (!was_disabled) trace_hardirqs_off(); } while (0);
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 lock_acquire(&lock->dep_map, 0, 0, 0, 1, ((void *)0), (unsigned long)__builtin_return_address(0));
 do { if (!do_raw_write_trylock(lock)) { lock_contended(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); do_raw_write_lock(lock); } lock_acquired(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __raw_write_lock_bh(rwlock_t *lock)
{
 __local_bh_disable_ip((unsigned long)__builtin_return_address(0), ((2 * (1UL << (0 + 8))) + (1UL << 0)));
 lock_acquire(&lock->dep_map, 0, 0, 0, 1, ((void *)0), (unsigned long)__builtin_return_address(0));
 do { if (!do_raw_write_trylock(lock)) { lock_contended(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); do_raw_write_lock(lock); } lock_acquired(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __raw_write_lock(rwlock_t *lock)
{
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 lock_acquire(&lock->dep_map, 0, 0, 0, 1, ((void *)0), (unsigned long)__builtin_return_address(0));
 do { if (!do_raw_write_trylock(lock)) { lock_contended(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); do_raw_write_lock(lock); } lock_acquired(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __raw_write_lock_nested(rwlock_t *lock, int subclass)
{
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 lock_acquire(&lock->dep_map, subclass, 0, 0, 1, ((void *)0), (unsigned long)__builtin_return_address(0));
 do { if (!do_raw_write_trylock(lock)) { lock_contended(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); do_raw_write_lock(lock); } lock_acquired(&(lock)->dep_map, (unsigned long)__builtin_return_address(0)); } while (0);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __raw_write_unlock(rwlock_t *lock)
{
 lock_release(&lock->dep_map, (unsigned long)__builtin_return_address(0));
 do_raw_write_unlock(lock);
 do { __asm__ __volatile__("": : :"memory"); __preempt_count_sub(1); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __raw_read_unlock(rwlock_t *lock)
{
 lock_release(&lock->dep_map, (unsigned long)__builtin_return_address(0));
 do_raw_read_unlock(lock);
 do { __asm__ __volatile__("": : :"memory"); __preempt_count_sub(1); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void
__raw_read_unlock_irqrestore(rwlock_t *lock, unsigned long flags)
{
 lock_release(&lock->dep_map, (unsigned long)__builtin_return_address(0));
 do_raw_read_unlock(lock);
 do { if (!({ ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); arch_irqs_disabled_flags(flags); })) trace_hardirqs_on(); do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); do { if (__builtin_expect(!!(!arch_irqs_disabled()), 0)) warn_bogus_irq_restore(); } while (0); arch_local_irq_restore(flags); } while (0); } while (0);
 do { __asm__ __volatile__("": : :"memory"); __preempt_count_sub(1); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __raw_read_unlock_irq(rwlock_t *lock)
{
 lock_release(&lock->dep_map, (unsigned long)__builtin_return_address(0));
 do_raw_read_unlock(lock);
 do { trace_hardirqs_on(); arch_local_irq_enable(); } while (0);
 do { __asm__ __volatile__("": : :"memory"); __preempt_count_sub(1); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __raw_read_unlock_bh(rwlock_t *lock)
{
 lock_release(&lock->dep_map, (unsigned long)__builtin_return_address(0));
 do_raw_read_unlock(lock);
 __local_bh_enable_ip((unsigned long)__builtin_return_address(0), ((2 * (1UL << (0 + 8))) + (1UL << 0)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __raw_write_unlock_irqrestore(rwlock_t *lock,
          unsigned long flags)
{
 lock_release(&lock->dep_map, (unsigned long)__builtin_return_address(0));
 do_raw_write_unlock(lock);
 do { if (!({ ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); arch_irqs_disabled_flags(flags); })) trace_hardirqs_on(); do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); do { if (__builtin_expect(!!(!arch_irqs_disabled()), 0)) warn_bogus_irq_restore(); } while (0); arch_local_irq_restore(flags); } while (0); } while (0);
 do { __asm__ __volatile__("": : :"memory"); __preempt_count_sub(1); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __raw_write_unlock_irq(rwlock_t *lock)
{
 lock_release(&lock->dep_map, (unsigned long)__builtin_return_address(0));
 do_raw_write_unlock(lock);
 do { trace_hardirqs_on(); arch_local_irq_enable(); } while (0);
 do { __asm__ __volatile__("": : :"memory"); __preempt_count_sub(1); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __raw_write_unlock_bh(rwlock_t *lock)
{
 lock_release(&lock->dep_map, (unsigned long)__builtin_return_address(0));
 do_raw_write_unlock(lock);
 __local_bh_enable_ip((unsigned long)__builtin_return_address(0), ((2 * (1UL << (0 + 8))) + (1UL << 0)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) raw_spinlock_t *spinlock_check(spinlock_t *lock)
{
 return &lock->rlock;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void spin_lock(spinlock_t *lock)
{
 _raw_spin_lock(&lock->rlock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void spin_lock_bh(spinlock_t *lock)
{
 _raw_spin_lock_bh(&lock->rlock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int spin_trylock(spinlock_t *lock)
{
 return (_raw_spin_trylock(&lock->rlock));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void spin_lock_irq(spinlock_t *lock)
{
 _raw_spin_lock_irq(&lock->rlock);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void spin_unlock(spinlock_t *lock)
{
 _raw_spin_unlock(&lock->rlock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void spin_unlock_bh(spinlock_t *lock)
{
 _raw_spin_unlock_bh(&lock->rlock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void spin_unlock_irq(spinlock_t *lock)
{
 _raw_spin_unlock_irq(&lock->rlock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
{
 do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); _raw_spin_unlock_irqrestore(&lock->rlock, flags); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int spin_trylock_bh(spinlock_t *lock)
{
 return (_raw_spin_trylock_bh(&lock->rlock));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int spin_trylock_irq(spinlock_t *lock)
{
 return ({ do { bool was_disabled = (arch_irqs_disabled()); arch_local_irq_disable(); if (!was_disabled) trace_hardirqs_off(); } while (0); (_raw_spin_trylock(&lock->rlock)) ? 1 : ({ do { trace_hardirqs_on(); arch_local_irq_enable(); } while (0); 0; }); });
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int spin_is_locked(spinlock_t *lock)
{
 return queued_spin_is_locked(&(&lock->rlock)->raw_lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int spin_is_contended(spinlock_t *lock)
{
 return queued_spin_is_contended(&(&lock->rlock)->raw_lock);
}
extern int _atomic_dec_and_lock(atomic_t *atomic, spinlock_t *lock);



extern int _atomic_dec_and_lock_irqsave(atomic_t *atomic, spinlock_t *lock,
     unsigned long *flags);



extern int _atomic_dec_and_raw_lock(atomic_t *atomic, raw_spinlock_t *lock);



extern int _atomic_dec_and_raw_lock_irqsave(atomic_t *atomic, raw_spinlock_t *lock,
     unsigned long *flags);



int __alloc_bucket_spinlocks(spinlock_t **locks, unsigned int *lock_mask,
        size_t max_size, unsigned int cpu_mult,
        gfp_t gfp, const char *name,
        struct lock_class_key *key);
void free_bucket_spinlocks(spinlock_t *locks);

typedef struct { raw_spinlock_t *lock; ; } class_raw_spinlock_t; static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void class_raw_spinlock_destructor(class_raw_spinlock_t *_T) { if (_T->lock) { _raw_spin_unlock(_T->lock); } } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) class_raw_spinlock_t class_raw_spinlock_constructor(raw_spinlock_t *l) { class_raw_spinlock_t _t = { .lock = l }, *_T = &_t; _raw_spin_lock(_T->lock); return _t; }



typedef struct { raw_spinlock_t *lock; ; } class_raw_spinlock_nested_t; static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void class_raw_spinlock_nested_destructor(class_raw_spinlock_nested_t *_T) { if (_T->lock) { _raw_spin_unlock(_T->lock); } } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) class_raw_spinlock_nested_t class_raw_spinlock_nested_constructor(raw_spinlock_t *l) { class_raw_spinlock_nested_t _t = { .lock = l }, *_T = &_t; _raw_spin_lock_nested(_T->lock, 1); return _t; }



typedef struct { raw_spinlock_t *lock; ; } class_raw_spinlock_irq_t; static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void class_raw_spinlock_irq_destructor(class_raw_spinlock_irq_t *_T) { if (_T->lock) { _raw_spin_unlock_irq(_T->lock); } } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) class_raw_spinlock_irq_t class_raw_spinlock_irq_constructor(raw_spinlock_t *l) { class_raw_spinlock_irq_t _t = { .lock = l }, *_T = &_t; _raw_spin_lock_irq(_T->lock); return _t; }



typedef struct { raw_spinlock_t *lock; unsigned long flags; } class_raw_spinlock_irqsave_t; static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void class_raw_spinlock_irqsave_destructor(class_raw_spinlock_irqsave_t *_T) { if (_T->lock) { do { ({ unsigned long __dummy; typeof(_T->flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); _raw_spin_unlock_irqrestore(_T->lock, _T->flags); } while (0); } } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) class_raw_spinlock_irqsave_t class_raw_spinlock_irqsave_constructor(raw_spinlock_t *l) { class_raw_spinlock_irqsave_t _t = { .lock = l }, *_T = &_t; do { ({ unsigned long __dummy; typeof(_T->flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); _T->flags = _raw_spin_lock_irqsave(_T->lock); } while (0); return _t; }




typedef struct { spinlock_t *lock; ; } class_spinlock_t; static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void class_spinlock_destructor(class_spinlock_t *_T) { if (_T->lock) { spin_unlock(_T->lock); } } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) class_spinlock_t class_spinlock_constructor(spinlock_t *l) { class_spinlock_t _t = { .lock = l }, *_T = &_t; spin_lock(_T->lock); return _t; }



typedef struct { spinlock_t *lock; ; } class_spinlock_irq_t; static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void class_spinlock_irq_destructor(class_spinlock_irq_t *_T) { if (_T->lock) { spin_unlock_irq(_T->lock); } } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) class_spinlock_irq_t class_spinlock_irq_constructor(spinlock_t *l) { class_spinlock_irq_t _t = { .lock = l }, *_T = &_t; spin_lock_irq(_T->lock); return _t; }



typedef struct { spinlock_t *lock; unsigned long flags; } class_spinlock_irqsave_t; static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void class_spinlock_irqsave_destructor(class_spinlock_irqsave_t *_T) { if (_T->lock) { spin_unlock_irqrestore(_T->lock, _T->flags); } } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) class_spinlock_irqsave_t class_spinlock_irqsave_constructor(spinlock_t *l) { class_spinlock_irqsave_t _t = { .lock = l }, *_T = &_t; do { do { ({ unsigned long __dummy; typeof(_T->flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); _T->flags = _raw_spin_lock_irqsave(spinlock_check(_T->lock)); } while (0); } while (0); return _t; }

struct hlist_nulls_head {
 struct hlist_nulls_node *first;
};

struct hlist_nulls_node {
 struct hlist_nulls_node *next, **pprev;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int is_a_nulls(const struct hlist_nulls_node *ptr)
{
 return ((unsigned long)ptr & 1);
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long get_nulls_value(const struct hlist_nulls_node *ptr)
{
 return ((unsigned long)ptr) >> 1;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int hlist_nulls_unhashed(const struct hlist_nulls_node *h)
{
 return !h->pprev;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int hlist_nulls_unhashed_lockless(const struct hlist_nulls_node *h)
{
 return !({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_54(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(h->pprev) == sizeof(char) || sizeof(h->pprev) == sizeof(short) || sizeof(h->pprev) == sizeof(int) || sizeof(h->pprev) == sizeof(long)) || sizeof(h->pprev) == sizeof(long long))) __compiletime_assert_54(); } while (0); (*(const volatile typeof( _Generic((h->pprev), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (h->pprev))) *)&(h->pprev)); });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int hlist_nulls_empty(const struct hlist_nulls_head *h)
{
 return is_a_nulls(({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_55(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(h->first) == sizeof(char) || sizeof(h->first) == sizeof(short) || sizeof(h->first) == sizeof(int) || sizeof(h->first) == sizeof(long)) || sizeof(h->first) == sizeof(long long))) __compiletime_assert_55(); } while (0); (*(const volatile typeof( _Generic((h->first), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (h->first))) *)&(h->first)); }));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void hlist_nulls_add_head(struct hlist_nulls_node *n,
     struct hlist_nulls_head *h)
{
 struct hlist_nulls_node *first = h->first;

 n->next = first;
 do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_56(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(n->pprev) == sizeof(char) || sizeof(n->pprev) == sizeof(short) || sizeof(n->pprev) == sizeof(int) || sizeof(n->pprev) == sizeof(long)) || sizeof(n->pprev) == sizeof(long long))) __compiletime_assert_56(); } while (0); do { *(volatile typeof(n->pprev) *)&(n->pprev) = (&h->first); } while (0); } while (0);
 h->first = n;
 if (!is_a_nulls(first))
  do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_57(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(first->pprev) == sizeof(char) || sizeof(first->pprev) == sizeof(short) || sizeof(first->pprev) == sizeof(int) || sizeof(first->pprev) == sizeof(long)) || sizeof(first->pprev) == sizeof(long long))) __compiletime_assert_57(); } while (0); do { *(volatile typeof(first->pprev) *)&(first->pprev) = (&n->next); } while (0); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __hlist_nulls_del(struct hlist_nulls_node *n)
{
 struct hlist_nulls_node *next = n->next;
 struct hlist_nulls_node **pprev = n->pprev;

 do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_58(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*pprev) == sizeof(char) || sizeof(*pprev) == sizeof(short) || sizeof(*pprev) == sizeof(int) || sizeof(*pprev) == sizeof(long)) || sizeof(*pprev) == sizeof(long long))) __compiletime_assert_58(); } while (0); do { *(volatile typeof(*pprev) *)&(*pprev) = (next); } while (0); } while (0);
 if (!is_a_nulls(next))
  do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_59(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(next->pprev) == sizeof(char) || sizeof(next->pprev) == sizeof(short) || sizeof(next->pprev) == sizeof(int) || sizeof(next->pprev) == sizeof(long)) || sizeof(next->pprev) == sizeof(long long))) __compiletime_assert_59(); } while (0); do { *(volatile typeof(next->pprev) *)&(next->pprev) = (pprev); } while (0); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void hlist_nulls_del(struct hlist_nulls_node *n)
{
 __hlist_nulls_del(n);
 do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_60(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(n->pprev) == sizeof(char) || sizeof(n->pprev) == sizeof(short) || sizeof(n->pprev) == sizeof(int) || sizeof(n->pprev) == sizeof(long)) || sizeof(n->pprev) == sizeof(long long))) __compiletime_assert_60(); } while (0); do { *(volatile typeof(n->pprev) *)&(n->pprev) = (((void *) 0x122 + 0)); } while (0); } while (0);
}

typedef struct wait_queue_entry wait_queue_entry_t;

typedef int (*wait_queue_func_t)(struct wait_queue_entry *wq_entry, unsigned mode, int flags, void *key);
int default_wake_function(struct wait_queue_entry *wq_entry, unsigned mode, int flags, void *key);
struct wait_queue_entry {
 unsigned int flags;
 void *private;
 wait_queue_func_t func;
 struct list_head entry;
};

struct wait_queue_head {
 spinlock_t lock;
 struct list_head head;
};
typedef struct wait_queue_head wait_queue_head_t;

struct task_struct;
extern void __init_waitqueue_head(struct wait_queue_head *wq_head, const char *name, struct lock_class_key *);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void init_waitqueue_entry(struct wait_queue_entry *wq_entry, struct task_struct *p)
{
 wq_entry->flags = 0;
 wq_entry->private = p;
 wq_entry->func = default_wake_function;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void
init_waitqueue_func_entry(struct wait_queue_entry *wq_entry, wait_queue_func_t func)
{
 wq_entry->flags = 0;
 wq_entry->private = ((void *)0);
 wq_entry->func = func;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int waitqueue_active(struct wait_queue_head *wq_head)
{
 return !list_empty(&wq_head->head);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool wq_has_single_sleeper(struct wait_queue_head *wq_head)
{
 return list_is_singular(&wq_head->head);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool wq_has_sleeper(struct wait_queue_head *wq_head)
{







 do { do { } while (0); __asm__ __volatile__("dbar %0 " : : "I"(0b10000) : "memory"); } while (0);
 return waitqueue_active(wq_head);
}

extern void add_wait_queue(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry);
extern void add_wait_queue_exclusive(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry);
extern void add_wait_queue_priority(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry);
extern void remove_wait_queue(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __add_wait_queue(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry)
{
 struct list_head *head = &wq_head->head;
 struct wait_queue_entry *wq;

 for (wq = ({ void *__mptr = (void *)((&wq_head->head)->next); _Static_assert(__builtin_types_compatible_p(typeof(*((&wq_head->head)->next)), typeof(((typeof(*wq) *)0)->entry)) || __builtin_types_compatible_p(typeof(*((&wq_head->head)->next)), typeof(void)), "pointer type mismatch in container_of()"); ((typeof(*wq) *)(__mptr - __builtin_offsetof(typeof(*wq), entry))); }); !(&wq->entry == (&wq_head->head)); wq = ({ void *__mptr = (void *)((wq)->entry.next); _Static_assert(__builtin_types_compatible_p(typeof(*((wq)->entry.next)), typeof(((typeof(*(wq)) *)0)->entry)) || __builtin_types_compatible_p(typeof(*((wq)->entry.next)), typeof(void)), "pointer type mismatch in container_of()"); ((typeof(*(wq)) *)(__mptr - __builtin_offsetof(typeof(*(wq)), entry))); })) {
  if (!(wq->flags & 0x20))
   break;
  head = &wq->entry;
 }
 list_add(&wq_entry->entry, head);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void
__add_wait_queue_exclusive(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry)
{
 wq_entry->flags |= 0x01;
 __add_wait_queue(wq_head, wq_entry);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __add_wait_queue_entry_tail(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry)
{
 list_add_tail(&wq_entry->entry, &wq_head->head);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void
__add_wait_queue_entry_tail_exclusive(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry)
{
 wq_entry->flags |= 0x01;
 __add_wait_queue_entry_tail(wq_head, wq_entry);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void
__remove_wait_queue(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry)
{
 list_del(&wq_entry->entry);
}

int __wake_up(struct wait_queue_head *wq_head, unsigned int mode, int nr, void *key);
void __wake_up_on_current_cpu(struct wait_queue_head *wq_head, unsigned int mode, void *key);
void __wake_up_locked_key(struct wait_queue_head *wq_head, unsigned int mode, void *key);
void __wake_up_locked_key_bookmark(struct wait_queue_head *wq_head,
  unsigned int mode, void *key, wait_queue_entry_t *bookmark);
void __wake_up_sync_key(struct wait_queue_head *wq_head, unsigned int mode, void *key);
void __wake_up_locked_sync_key(struct wait_queue_head *wq_head, unsigned int mode, void *key);
void __wake_up_locked(struct wait_queue_head *wq_head, unsigned int mode, int nr);
void __wake_up_sync(struct wait_queue_head *wq_head, unsigned int mode);
void __wake_up_pollfree(struct wait_queue_head *wq_head);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void wake_up_pollfree(struct wait_queue_head *wq_head)
{







 if (waitqueue_active(wq_head))
  __wake_up_pollfree(wq_head);
}
extern void init_wait_entry(struct wait_queue_entry *wq_entry, int flags);
extern int do_wait_intr(wait_queue_head_t *, wait_queue_entry_t *);
extern int do_wait_intr_irq(wait_queue_head_t *, wait_queue_entry_t *);
void prepare_to_wait(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry, int state);
bool prepare_to_wait_exclusive(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry, int state);
long prepare_to_wait_event(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry, int state);
void finish_wait(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry);
long wait_woken(struct wait_queue_entry *wq_entry, unsigned mode, long timeout);
int woken_wake_function(struct wait_queue_entry *wq_entry, unsigned mode, int sync, void *key);
int autoremove_wake_function(struct wait_queue_entry *wq_entry, unsigned mode, int sync, void *key);
typedef int (*task_call_f)(struct task_struct *p, void *arg);
extern int task_call_func(struct task_struct *p, task_call_f func, void *arg);


















struct optimistic_spin_node {
 struct optimistic_spin_node *next, *prev;
 int locked;
 int cpu;
};

struct optimistic_spin_queue {




 atomic_t tail;
};






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void osq_lock_init(struct optimistic_spin_queue *lock)
{
 atomic_set(&lock->tail, (0));
}

extern bool osq_lock(struct optimistic_spin_queue *lock);
extern void osq_unlock(struct optimistic_spin_queue *lock);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool osq_is_locked(struct optimistic_spin_queue *lock)
{
 return atomic_read(&lock->tail) != (0);
}
struct mutex {
 atomic_long_t owner;
 raw_spinlock_t wait_lock;

 struct optimistic_spin_queue osq;

 struct list_head wait_list;

 void *magic;


 struct lockdep_map dep_map;

};






extern void mutex_destroy(struct mutex *lock);
extern void __mutex_init(struct mutex *lock, const char *name,
    struct lock_class_key *key);







extern bool mutex_is_locked(struct mutex *lock);
extern void mutex_lock_nested(struct mutex *lock, unsigned int subclass);
extern void _mutex_lock_nest_lock(struct mutex *lock, struct lockdep_map *nest_lock);

extern int __attribute__((__warn_unused_result__)) mutex_lock_interruptible_nested(struct mutex *lock,
     unsigned int subclass);
extern int __attribute__((__warn_unused_result__)) mutex_lock_killable_nested(struct mutex *lock,
     unsigned int subclass);
extern void mutex_lock_io_nested(struct mutex *lock, unsigned int subclass);
extern int mutex_trylock(struct mutex *lock);
extern void mutex_unlock(struct mutex *lock);

extern int atomic_dec_and_mutex_lock(atomic_t *cnt, struct mutex *lock);

typedef struct mutex * class_mutex_t; static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void class_mutex_destructor(struct mutex * *p) { struct mutex * _T = *p; mutex_unlock(_T); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) struct mutex * class_mutex_constructor(struct mutex * _T) { struct mutex * t = ({ mutex_lock_nested(_T, 0); _T; }); return t; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __free_mutex(void *p) { struct mutex * _T = *(struct mutex * *)p; if (_T) mutex_unlock(_T); }
typedef struct seqcount {
 unsigned sequence;

 struct lockdep_map dep_map;

} seqcount_t;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __seqcount_init(seqcount_t *s, const char *name,
       struct lock_class_key *key)
{



 lockdep_init_map(&s->dep_map, name, key, 0);
 s->sequence = 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void seqcount_lockdep_reader_access(const seqcount_t *s)
{
 seqcount_t *l = (seqcount_t *)s;
 unsigned long flags;

 do { do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); flags = arch_local_irq_save(); } while (0); if (!({ ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); arch_irqs_disabled_flags(flags); })) trace_hardirqs_off(); } while (0);
 lock_acquire(&l->dep_map, 0, 0, 2, 1, ((void *)0), (unsigned long)__builtin_return_address(0));
 lock_release(&l->dep_map, (unsigned long)__builtin_return_address(0));
 do { if (!({ ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); arch_irqs_disabled_flags(flags); })) trace_hardirqs_on(); do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); do { if (__builtin_expect(!!(!arch_irqs_disabled()), 0)) warn_bogus_irq_restore(); } while (0); arch_local_irq_restore(flags); } while (0); } while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) seqcount_t *__seqprop_ptr(seqcount_t *s)
{
 return s;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned __seqprop_sequence(const seqcount_t *s)
{
 return ({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_61(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(s->sequence) == sizeof(char) || sizeof(s->sequence) == sizeof(short) || sizeof(s->sequence) == sizeof(int) || sizeof(s->sequence) == sizeof(long)) || sizeof(s->sequence) == sizeof(long long))) __compiletime_assert_61(); } while (0); (*(const volatile typeof( _Generic((s->sequence), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (s->sequence))) *)&(s->sequence)); });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool __seqprop_preemptible(const seqcount_t *s)
{
 return false;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __seqprop_assert(const seqcount_t *s)
{
 do { ({ int __ret_warn_on = !!(1 && (debug_locks && !({ typeof(lockdep_recursion) pscr_ret__; do { const void *__vpp_verify = (typeof((&(lockdep_recursion)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(lockdep_recursion)) { case 1: pscr_ret__ = ({ typeof(lockdep_recursion) __retval; __retval = (typeof(lockdep_recursion))__percpu_read(&(lockdep_recursion), sizeof(lockdep_recursion)); __retval; }); break; case 2: pscr_ret__ = ({ typeof(lockdep_recursion) __retval; __retval = (typeof(lockdep_recursion))__percpu_read(&(lockdep_recursion), sizeof(lockdep_recursion)); __retval; }); break; case 4: pscr_ret__ = ({ typeof(lockdep_recursion) __retval; __retval = (typeof(lockdep_recursion))__percpu_read(&(lockdep_recursion), sizeof(lockdep_recursion)); __retval; }); break; case 8: pscr_ret__ = ({ typeof(lockdep_recursion) __retval; __retval = (typeof(lockdep_recursion))__percpu_read(&(lockdep_recursion), sizeof(lockdep_recursion)); __retval; }); break; default: __bad_size_call_parameter(); break; } pscr_ret__; })) && (preempt_count() == 0 && ({ typeof(hardirqs_enabled) pscr_ret__; do { const void *__vpp_verify = (typeof((&(hardirqs_enabled)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(hardirqs_enabled)) { case 1: pscr_ret__ = ({ typeof(hardirqs_enabled) __retval; __retval = (typeof(hardirqs_enabled))__percpu_read(&(hardirqs_enabled), sizeof(hardirqs_enabled)); __retval; }); break; case 2: pscr_ret__ = ({ typeof(hardirqs_enabled) __retval; __retval = (typeof(hardirqs_enabled))__percpu_read(&(hardirqs_enabled), sizeof(hardirqs_enabled)); __retval; }); break; case 4: pscr_ret__ = ({ typeof(hardirqs_enabled) __retval; __retval = (typeof(hardirqs_enabled))__percpu_read(&(hardirqs_enabled), sizeof(hardirqs_enabled)); __retval; }); break; case 8: pscr_ret__ = ({ typeof(hardirqs_enabled) __retval; __retval = (typeof(hardirqs_enabled))__percpu_read(&(hardirqs_enabled), sizeof(hardirqs_enabled)); __retval; }); break; default: __bad_size_call_parameter(); break; } pscr_ret__; }))); if (__builtin_expect(!!(__ret_warn_on), 0)) do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/seqlock.h\"; .popsection; .long 10002b - .; .short 269; .short (1 << 0)|((1 << 1) | ((9) << 8)); .popsection; 10001: break 1");; do { } while(0); } while (0); __builtin_expect(!!(__ret_warn_on), 0); }); } while (0);
}



typedef struct seqcount_raw_spinlock { seqcount_t seqcount; raw_spinlock_t *lock; } seqcount_raw_spinlock_t; static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) seqcount_t * __seqprop_raw_spinlock_ptr(seqcount_raw_spinlock_t *s) { return &s->seqcount; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) unsigned __seqprop_raw_spinlock_sequence(const seqcount_raw_spinlock_t *s) { unsigned seq = ({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_62(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(s->seqcount.sequence) == sizeof(char) || sizeof(s->seqcount.sequence) == sizeof(short) || sizeof(s->seqcount.sequence) == sizeof(int) || sizeof(s->seqcount.sequence) == sizeof(long)) || sizeof(s->seqcount.sequence) == sizeof(long long))) __compiletime_assert_62(); } while (0); (*(const volatile typeof( _Generic((s->seqcount.sequence), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (s->seqcount.sequence))) *)&(s->seqcount.sequence)); }); if (!0) return seq; if (false && __builtin_expect(!!(seq & 1), 0)) { _raw_spin_lock(s->lock); _raw_spin_unlock(s->lock); seq = ({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_63(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(s->seqcount.sequence) == sizeof(char) || sizeof(s->seqcount.sequence) == sizeof(short) || sizeof(s->seqcount.sequence) == sizeof(int) || sizeof(s->seqcount.sequence) == sizeof(long)) || sizeof(s->seqcount.sequence) == sizeof(long long))) __compiletime_assert_63(); } while (0); (*(const volatile typeof( _Generic((s->seqcount.sequence), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (s->seqcount.sequence))) *)&(s->seqcount.sequence)); }); } return seq; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool __seqprop_raw_spinlock_preemptible(const seqcount_raw_spinlock_t *s) { if (!0) return false; return false; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __seqprop_raw_spinlock_assert(const seqcount_raw_spinlock_t *s) { do { ({ int __ret_warn_on = !!(debug_locks && !(lock_is_held(&(s->lock)->dep_map) != 0)); if (__builtin_expect(!!(__ret_warn_on), 0)) do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/seqlock.h\"; .popsection; .long 10002b - .; .short 274; .short (1 << 0)|(((9) << 8)); .popsection; 10001: break 1");; do { } while(0); } while (0); __builtin_expect(!!(__ret_warn_on), 0); }); } while (0); }
typedef struct seqcount_spinlock { seqcount_t seqcount; spinlock_t *lock; } seqcount_spinlock_t; static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) seqcount_t * __seqprop_spinlock_ptr(seqcount_spinlock_t *s) { return &s->seqcount; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) unsigned __seqprop_spinlock_sequence(const seqcount_spinlock_t *s) { unsigned seq = ({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_64(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(s->seqcount.sequence) == sizeof(char) || sizeof(s->seqcount.sequence) == sizeof(short) || sizeof(s->seqcount.sequence) == sizeof(int) || sizeof(s->seqcount.sequence) == sizeof(long)) || sizeof(s->seqcount.sequence) == sizeof(long long))) __compiletime_assert_64(); } while (0); (*(const volatile typeof( _Generic((s->seqcount.sequence), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (s->seqcount.sequence))) *)&(s->seqcount.sequence)); }); if (!0) return seq; if (0 && __builtin_expect(!!(seq & 1), 0)) { spin_lock(s->lock); spin_unlock(s->lock); seq = ({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_65(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(s->seqcount.sequence) == sizeof(char) || sizeof(s->seqcount.sequence) == sizeof(short) || sizeof(s->seqcount.sequence) == sizeof(int) || sizeof(s->seqcount.sequence) == sizeof(long)) || sizeof(s->seqcount.sequence) == sizeof(long long))) __compiletime_assert_65(); } while (0); (*(const volatile typeof( _Generic((s->seqcount.sequence), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (s->seqcount.sequence))) *)&(s->seqcount.sequence)); }); } return seq; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool __seqprop_spinlock_preemptible(const seqcount_spinlock_t *s) { if (!0) return 0; return false; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __seqprop_spinlock_assert(const seqcount_spinlock_t *s) { do { ({ int __ret_warn_on = !!(debug_locks && !(lock_is_held(&(s->lock)->dep_map) != 0)); if (__builtin_expect(!!(__ret_warn_on), 0)) do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/seqlock.h\"; .popsection; .long 10002b - .; .short 275; .short (1 << 0)|(((9) << 8)); .popsection; 10001: break 1");; do { } while(0); } while (0); __builtin_expect(!!(__ret_warn_on), 0); }); } while (0); }
typedef struct seqcount_rwlock { seqcount_t seqcount; rwlock_t *lock; } seqcount_rwlock_t; static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) seqcount_t * __seqprop_rwlock_ptr(seqcount_rwlock_t *s) { return &s->seqcount; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) unsigned __seqprop_rwlock_sequence(const seqcount_rwlock_t *s) { unsigned seq = ({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_66(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(s->seqcount.sequence) == sizeof(char) || sizeof(s->seqcount.sequence) == sizeof(short) || sizeof(s->seqcount.sequence) == sizeof(int) || sizeof(s->seqcount.sequence) == sizeof(long)) || sizeof(s->seqcount.sequence) == sizeof(long long))) __compiletime_assert_66(); } while (0); (*(const volatile typeof( _Generic((s->seqcount.sequence), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (s->seqcount.sequence))) *)&(s->seqcount.sequence)); }); if (!0) return seq; if (0 && __builtin_expect(!!(seq & 1), 0)) { _raw_read_lock(s->lock); _raw_read_unlock(s->lock); seq = ({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_67(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(s->seqcount.sequence) == sizeof(char) || sizeof(s->seqcount.sequence) == sizeof(short) || sizeof(s->seqcount.sequence) == sizeof(int) || sizeof(s->seqcount.sequence) == sizeof(long)) || sizeof(s->seqcount.sequence) == sizeof(long long))) __compiletime_assert_67(); } while (0); (*(const volatile typeof( _Generic((s->seqcount.sequence), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (s->seqcount.sequence))) *)&(s->seqcount.sequence)); }); } return seq; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool __seqprop_rwlock_preemptible(const seqcount_rwlock_t *s) { if (!0) return 0; return false; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __seqprop_rwlock_assert(const seqcount_rwlock_t *s) { do { ({ int __ret_warn_on = !!(debug_locks && !(lock_is_held(&(s->lock)->dep_map) != 0)); if (__builtin_expect(!!(__ret_warn_on), 0)) do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/seqlock.h\"; .popsection; .long 10002b - .; .short 276; .short (1 << 0)|(((9) << 8)); .popsection; 10001: break 1");; do { } while(0); } while (0); __builtin_expect(!!(__ret_warn_on), 0); }); } while (0); }
typedef struct seqcount_mutex { seqcount_t seqcount; struct mutex *lock; } seqcount_mutex_t; static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) seqcount_t * __seqprop_mutex_ptr(seqcount_mutex_t *s) { return &s->seqcount; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) unsigned __seqprop_mutex_sequence(const seqcount_mutex_t *s) { unsigned seq = ({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_68(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(s->seqcount.sequence) == sizeof(char) || sizeof(s->seqcount.sequence) == sizeof(short) || sizeof(s->seqcount.sequence) == sizeof(int) || sizeof(s->seqcount.sequence) == sizeof(long)) || sizeof(s->seqcount.sequence) == sizeof(long long))) __compiletime_assert_68(); } while (0); (*(const volatile typeof( _Generic((s->seqcount.sequence), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (s->seqcount.sequence))) *)&(s->seqcount.sequence)); }); if (!0) return seq; if (true && __builtin_expect(!!(seq & 1), 0)) { mutex_lock_nested(s->lock, 0); mutex_unlock(s->lock); seq = ({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_69(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(s->seqcount.sequence) == sizeof(char) || sizeof(s->seqcount.sequence) == sizeof(short) || sizeof(s->seqcount.sequence) == sizeof(int) || sizeof(s->seqcount.sequence) == sizeof(long)) || sizeof(s->seqcount.sequence) == sizeof(long long))) __compiletime_assert_69(); } while (0); (*(const volatile typeof( _Generic((s->seqcount.sequence), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (s->seqcount.sequence))) *)&(s->seqcount.sequence)); }); } return seq; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool __seqprop_mutex_preemptible(const seqcount_mutex_t *s) { if (!0) return true; return false; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __seqprop_mutex_assert(const seqcount_mutex_t *s) { do { ({ int __ret_warn_on = !!(debug_locks && !(lock_is_held(&(s->lock)->dep_map) != 0)); if (__builtin_expect(!!(__ret_warn_on), 0)) do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/seqlock.h\"; .popsection; .long 10002b - .; .short 277; .short (1 << 0)|(((9) << 8)); .popsection; 10001: break 1");; do { } while(0); } while (0); __builtin_expect(!!(__ret_warn_on), 0); }); } while (0); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int do___read_seqcount_retry(const seqcount_t *s, unsigned start)
{
 kcsan_atomic_next(0);
 return __builtin_expect(!!(({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_70(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(s->sequence) == sizeof(char) || sizeof(s->sequence) == sizeof(short) || sizeof(s->sequence) == sizeof(int) || sizeof(s->sequence) == sizeof(long)) || sizeof(s->sequence) == sizeof(long long))) __compiletime_assert_70(); } while (0); (*(const volatile typeof( _Generic((s->sequence), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (s->sequence))) *)&(s->sequence)); }) != start), 0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int do_read_seqcount_retry(const seqcount_t *s, unsigned start)
{
 do { do { } while (0); __asm__ __volatile__("dbar %0 " : : "I"(0b10101) : "memory"); } while (0);
 return do___read_seqcount_retry(s, start);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void do_raw_write_seqcount_begin(seqcount_t *s)
{
 kcsan_nestable_atomic_begin();
 s->sequence++;
 do { do { } while (0); __asm__ __volatile__("dbar %0 " : : "I"(0b11010) : "memory"); } while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void do_raw_write_seqcount_end(seqcount_t *s)
{
 do { do { } while (0); __asm__ __volatile__("dbar %0 " : : "I"(0b11010) : "memory"); } while (0);
 s->sequence++;
 kcsan_nestable_atomic_end();
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void do_write_seqcount_begin_nested(seqcount_t *s, int subclass)
{
 lock_acquire(&s->dep_map, subclass, 0, 0, 1, ((void *)0), (unsigned long)__builtin_return_address(0));
 do_raw_write_seqcount_begin(s);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void do_write_seqcount_begin(seqcount_t *s)
{
 do_write_seqcount_begin_nested(s, 0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void do_write_seqcount_end(seqcount_t *s)
{
 lock_release(&s->dep_map, (unsigned long)__builtin_return_address(0));
 do_raw_write_seqcount_end(s);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void do_raw_write_seqcount_barrier(seqcount_t *s)
{
 kcsan_nestable_atomic_begin();
 s->sequence++;
 do { do { } while (0); __asm__ __volatile__("dbar %0 " : : "I"(0b11010) : "memory"); } while (0);
 s->sequence++;
 kcsan_nestable_atomic_end();
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void do_write_seqcount_invalidate(seqcount_t *s)
{
 do { do { } while (0); __asm__ __volatile__("dbar %0 " : : "I"(0b11010) : "memory"); } while (0);
 kcsan_nestable_atomic_begin();
 s->sequence+=2;
 kcsan_nestable_atomic_end();
}
typedef struct {
 seqcount_t seqcount;
} seqcount_latch_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) unsigned raw_read_seqcount_latch(const seqcount_latch_t *s)
{




 return ({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_71(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(s->seqcount.sequence) == sizeof(char) || sizeof(s->seqcount.sequence) == sizeof(short) || sizeof(s->seqcount.sequence) == sizeof(int) || sizeof(s->seqcount.sequence) == sizeof(long)) || sizeof(s->seqcount.sequence) == sizeof(long long))) __compiletime_assert_71(); } while (0); (*(const volatile typeof( _Generic((s->seqcount.sequence), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (s->seqcount.sequence))) *)&(s->seqcount.sequence)); });
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int
raw_read_seqcount_latch_retry(const seqcount_latch_t *s, unsigned start)
{
 do { do { } while (0); __asm__ __volatile__("dbar %0 " : : "I"(0b10101) : "memory"); } while (0);
 return __builtin_expect(!!(({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_72(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(s->seqcount.sequence) == sizeof(char) || sizeof(s->seqcount.sequence) == sizeof(short) || sizeof(s->seqcount.sequence) == sizeof(int) || sizeof(s->seqcount.sequence) == sizeof(long)) || sizeof(s->seqcount.sequence) == sizeof(long long))) __compiletime_assert_72(); } while (0); (*(const volatile typeof( _Generic((s->seqcount.sequence), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (s->seqcount.sequence))) *)&(s->seqcount.sequence)); }) != start), 0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void raw_write_seqcount_latch(seqcount_latch_t *s)
{
 do { do { } while (0); __asm__ __volatile__("dbar %0 " : : "I"(0b11010) : "memory"); } while (0);
 s->seqcount.sequence++;
 do { do { } while (0); __asm__ __volatile__("dbar %0 " : : "I"(0b11010) : "memory"); } while (0);
}
typedef struct {




 seqcount_spinlock_t seqcount;
 spinlock_t lock;
} seqlock_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned read_seqbegin(const seqlock_t *sl)
{
 unsigned ret = ({ seqcount_lockdep_reader_access(_Generic(*(&sl->seqcount), seqcount_t: __seqprop_ptr((void *)(&sl->seqcount)), seqcount_raw_spinlock_t: __seqprop_raw_spinlock_ptr((void *)((&sl->seqcount))), seqcount_spinlock_t: __seqprop_spinlock_ptr((void *)((&sl->seqcount))), seqcount_rwlock_t: __seqprop_rwlock_ptr((void *)((&sl->seqcount))), seqcount_mutex_t: __seqprop_mutex_ptr((void *)((&sl->seqcount))))); ({ unsigned _seq = ({ unsigned __seq; while ((__seq = _Generic(*(&sl->seqcount), seqcount_t: __seqprop_sequence((void *)(&sl->seqcount)), seqcount_raw_spinlock_t: __seqprop_raw_spinlock_sequence((void *)((&sl->seqcount))), seqcount_spinlock_t: __seqprop_spinlock_sequence((void *)((&sl->seqcount))), seqcount_rwlock_t: __seqprop_rwlock_sequence((void *)((&sl->seqcount))), seqcount_mutex_t: __seqprop_mutex_sequence((void *)((&sl->seqcount))))) & 1) __asm__ __volatile__("": : :"memory"); kcsan_atomic_next(1000); __seq; }); do { do { } while (0); __asm__ __volatile__("dbar %0 " : : "I"(0b10101) : "memory"); } while (0); _seq; }); });

 kcsan_atomic_next(0);
 kcsan_flat_atomic_begin();
 return ret;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned read_seqretry(const seqlock_t *sl, unsigned start)
{




 kcsan_flat_atomic_end();

 return do_read_seqcount_retry(_Generic(*(&sl->seqcount), seqcount_t: __seqprop_ptr((void *)(&sl->seqcount)), seqcount_raw_spinlock_t: __seqprop_raw_spinlock_ptr((void *)((&sl->seqcount))), seqcount_spinlock_t: __seqprop_spinlock_ptr((void *)((&sl->seqcount))), seqcount_rwlock_t: __seqprop_rwlock_ptr((void *)((&sl->seqcount))), seqcount_mutex_t: __seqprop_mutex_ptr((void *)((&sl->seqcount)))), start);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void write_seqlock(seqlock_t *sl)
{
 spin_lock(&sl->lock);
 do_write_seqcount_begin(&sl->seqcount.seqcount);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void write_sequnlock(seqlock_t *sl)
{
 do_write_seqcount_end(&sl->seqcount.seqcount);
 spin_unlock(&sl->lock);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void write_seqlock_bh(seqlock_t *sl)
{
 spin_lock_bh(&sl->lock);
 do_write_seqcount_begin(&sl->seqcount.seqcount);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void write_sequnlock_bh(seqlock_t *sl)
{
 do_write_seqcount_end(&sl->seqcount.seqcount);
 spin_unlock_bh(&sl->lock);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void write_seqlock_irq(seqlock_t *sl)
{
 spin_lock_irq(&sl->lock);
 do_write_seqcount_begin(&sl->seqcount.seqcount);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void write_sequnlock_irq(seqlock_t *sl)
{
 do_write_seqcount_end(&sl->seqcount.seqcount);
 spin_unlock_irq(&sl->lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long __write_seqlock_irqsave(seqlock_t *sl)
{
 unsigned long flags;

 do { do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); flags = _raw_spin_lock_irqsave(spinlock_check(&sl->lock)); } while (0); } while (0);
 do_write_seqcount_begin(&sl->seqcount.seqcount);
 return flags;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void
write_sequnlock_irqrestore(seqlock_t *sl, unsigned long flags)
{
 do_write_seqcount_end(&sl->seqcount.seqcount);
 spin_unlock_irqrestore(&sl->lock, flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void read_seqlock_excl(seqlock_t *sl)
{
 spin_lock(&sl->lock);
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void read_sequnlock_excl(seqlock_t *sl)
{
 spin_unlock(&sl->lock);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void read_seqlock_excl_bh(seqlock_t *sl)
{
 spin_lock_bh(&sl->lock);
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void read_sequnlock_excl_bh(seqlock_t *sl)
{
 spin_unlock_bh(&sl->lock);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void read_seqlock_excl_irq(seqlock_t *sl)
{
 spin_lock_irq(&sl->lock);
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void read_sequnlock_excl_irq(seqlock_t *sl)
{
 spin_unlock_irq(&sl->lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long __read_seqlock_excl_irqsave(seqlock_t *sl)
{
 unsigned long flags;

 do { do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); flags = _raw_spin_lock_irqsave(spinlock_check(&sl->lock)); } while (0); } while (0);
 return flags;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void
read_sequnlock_excl_irqrestore(seqlock_t *sl, unsigned long flags)
{
 spin_unlock_irqrestore(&sl->lock, flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void read_seqbegin_or_lock(seqlock_t *lock, int *seq)
{
 if (!(*seq & 1))
  *seq = read_seqbegin(lock);
 else
  read_seqlock_excl(lock);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int need_seqretry(seqlock_t *lock, int seq)
{
 return !(seq & 1) && read_seqretry(lock, seq);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void done_seqretry(seqlock_t *lock, int seq)
{
 if (seq & 1)
  read_sequnlock_excl(lock);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long
read_seqbegin_or_lock_irqsave(seqlock_t *lock, int *seq)
{
 unsigned long flags = 0;

 if (!(*seq & 1))
  *seq = read_seqbegin(lock);
 else
  do { flags = __read_seqlock_excl_irqsave(lock); } while (0);

 return flags;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void
done_seqretry_irqrestore(seqlock_t *lock, int seq, unsigned long flags)
{
 if (seq & 1)
  read_sequnlock_excl_irqrestore(lock, flags);
}













extern unsigned int __invalid_size_argument_for_IOC;






extern int nr_irqs;
extern struct irq_desc *irq_to_desc(unsigned int irq);
unsigned int irq_get_next_irq(unsigned int offset);
struct rand_pool_info {
 int entropy_count;
 int buf_size;
 __u32 buf[];
};

struct notifier_block;

void add_device_randomness(const void *buf, size_t len);
void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) add_bootloader_randomness(const void *buf, size_t len);
void add_input_randomness(unsigned int type, unsigned int code,
     unsigned int value) ;
void add_interrupt_randomness(int irq) ;
void add_hwgenerator_randomness(const void *buf, size_t len, size_t entropy, bool sleep_after);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void add_latent_entropy(void)
{



 add_device_randomness(((void *)0), 0);

}


void add_vmfork_randomness(const void *unique_vm_id, size_t len);
int register_random_vmfork_notifier(struct notifier_block *nb);
int unregister_random_vmfork_notifier(struct notifier_block *nb);





void get_random_bytes(void *buf, size_t len);
u8 get_random_u8(void);
u16 get_random_u16(void);
u32 get_random_u32(void);
u64 get_random_u64(void);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long get_random_long(void)
{

 return get_random_u64();



}

u32 __get_random_u32_below(u32 ceil);






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u32 get_random_u32_below(u32 ceil)
{
 if (!__builtin_constant_p(ceil))
  return __get_random_u32_below(ceil);
 do { __attribute__((__noreturn__)) extern void __compiletime_assert_73(void) __attribute__((__error__("get_random_u32_below() must take ceil > 0"))); if (!(!(!ceil))) __compiletime_assert_73(); } while (0);
 if (ceil <= 1)
  return 0;
 for (;;) {
  if (ceil <= 1U << 8) {
   u32 mult = ceil * get_random_u8();
   if (__builtin_expect(!!(is_power_of_2(ceil) || (u8)mult >= (1U << 8) % ceil), 1))
    return mult >> 8;
  } else if (ceil <= 1U << 16) {
   u32 mult = ceil * get_random_u16();
   if (__builtin_expect(!!(is_power_of_2(ceil) || (u16)mult >= (1U << 16) % ceil), 1))
    return mult >> 16;
  } else {
   u64 mult = (u64)ceil * get_random_u32();
   if (__builtin_expect(!!(is_power_of_2(ceil) || (u32)mult >= -ceil % ceil), 1))
    return mult >> 32;
  }
 }
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u32 get_random_u32_above(u32 floor)
{
 do { __attribute__((__noreturn__)) extern void __compiletime_assert_74(void) __attribute__((__error__("get_random_u32_above() must take floor < U32_MAX"))); if (!(!(__builtin_constant_p(floor) && floor == ((u32)~0U)))) __compiletime_assert_74(); } while (0);

 return floor + 1 + get_random_u32_below(((u32)~0U) - floor);
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u32 get_random_u32_inclusive(u32 floor, u32 ceil)
{
 do { __attribute__((__noreturn__)) extern void __compiletime_assert_75(void) __attribute__((__error__("get_random_u32_inclusive() must take floor <= ceil"))); if (!(!(__builtin_constant_p(floor) && __builtin_constant_p(ceil) && (floor > ceil || ceil - floor == ((u32)~0U))))) __compiletime_assert_75(); } while (0);


 return floor + get_random_u32_below(ceil - floor + 1);
}

void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) random_init_early(const char *command_line);
void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) random_init(void);
bool rng_is_initialized(void);
int wait_for_random_bytes(void);
int execute_with_initialized_rng(struct notifier_block *nb);



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int get_random_bytes_wait(void *buf, size_t nbytes)
{
 int ret = wait_for_random_bytes();
 get_random_bytes(buf, nbytes);
 return ret;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int get_random_u8_wait(u8 *out) { int ret = wait_for_random_bytes(); if (__builtin_expect(!!(ret), 0)) return ret; *out = get_random_u8(); return 0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int get_random_u16_wait(u16 *out) { int ret = wait_for_random_bytes(); if (__builtin_expect(!!(ret), 0)) return ret; *out = get_random_u16(); return 0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int get_random_u32_wait(u32 *out) { int ret = wait_for_random_bytes(); if (__builtin_expect(!!(ret), 0)) return ret; *out = get_random_u32(); return 0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int get_random_u64_wait(u32 *out) { int ret = wait_for_random_bytes(); if (__builtin_expect(!!(ret), 0)) return ret; *out = get_random_u64(); return 0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int get_random_long_wait(unsigned long *out) { int ret = wait_for_random_bytes(); if (__builtin_expect(!!(ret), 0)) return ret; *out = get_random_long(); return 0; }







bool __do_once_start(bool *done, unsigned long *flags);
void __do_once_done(bool *done, struct static_key_true *once_key,
      unsigned long *flags, struct module *mod);


bool __do_once_sleepable_start(bool *done);
void __do_once_sleepable_done(bool *done, struct static_key_true *once_key,
         struct module *mod);











struct page;
struct vm_area_struct;
struct mm_struct;
struct vma_iterator;

void dump_page(struct page *page, const char *reason);
void dump_vma(const struct vm_area_struct *vma);
void dump_mm(const struct mm_struct *mm);
void vma_iter_dump_tree(const struct vma_iterator *vmi);



typedef struct {
 u64 val;
} pfn_t;
extern void *pcpu_base_addr;
extern const unsigned long *pcpu_unit_offsets;

struct pcpu_group_info {
 int nr_units;
 unsigned long base_offset;
 unsigned int *cpu_map;

};

struct pcpu_alloc_info {
 size_t static_size;
 size_t reserved_size;
 size_t dyn_size;
 size_t unit_size;
 size_t atom_size;
 size_t alloc_size;
 size_t __ai_size;
 int nr_groups;
 struct pcpu_group_info groups[];
};

enum pcpu_fc {
 PCPU_FC_AUTO,
 PCPU_FC_EMBED,
 PCPU_FC_PAGE,

 PCPU_FC_NR,
};
extern const char * const pcpu_fc_names[PCPU_FC_NR];

extern enum pcpu_fc pcpu_chosen_fc;

typedef int (pcpu_fc_cpu_to_node_fn_t)(int cpu);
typedef int (pcpu_fc_cpu_distance_fn_t)(unsigned int from, unsigned int to);

extern struct pcpu_alloc_info * __attribute__((__section__(".init.text"))) __attribute__((__cold__)) pcpu_alloc_alloc_info(int nr_groups,
            int nr_units);
extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) pcpu_free_alloc_info(struct pcpu_alloc_info *ai);

extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) pcpu_setup_first_chunk(const struct pcpu_alloc_info *ai,
      void *base_addr);

extern int __attribute__((__section__(".init.text"))) __attribute__((__cold__)) pcpu_embed_first_chunk(size_t reserved_size, size_t dyn_size,
    size_t atom_size,
    pcpu_fc_cpu_distance_fn_t cpu_distance_fn,
    pcpu_fc_cpu_to_node_fn_t cpu_to_nd_fn);


void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) pcpu_populate_pte(unsigned long addr);
extern int __attribute__((__section__(".init.text"))) __attribute__((__cold__)) pcpu_page_first_chunk(size_t reserved_size,
    pcpu_fc_cpu_to_node_fn_t cpu_to_nd_fn);


extern void *__alloc_reserved_percpu(size_t size, size_t align) __attribute__((__alloc_size__(1))) __attribute__((__malloc__));
extern bool __is_kernel_percpu_address(unsigned long addr, unsigned long *can_addr);
extern bool is_kernel_percpu_address(unsigned long addr);





extern void *__alloc_percpu_gfp(size_t size, size_t align, gfp_t gfp) __attribute__((__alloc_size__(1))) __attribute__((__malloc__));
extern void *__alloc_percpu(size_t size, size_t align) __attribute__((__alloc_size__(1))) __attribute__((__malloc__));
extern void free_percpu(void *__pdata);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __free_free_percpu(void *p) { void * _T = *(void * *)p; free_percpu(_T); }

extern phys_addr_t per_cpu_ptr_to_phys(void *addr);
extern unsigned long pcpu_nr_pages(void);

struct rnd_state {
 __u32 s1, s2, s3, s4;
};

u32 prandom_u32_state(struct rnd_state *state);
void prandom_bytes_state(struct rnd_state *state, void *buf, size_t nbytes);
void prandom_seed_full_state(struct rnd_state *pcpu_state);







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u32 __seed(u32 x, u32 m)
{
 return (x < m) ? x + m : x;
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void prandom_seed_state(struct rnd_state *state, u64 seed)
{
 u32 i = ((seed >> 32) ^ (seed << 10) ^ seed) & 0xffffffffUL;

 state->s1 = __seed(i, 2U);
 state->s2 = __seed(i, 8U);
 state->s3 = __seed(i, 16U);
 state->s4 = __seed(i, 128U);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u32 next_pseudo_random32(u32 seed)
{
 return seed * 1664525 + 1013904223;
}


int random_prepare_cpu(unsigned int cpu);
int random_online_cpu(unsigned int cpu);



extern const struct file_operations random_fops, urandom_fops;

typedef struct { unsigned long bits[((((1 << 6)) + ((sizeof(long) * 8)) - 1) / ((sizeof(long) * 8)))]; } nodemask_t;
extern nodemask_t _unused_nodemask_arg_;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned int __nodemask_pr_numnodes(const nodemask_t *m)
{
 return m ? (1 << 6) : 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) const unsigned long *__nodemask_pr_bits(const nodemask_t *m)
{
 return m ? m->bits : ((void *)0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __node_set(int node, volatile nodemask_t *dstp)
{
 set_bit(node, dstp->bits);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __node_clear(int node, volatile nodemask_t *dstp)
{
 clear_bit(node, dstp->bits);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __nodes_setall(nodemask_t *dstp, unsigned int nbits)
{
 bitmap_fill(dstp->bits, nbits);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __nodes_clear(nodemask_t *dstp, unsigned int nbits)
{
 bitmap_zero(dstp->bits, nbits);
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool __node_test_and_set(int node, nodemask_t *addr)
{
 return test_and_set_bit(node, addr->bits);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __nodes_and(nodemask_t *dstp, const nodemask_t *src1p,
     const nodemask_t *src2p, unsigned int nbits)
{
 bitmap_and(dstp->bits, src1p->bits, src2p->bits, nbits);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __nodes_or(nodemask_t *dstp, const nodemask_t *src1p,
     const nodemask_t *src2p, unsigned int nbits)
{
 bitmap_or(dstp->bits, src1p->bits, src2p->bits, nbits);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __nodes_xor(nodemask_t *dstp, const nodemask_t *src1p,
     const nodemask_t *src2p, unsigned int nbits)
{
 bitmap_xor(dstp->bits, src1p->bits, src2p->bits, nbits);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __nodes_andnot(nodemask_t *dstp, const nodemask_t *src1p,
     const nodemask_t *src2p, unsigned int nbits)
{
 bitmap_andnot(dstp->bits, src1p->bits, src2p->bits, nbits);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __nodes_complement(nodemask_t *dstp,
     const nodemask_t *srcp, unsigned int nbits)
{
 bitmap_complement(dstp->bits, srcp->bits, nbits);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool __nodes_equal(const nodemask_t *src1p,
     const nodemask_t *src2p, unsigned int nbits)
{
 return bitmap_equal(src1p->bits, src2p->bits, nbits);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool __nodes_intersects(const nodemask_t *src1p,
     const nodemask_t *src2p, unsigned int nbits)
{
 return bitmap_intersects(src1p->bits, src2p->bits, nbits);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool __nodes_subset(const nodemask_t *src1p,
     const nodemask_t *src2p, unsigned int nbits)
{
 return bitmap_subset(src1p->bits, src2p->bits, nbits);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool __nodes_empty(const nodemask_t *srcp, unsigned int nbits)
{
 return bitmap_empty(srcp->bits, nbits);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool __nodes_full(const nodemask_t *srcp, unsigned int nbits)
{
 return bitmap_full(srcp->bits, nbits);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int __nodes_weight(const nodemask_t *srcp, unsigned int nbits)
{
 return bitmap_weight(srcp->bits, nbits);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __nodes_shift_right(nodemask_t *dstp,
     const nodemask_t *srcp, int n, int nbits)
{
 bitmap_shift_right(dstp->bits, srcp->bits, n, nbits);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __nodes_shift_left(nodemask_t *dstp,
     const nodemask_t *srcp, int n, int nbits)
{
 bitmap_shift_left(dstp->bits, srcp->bits, n, nbits);
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned int __first_node(const nodemask_t *srcp)
{
 return __builtin_choose_expr(((!!(sizeof((typeof((unsigned int)((1 << 6))) *)1 == (typeof((unsigned int)(find_first_bit(srcp->bits, (1 << 6)))) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)((unsigned int)((1 << 6))) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)((unsigned int)(find_first_bit(srcp->bits, (1 << 6)))) * 0l)) : (int *)8))))), (((unsigned int)((1 << 6))) < ((unsigned int)(find_first_bit(srcp->bits, (1 << 6)))) ? ((unsigned int)((1 << 6))) : ((unsigned int)(find_first_bit(srcp->bits, (1 << 6))))), ({ typeof((unsigned int)((1 << 6))) __UNIQUE_ID___x76 = ((unsigned int)((1 << 6))); typeof((unsigned int)(find_first_bit(srcp->bits, (1 << 6)))) __UNIQUE_ID___y77 = ((unsigned int)(find_first_bit(srcp->bits, (1 << 6)))); ((__UNIQUE_ID___x76) < (__UNIQUE_ID___y77) ? (__UNIQUE_ID___x76) : (__UNIQUE_ID___y77)); }));
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned int __next_node(int n, const nodemask_t *srcp)
{
 return __builtin_choose_expr(((!!(sizeof((typeof((unsigned int)((1 << 6))) *)1 == (typeof((unsigned int)(find_next_bit(srcp->bits, (1 << 6), n+1))) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)((unsigned int)((1 << 6))) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)((unsigned int)(find_next_bit(srcp->bits, (1 << 6), n+1))) * 0l)) : (int *)8))))), (((unsigned int)((1 << 6))) < ((unsigned int)(find_next_bit(srcp->bits, (1 << 6), n+1))) ? ((unsigned int)((1 << 6))) : ((unsigned int)(find_next_bit(srcp->bits, (1 << 6), n+1)))), ({ typeof((unsigned int)((1 << 6))) __UNIQUE_ID___x78 = ((unsigned int)((1 << 6))); typeof((unsigned int)(find_next_bit(srcp->bits, (1 << 6), n+1))) __UNIQUE_ID___y79 = ((unsigned int)(find_next_bit(srcp->bits, (1 << 6), n+1))); ((__UNIQUE_ID___x78) < (__UNIQUE_ID___y79) ? (__UNIQUE_ID___x78) : (__UNIQUE_ID___y79)); }));
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned int __next_node_in(int node, const nodemask_t *srcp)
{
 unsigned int ret = __next_node(node, srcp);

 if (ret == (1 << 6))
  ret = __first_node(srcp);
 return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void init_nodemask_of_node(nodemask_t *mask, int node)
{
 __nodes_clear(&(*mask), (1 << 6));
 __node_set((node), &(*mask));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned int __first_unset_node(const nodemask_t *maskp)
{
 return __builtin_choose_expr(((!!(sizeof((typeof((unsigned int)((1 << 6))) *)1 == (typeof((unsigned int)(find_first_zero_bit(maskp->bits, (1 << 6)))) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)((unsigned int)((1 << 6))) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)((unsigned int)(find_first_zero_bit(maskp->bits, (1 << 6)))) * 0l)) : (int *)8))))), (((unsigned int)((1 << 6))) < ((unsigned int)(find_first_zero_bit(maskp->bits, (1 << 6)))) ? ((unsigned int)((1 << 6))) : ((unsigned int)(find_first_zero_bit(maskp->bits, (1 << 6))))), ({ typeof((unsigned int)((1 << 6))) __UNIQUE_ID___x80 = ((unsigned int)((1 << 6))); typeof((unsigned int)(find_first_zero_bit(maskp->bits, (1 << 6)))) __UNIQUE_ID___y81 = ((unsigned int)(find_first_zero_bit(maskp->bits, (1 << 6)))); ((__UNIQUE_ID___x80) < (__UNIQUE_ID___y81) ? (__UNIQUE_ID___x80) : (__UNIQUE_ID___y81)); }));

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int __nodemask_parse_user(const char *buf, int len,
     nodemask_t *dstp, int nbits)
{
 return bitmap_parse_user(buf, len, dstp->bits, nbits);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int __nodelist_parse(const char *buf, nodemask_t *dstp, int nbits)
{
 return bitmap_parselist(buf, dstp->bits, nbits);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int __node_remap(int oldbit,
  const nodemask_t *oldp, const nodemask_t *newp, int nbits)
{
 return bitmap_bitremap(oldbit, oldp->bits, newp->bits, nbits);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __nodes_remap(nodemask_t *dstp, const nodemask_t *srcp,
  const nodemask_t *oldp, const nodemask_t *newp, int nbits)
{
 bitmap_remap(dstp->bits, srcp->bits, oldp->bits, newp->bits, nbits);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __nodes_onto(nodemask_t *dstp, const nodemask_t *origp,
  const nodemask_t *relmapp, int nbits)
{
 bitmap_onto(dstp->bits, origp->bits, relmapp->bits, nbits);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __nodes_fold(nodemask_t *dstp, const nodemask_t *origp,
  int sz, int nbits)
{
 bitmap_fold(dstp->bits, origp->bits, sz, nbits);
}
enum node_states {
 N_POSSIBLE,
 N_ONLINE,
 N_NORMAL_MEMORY,



 N_HIGH_MEMORY = N_NORMAL_MEMORY,

 N_MEMORY,
 N_CPU,
 N_GENERIC_INITIATOR,
 NR_NODE_STATES
};






extern nodemask_t node_states[NR_NODE_STATES];


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int node_state(int node, enum node_states state)
{
 return ((__builtin_constant_p((node)) && __builtin_constant_p((uintptr_t)((node_states[state]).bits) != (uintptr_t)((void *)0)) && (uintptr_t)((node_states[state]).bits) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)((node_states[state]).bits))) ? const_test_bit((node), (node_states[state]).bits) : generic_test_bit((node), (node_states[state]).bits));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void node_set_state(int node, enum node_states state)
{
 __node_set(node, &node_states[state]);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void node_clear_state(int node, enum node_states state)
{
 __node_clear(node, &node_states[state]);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int num_node_state(enum node_states state)
{
 return __nodes_weight(&(node_states[state]), (1 << 6));
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned int next_online_node(int nid)
{
 return __next_node((nid), &(node_states[N_ONLINE]));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned int next_memory_node(int nid)
{
 return __next_node((nid), &(node_states[N_MEMORY]));
}

extern unsigned int nr_node_ids;
extern unsigned int nr_online_nodes;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void node_set_online(int nid)
{
 node_set_state(nid, N_ONLINE);
 nr_online_nodes = num_node_state(N_ONLINE);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void node_set_offline(int nid)
{
 node_clear_state(nid, N_ONLINE);
 nr_online_nodes = num_node_state(N_ONLINE);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int node_random(const nodemask_t *maskp)
{

 int w, bit;

 w = __nodes_weight(&(*maskp), (1 << 6));
 switch (w) {
 case 0:
  bit = (-1);
  break;
 case 1:
  bit = __first_node(&(*maskp));
  break;
 default:
  bit = find_nth_bit(maskp->bits, (1 << 6), get_random_u32_below(w));
  break;
 }
 return bit;



}
struct nodemask_scratch {
 nodemask_t mask1;
 nodemask_t mask2;
};
enum pageblock_bits {
 PB_migrate,
 PB_migrate_end = PB_migrate + 3 - 1,

 PB_migrate_skip,





 NR_PAGEBLOCK_BITS
};
struct page;

unsigned long get_pfnblock_flags_mask(const struct page *page,
    unsigned long pfn,
    unsigned long mask);

void set_pfnblock_flags_mask(struct page *page,
    unsigned long flags,
    unsigned long pfn,
    unsigned long mask);


















extern unsigned long vm_map_base;
extern void clear_page(void *page);
extern void copy_page(void *to, void *from);




extern unsigned long shm_align_mask;

struct page;
struct vm_area_struct;
void copy_user_highpage(struct page *to, struct page *from,
       unsigned long vaddr, struct vm_area_struct *vma);



typedef struct { unsigned long pte; } pte_t;


typedef struct page *pgtable_t;

typedef struct { unsigned long pgd; } pgd_t;






typedef struct { unsigned long pgprot; } pgprot_t;
extern int __virt_addr_valid(volatile void *kaddr);







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __attribute__((__const__)) int get_order(unsigned long size)
{
 if (__builtin_constant_p(size)) {
  if (!size)
   return 64 - 14;

  if (size < (1UL << 14))
   return 0;

  return ( __builtin_constant_p((size) - 1) ? (((size) - 1) < 2 ? 0 : 63 - __builtin_clzll((size) - 1)) : (sizeof((size) - 1) <= 4) ? __ilog2_u32((size) - 1) : __ilog2_u64((size) - 1) ) - 14 + 1;
 }

 size--;
 size >>= 14;



 return fls64(size);

}
enum {
 MM_FILEPAGES,
 MM_ANONPAGES,
 MM_SWAPENTS,
 MM_SHMEMPAGES,
 NR_MM_COUNTERS
};

struct page_frag {
 struct page *page;

 __u32 offset;
 __u32 size;




};


struct tlbflush_unmap_batch {
};









struct mutex;
typedef struct refcount_struct {
 atomic_t refs;
} refcount_t;





enum refcount_saturation_type {
 REFCOUNT_ADD_NOT_ZERO_OVF,
 REFCOUNT_ADD_OVF,
 REFCOUNT_ADD_UAF,
 REFCOUNT_SUB_UAF,
 REFCOUNT_DEC_LEAK,
};

void refcount_warn_saturate(refcount_t *r, enum refcount_saturation_type t);






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void refcount_set(refcount_t *r, int n)
{
 atomic_set(&r->refs, n);
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned int refcount_read(const refcount_t *r)
{
 return atomic_read(&r->refs);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__warn_unused_result__)) bool __refcount_add_not_zero(int i, refcount_t *r, int *oldp)
{
 int old = refcount_read(r);

 do {
  if (!old)
   break;
 } while (!atomic_try_cmpxchg_relaxed(&r->refs, &old, old + i));

 if (oldp)
  *oldp = old;

 if (__builtin_expect(!!(old < 0 || old + i < 0), 0))
  refcount_warn_saturate(r, REFCOUNT_ADD_NOT_ZERO_OVF);

 return old;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__warn_unused_result__)) bool refcount_add_not_zero(int i, refcount_t *r)
{
 return __refcount_add_not_zero(i, r, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __refcount_add(int i, refcount_t *r, int *oldp)
{
 int old = atomic_fetch_add_relaxed(i, &r->refs);

 if (oldp)
  *oldp = old;

 if (__builtin_expect(!!(!old), 0))
  refcount_warn_saturate(r, REFCOUNT_ADD_UAF);
 else if (__builtin_expect(!!(old < 0 || old + i < 0), 0))
  refcount_warn_saturate(r, REFCOUNT_ADD_OVF);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void refcount_add(int i, refcount_t *r)
{
 __refcount_add(i, r, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__warn_unused_result__)) bool __refcount_inc_not_zero(refcount_t *r, int *oldp)
{
 return __refcount_add_not_zero(1, r, oldp);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__warn_unused_result__)) bool refcount_inc_not_zero(refcount_t *r)
{
 return __refcount_inc_not_zero(r, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __refcount_inc(refcount_t *r, int *oldp)
{
 __refcount_add(1, r, oldp);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void refcount_inc(refcount_t *r)
{
 __refcount_inc(r, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__warn_unused_result__)) bool __refcount_sub_and_test(int i, refcount_t *r, int *oldp)
{
 int old = atomic_fetch_sub_release(i, &r->refs);

 if (oldp)
  *oldp = old;

 if (old == i) {
  do { do { } while (0); __asm__ __volatile__("dbar %0 " : : "I"(0b10101) : "memory"); } while (0);
  return true;
 }

 if (__builtin_expect(!!(old < 0 || old - i < 0), 0))
  refcount_warn_saturate(r, REFCOUNT_SUB_UAF);

 return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__warn_unused_result__)) bool refcount_sub_and_test(int i, refcount_t *r)
{
 return __refcount_sub_and_test(i, r, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__warn_unused_result__)) bool __refcount_dec_and_test(refcount_t *r, int *oldp)
{
 return __refcount_sub_and_test(1, r, oldp);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__warn_unused_result__)) bool refcount_dec_and_test(refcount_t *r)
{
 return __refcount_dec_and_test(r, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __refcount_dec(refcount_t *r, int *oldp)
{
 int old = atomic_fetch_sub_release(1, &r->refs);

 if (oldp)
  *oldp = old;

 if (__builtin_expect(!!(old <= 1), 0))
  refcount_warn_saturate(r, REFCOUNT_DEC_LEAK);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void refcount_dec(refcount_t *r)
{
 __refcount_dec(r, ((void *)0));
}

extern __attribute__((__warn_unused_result__)) bool refcount_dec_if_one(refcount_t *r);
extern __attribute__((__warn_unused_result__)) bool refcount_dec_not_one(refcount_t *r);
extern __attribute__((__warn_unused_result__)) bool refcount_dec_and_mutex_lock(refcount_t *r, struct mutex *lock) ;
extern __attribute__((__warn_unused_result__)) bool refcount_dec_and_lock(refcount_t *r, spinlock_t *lock) ;
extern __attribute__((__warn_unused_result__)) bool refcount_dec_and_lock_irqsave(refcount_t *r,
             spinlock_t *lock,
             unsigned long *flags) ;

struct kref {
 refcount_t refcount;
};







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kref_init(struct kref *kref)
{
 refcount_set(&kref->refcount, 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned int kref_read(const struct kref *kref)
{
 return refcount_read(&kref->refcount);
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kref_get(struct kref *kref)
{
 refcount_inc(&kref->refcount);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int kref_put(struct kref *kref, void (*release)(struct kref *kref))
{
 if (refcount_dec_and_test(&kref->refcount)) {
  release(kref);
  return 1;
 }
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int kref_put_mutex(struct kref *kref,
     void (*release)(struct kref *kref),
     struct mutex *lock)
{
 if (refcount_dec_and_mutex_lock(&kref->refcount, lock)) {
  release(kref);
  return 1;
 }
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int kref_put_lock(struct kref *kref,
    void (*release)(struct kref *kref),
    spinlock_t *lock)
{
 if (refcount_dec_and_lock(&kref->refcount, lock)) {
  release(kref);
  return 1;
 }
 return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int __attribute__((__warn_unused_result__)) kref_get_unless_zero(struct kref *kref)
{
 return refcount_inc_not_zero(&kref->refcount);
}






struct rb_node {
 unsigned long __rb_parent_color;
 struct rb_node *rb_right;
 struct rb_node *rb_left;
} __attribute__((aligned(sizeof(long))));


struct rb_root {
 struct rb_node *rb_node;
};
struct rb_root_cached {
 struct rb_root rb_root;
 struct rb_node *rb_leftmost;
};







void ct_irq_enter(void);
void ct_irq_exit(void);
void ct_irq_enter_irqson(void);
void ct_irq_exit_irqson(void);
void ct_nmi_enter(void);
void ct_nmi_exit(void);








void call_rcu(struct callback_head *head, rcu_callback_t func);
void rcu_barrier_tasks(void);
void rcu_barrier_tasks_rude(void);
void synchronize_rcu(void);

struct rcu_gp_oldstate;
unsigned long get_completed_synchronize_rcu(void);
void get_completed_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool same_state_synchronize_rcu(unsigned long oldstate1, unsigned long oldstate2)
{
 return oldstate1 == oldstate2;
}
void rcu_read_unlock_strict(void);


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __rcu_read_lock(void)
{
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __rcu_read_unlock(void)
{
 do { __asm__ __volatile__("": : :"memory"); __preempt_count_sub(1); } while (0);
 if (0)
  rcu_read_unlock_strict();
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int rcu_preempt_depth(void)
{
 return 0;
}




void call_rcu_hurry(struct callback_head *head, rcu_callback_t func);
void rcu_init(void);
extern int rcu_scheduler_active;
void rcu_sched_clock_irq(int user);
void rcu_report_dead(unsigned int cpu);
void rcutree_migrate_callbacks(int cpu);


void rcu_init_tasks_generic(void);





void rcu_sysrq_start(void);
void rcu_sysrq_end(void);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void rcu_irq_work_resched(void) { }



void rcu_init_nohz(void);
int rcu_nocb_cpu_offload(int cpu);
int rcu_nocb_cpu_deoffload(int cpu);
void rcu_nocb_flush_deferred_wakeup(void);
void call_rcu_tasks(struct callback_head *head, rcu_callback_t func);
void synchronize_rcu_tasks(void);
u8 rcu_trc_cmpxchg_need_qs(struct task_struct *t, u8 old, u8 new);
void rcu_tasks_trace_qs_blkd(struct task_struct *t);
void call_rcu_tasks_rude(struct callback_head *head, rcu_callback_t func);
void synchronize_rcu_tasks_rude(void);



void exit_tasks_rcu_start(void);
void exit_tasks_rcu_stop(void);
void exit_tasks_rcu_finish(void);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool rcu_trace_implies_rcu_gp(void) { return true; }
void rcu_softirq_qs(void);
void rcu_note_context_switch(bool preempt);
int rcu_needs_cpu(void);
void rcu_cpu_stall_reset(void);
void rcu_request_urgent_qs_task(struct task_struct *t);






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void rcu_virt_note_context_switch(void)
{
 rcu_note_context_switch(false);
}

void synchronize_rcu_expedited(void);
void kvfree_call_rcu(struct callback_head *head, void *ptr);

void rcu_barrier(void);
bool rcu_eqs_special_set(int cpu);
void rcu_momentary_dyntick_idle(void);
void kfree_rcu_scheduler_running(void);
bool rcu_gp_might_be_stalled(void);

struct rcu_gp_oldstate {
 unsigned long rgos_norm;
 unsigned long rgos_exp;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool same_state_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp1,
         struct rcu_gp_oldstate *rgosp2)
{
 return rgosp1->rgos_norm == rgosp2->rgos_norm && rgosp1->rgos_exp == rgosp2->rgos_exp;
}

unsigned long start_poll_synchronize_rcu_expedited(void);
void start_poll_synchronize_rcu_expedited_full(struct rcu_gp_oldstate *rgosp);
void cond_synchronize_rcu_expedited(unsigned long oldstate);
void cond_synchronize_rcu_expedited_full(struct rcu_gp_oldstate *rgosp);
unsigned long get_state_synchronize_rcu(void);
void get_state_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp);
unsigned long start_poll_synchronize_rcu(void);
void start_poll_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp);
bool poll_state_synchronize_rcu(unsigned long oldstate);
bool poll_state_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp);
void cond_synchronize_rcu(unsigned long oldstate);
void cond_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp);


void rcu_irq_exit_check_preempt(void);




struct task_struct;
void rcu_preempt_deferred_qs(struct task_struct *t);

void exit_rcu(void);

void rcu_scheduler_starting(void);
extern int rcu_scheduler_active;
void rcu_end_inkernel_boot(void);
bool rcu_inkernel_boot_has_ended(void);
bool rcu_is_watching(void);

void rcu_all_qs(void);



int rcutree_prepare_cpu(unsigned int cpu);
int rcutree_online_cpu(unsigned int cpu);
int rcutree_offline_cpu(unsigned int cpu);
int rcutree_dead_cpu(unsigned int cpu);
int rcutree_dying_cpu(unsigned int cpu);
void rcu_cpu_starting(unsigned int cpu);
void init_rcu_head(struct callback_head *head);
void destroy_rcu_head(struct callback_head *head);
void init_rcu_head_on_stack(struct callback_head *head);
void destroy_rcu_head_on_stack(struct callback_head *head);
bool rcu_lockdep_current_cpu_online(void);




extern struct lockdep_map rcu_lock_map;
extern struct lockdep_map rcu_bh_lock_map;
extern struct lockdep_map rcu_sched_lock_map;
extern struct lockdep_map rcu_callback_map;



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void rcu_lock_acquire(struct lockdep_map *map)
{
 lock_acquire(map, 0, 0, 2, 0, ((void *)0), ({ __label__ __here; __here: (unsigned long)&&__here; }));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void rcu_lock_release(struct lockdep_map *map)
{
 lock_release(map, ({ __label__ __here; __here: (unsigned long)&&__here; }));
}

int debug_lockdep_rcu_enabled(void);
int rcu_read_lock_held(void);
int rcu_read_lock_bh_held(void);
int rcu_read_lock_sched_held(void);
int rcu_read_lock_any_held(void);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void rcu_preempt_sleep_check(void)
{
 do { static bool __attribute__((__section__(".data.unlikely"))) __warned; if (debug_lockdep_rcu_enabled() && (lock_is_held(&rcu_lock_map)) && debug_lockdep_rcu_enabled() && !__warned) { __warned = true; lockdep_rcu_suspicious("include/linux/rcupdate.h", 376, "Illegal context switch in RCU read-side critical section"); } } while (0);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void rcu_read_lock(void)
{
 __rcu_read_lock();
 (void)0;
 rcu_lock_acquire(&rcu_lock_map);
 do { static bool __attribute__((__section__(".data.unlikely"))) __warned; if (debug_lockdep_rcu_enabled() && (!rcu_is_watching()) && debug_lockdep_rcu_enabled() && !__warned) { __warned = true; lockdep_rcu_suspicious("include/linux/rcupdate.h", 751, "rcu_read_lock() used illegally while idle"); } } while (0);

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void rcu_read_unlock(void)
{
 do { static bool __attribute__((__section__(".data.unlikely"))) __warned; if (debug_lockdep_rcu_enabled() && (!rcu_is_watching()) && debug_lockdep_rcu_enabled() && !__warned) { __warned = true; lockdep_rcu_suspicious("include/linux/rcupdate.h", 779, "rcu_read_unlock() used illegally while idle"); } } while (0);

 (void)0;
 __rcu_read_unlock();
 rcu_lock_release(&rcu_lock_map);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void rcu_read_lock_bh(void)
{
 local_bh_disable();
 (void)0;
 rcu_lock_acquire(&rcu_bh_lock_map);
 do { static bool __attribute__((__section__(".data.unlikely"))) __warned; if (debug_lockdep_rcu_enabled() && (!rcu_is_watching()) && debug_lockdep_rcu_enabled() && !__warned) { __warned = true; lockdep_rcu_suspicious("include/linux/rcupdate.h", 805, "rcu_read_lock_bh() used illegally while idle"); } } while (0);

}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void rcu_read_unlock_bh(void)
{
 do { static bool __attribute__((__section__(".data.unlikely"))) __warned; if (debug_lockdep_rcu_enabled() && (!rcu_is_watching()) && debug_lockdep_rcu_enabled() && !__warned) { __warned = true; lockdep_rcu_suspicious("include/linux/rcupdate.h", 816, "rcu_read_unlock_bh() used illegally while idle"); } } while (0);

 rcu_lock_release(&rcu_bh_lock_map);
 (void)0;
 local_bh_enable();
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void rcu_read_lock_sched(void)
{
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 (void)0;
 rcu_lock_acquire(&rcu_sched_lock_map);
 do { static bool __attribute__((__section__(".data.unlikely"))) __warned; if (debug_lockdep_rcu_enabled() && (!rcu_is_watching()) && debug_lockdep_rcu_enabled() && !__warned) { __warned = true; lockdep_rcu_suspicious("include/linux/rcupdate.h", 843, "rcu_read_lock_sched() used illegally while idle"); } } while (0);

}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((patchable_function_entry(0, 0))) void rcu_read_lock_sched_notrace(void)
{
 do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0);
 (void)0;
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void rcu_read_unlock_sched(void)
{
 do { static bool __attribute__((__section__(".data.unlikely"))) __warned; if (debug_lockdep_rcu_enabled() && (!rcu_is_watching()) && debug_lockdep_rcu_enabled() && !__warned) { __warned = true; lockdep_rcu_suspicious("include/linux/rcupdate.h", 861, "rcu_read_unlock_sched() used illegally while idle"); } } while (0);

 rcu_lock_release(&rcu_sched_lock_map);
 (void)0;
 do { __asm__ __volatile__("": : :"memory"); __preempt_count_sub(1); } while (0);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((patchable_function_entry(0, 0))) void rcu_read_unlock_sched_notrace(void)
{
 (void)0;
 do { __asm__ __volatile__("": : :"memory"); __preempt_count_sub(1); } while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void rcu_head_init(struct callback_head *rhp)
{
 rhp->func = (rcu_callback_t)~0L;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool
rcu_head_after_call_rcu(struct callback_head *rhp, rcu_callback_t f)
{
 rcu_callback_t func = ({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_82(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(rhp->func) == sizeof(char) || sizeof(rhp->func) == sizeof(short) || sizeof(rhp->func) == sizeof(int) || sizeof(rhp->func) == sizeof(long)) || sizeof(rhp->func) == sizeof(long long))) __compiletime_assert_82(); } while (0); (*(const volatile typeof( _Generic((rhp->func), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (rhp->func))) *)&(rhp->func)); });

 if (func == f)
  return true;
 ({ int __ret_warn_on = !!(func != (rcu_callback_t)~0L); if (__builtin_expect(!!(__ret_warn_on), 0)) do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/rcupdate.h\"; .popsection; .long 10002b - .; .short 1053; .short (1 << 0)|((1 << 1) | ((9) << 8)); .popsection; 10001: break 1");; do { } while(0); } while (0); __builtin_expect(!!(__ret_warn_on), 0); });
 return false;
}


extern int rcu_expedited;
extern int rcu_normal;

typedef struct { void *lock; ; } class_rcu_t; static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void class_rcu_destructor(class_rcu_t *_T) { if (_T->lock) { rcu_read_unlock(); } } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) class_rcu_t class_rcu_constructor(void) { class_rcu_t _t = { .lock = (void*)1 }, *_T __attribute__((__unused__)) = &_t; rcu_read_lock(); return _t; }
extern void rb_insert_color(struct rb_node *, struct rb_root *);
extern void rb_erase(struct rb_node *, struct rb_root *);



extern struct rb_node *rb_next(const struct rb_node *);
extern struct rb_node *rb_prev(const struct rb_node *);
extern struct rb_node *rb_first(const struct rb_root *);
extern struct rb_node *rb_last(const struct rb_root *);


extern struct rb_node *rb_first_postorder(const struct rb_root *);
extern struct rb_node *rb_next_postorder(const struct rb_node *);


extern void rb_replace_node(struct rb_node *victim, struct rb_node *new,
       struct rb_root *root);
extern void rb_replace_node_rcu(struct rb_node *victim, struct rb_node *new,
    struct rb_root *root);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void rb_link_node(struct rb_node *node, struct rb_node *parent,
    struct rb_node **rb_link)
{
 node->__rb_parent_color = (unsigned long)parent;
 node->rb_left = node->rb_right = ((void *)0);

 *rb_link = node;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void rb_link_node_rcu(struct rb_node *node, struct rb_node *parent,
        struct rb_node **rb_link)
{
 node->__rb_parent_color = (unsigned long)parent;
 node->rb_left = node->rb_right = ((void *)0);

 do { uintptr_t _r_a_p__v = (uintptr_t)(node); ; if (__builtin_constant_p(node) && (_r_a_p__v) == (uintptr_t)((void *)0)) do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_83(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof((*rb_link)) == sizeof(char) || sizeof((*rb_link)) == sizeof(short) || sizeof((*rb_link)) == sizeof(int) || sizeof((*rb_link)) == sizeof(long)) || sizeof((*rb_link)) == sizeof(long long))) __compiletime_assert_83(); } while (0); do { *(volatile typeof((*rb_link)) *)&((*rb_link)) = ((typeof(*rb_link))(_r_a_p__v)); } while (0); } while (0); else do { do { } while (0); do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_84(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&*rb_link) == sizeof(char) || sizeof(*&*rb_link) == sizeof(short) || sizeof(*&*rb_link) == sizeof(int) || sizeof(*&*rb_link) == sizeof(long)))) __compiletime_assert_84(); } while (0); __asm__ __volatile__("dbar %0 " : : "I"(0b10010) : "memory"); do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_85(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*&*rb_link) == sizeof(char) || sizeof(*&*rb_link) == sizeof(short) || sizeof(*&*rb_link) == sizeof(int) || sizeof(*&*rb_link) == sizeof(long)) || sizeof(*&*rb_link) == sizeof(long long))) __compiletime_assert_85(); } while (0); do { *(volatile typeof(*&*rb_link) *)&(*&*rb_link) = ((typeof(*((typeof(*rb_link))_r_a_p__v)) *)((typeof(*rb_link))_r_a_p__v)); } while (0); } while (0); } while (0); } while (0); } while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void rb_insert_color_cached(struct rb_node *node,
       struct rb_root_cached *root,
       bool leftmost)
{
 if (leftmost)
  root->rb_leftmost = node;
 rb_insert_color(node, &root->rb_root);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) struct rb_node *
rb_erase_cached(struct rb_node *node, struct rb_root_cached *root)
{
 struct rb_node *leftmost = ((void *)0);

 if (root->rb_leftmost == node)
  leftmost = root->rb_leftmost = rb_next(node);

 rb_erase(node, &root->rb_root);

 return leftmost;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void rb_replace_node_cached(struct rb_node *victim,
       struct rb_node *new,
       struct rb_root_cached *root)
{
 if (root->rb_leftmost == victim)
  root->rb_leftmost = new;
 rb_replace_node(victim, new, &root->rb_root);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) struct rb_node *
rb_add_cached(struct rb_node *node, struct rb_root_cached *tree,
       bool (*less)(struct rb_node *, const struct rb_node *))
{
 struct rb_node **link = &tree->rb_root.rb_node;
 struct rb_node *parent = ((void *)0);
 bool leftmost = true;

 while (*link) {
  parent = *link;
  if (less(node, parent)) {
   link = &parent->rb_left;
  } else {
   link = &parent->rb_right;
   leftmost = false;
  }
 }

 rb_link_node(node, parent, link);
 rb_insert_color_cached(node, tree, leftmost);

 return leftmost ? node : ((void *)0);
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void
rb_add(struct rb_node *node, struct rb_root *tree,
       bool (*less)(struct rb_node *, const struct rb_node *))
{
 struct rb_node **link = &tree->rb_node;
 struct rb_node *parent = ((void *)0);

 while (*link) {
  parent = *link;
  if (less(node, parent))
   link = &parent->rb_left;
  else
   link = &parent->rb_right;
 }

 rb_link_node(node, parent, link);
 rb_insert_color(node, tree);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) struct rb_node *
rb_find_add(struct rb_node *node, struct rb_root *tree,
     int (*cmp)(struct rb_node *, const struct rb_node *))
{
 struct rb_node **link = &tree->rb_node;
 struct rb_node *parent = ((void *)0);
 int c;

 while (*link) {
  parent = *link;
  c = cmp(node, parent);

  if (c < 0)
   link = &parent->rb_left;
  else if (c > 0)
   link = &parent->rb_right;
  else
   return parent;
 }

 rb_link_node(node, parent, link);
 rb_insert_color(node, tree);
 return ((void *)0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) struct rb_node *
rb_find(const void *key, const struct rb_root *tree,
 int (*cmp)(const void *key, const struct rb_node *))
{
 struct rb_node *node = tree->rb_node;

 while (node) {
  int c = cmp(key, node);

  if (c < 0)
   node = node->rb_left;
  else if (c > 0)
   node = node->rb_right;
  else
   return node;
 }

 return ((void *)0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) struct rb_node *
rb_find_first(const void *key, const struct rb_root *tree,
       int (*cmp)(const void *key, const struct rb_node *))
{
 struct rb_node *node = tree->rb_node;
 struct rb_node *match = ((void *)0);

 while (node) {
  int c = cmp(key, node);

  if (c <= 0) {
   if (!c)
    match = node;
   node = node->rb_left;
  } else if (c > 0) {
   node = node->rb_right;
  }
 }

 return match;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) struct rb_node *
rb_next_match(const void *key, struct rb_node *node,
       int (*cmp)(const void *key, const struct rb_node *))
{
 node = rb_next(node);
 if (node && cmp(key, node))
  node = ((void *)0);
 return node;
}
struct maple_metadata {
 unsigned char end;
 unsigned char gap;
};
struct maple_range_64 {
 struct maple_pnode *parent;
 unsigned long pivot[16 - 1];
 union {
  void *slot[16];
  struct {
   void *pad[16 - 1];
   struct maple_metadata meta;
  };
 };
};
struct maple_arange_64 {
 struct maple_pnode *parent;
 unsigned long pivot[10 - 1];
 void *slot[10];
 unsigned long gap[10];
 struct maple_metadata meta;
};

struct maple_alloc {
 unsigned long total;
 unsigned char node_count;
 unsigned int request_count;
 struct maple_alloc *slot[(31 - 1)];
};

struct maple_topiary {
 struct maple_pnode *parent;
 struct maple_enode *next;
};

enum maple_type {
 maple_dense,
 maple_leaf_64,
 maple_range_64,
 maple_arange_64,
};
typedef struct lockdep_map *lockdep_map_p;
struct maple_tree {
 union {
  spinlock_t ma_lock;
  lockdep_map_p ma_external_lock;
 };
 void *ma_root;
 unsigned int ma_flags;
};
struct maple_node {
 union {
  struct {
   struct maple_pnode *parent;
   void *slot[31];
  };
  struct {
   void *pad;
   struct callback_head rcu;
   struct maple_enode *piv_parent;
   unsigned char parent_slot;
   enum maple_type type;
   unsigned char slot_len;
   unsigned int ma_flags;
  };
  struct maple_range_64 mr64;
  struct maple_arange_64 ma64;
  struct maple_alloc alloc;
 };
};
struct ma_topiary {
 struct maple_enode *head;
 struct maple_enode *tail;
 struct maple_tree *mtree;
};

void *mtree_load(struct maple_tree *mt, unsigned long index);

int mtree_insert(struct maple_tree *mt, unsigned long index,
  void *entry, gfp_t gfp);
int mtree_insert_range(struct maple_tree *mt, unsigned long first,
  unsigned long last, void *entry, gfp_t gfp);
int mtree_alloc_range(struct maple_tree *mt, unsigned long *startp,
  void *entry, unsigned long size, unsigned long min,
  unsigned long max, gfp_t gfp);
int mtree_alloc_rrange(struct maple_tree *mt, unsigned long *startp,
  void *entry, unsigned long size, unsigned long min,
  unsigned long max, gfp_t gfp);

int mtree_store_range(struct maple_tree *mt, unsigned long first,
        unsigned long last, void *entry, gfp_t gfp);
int mtree_store(struct maple_tree *mt, unsigned long index,
  void *entry, gfp_t gfp);
void *mtree_erase(struct maple_tree *mt, unsigned long index);

void mtree_destroy(struct maple_tree *mt);
void __mt_destroy(struct maple_tree *mt);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool mtree_empty(const struct maple_tree *mt)
{
 return mt->ma_root == ((void *)0);
}
struct ma_state {
 struct maple_tree *tree;
 unsigned long index;
 unsigned long last;
 struct maple_enode *node;
 unsigned long min;
 unsigned long max;
 struct maple_alloc *alloc;
 unsigned char depth;
 unsigned char offset;
 unsigned char mas_flags;
};

struct ma_wr_state {
 struct ma_state *mas;
 struct maple_node *node;
 unsigned long r_min;
 unsigned long r_max;
 enum maple_type type;
 unsigned char offset_end;
 unsigned char node_end;
 unsigned long *pivots;
 unsigned long end_piv;
 void **slots;
 void *entry;
 void *content;
};
void *mas_walk(struct ma_state *mas);
void *mas_store(struct ma_state *mas, void *entry);
void *mas_erase(struct ma_state *mas);
int mas_store_gfp(struct ma_state *mas, void *entry, gfp_t gfp);
void mas_store_prealloc(struct ma_state *mas, void *entry);
void *mas_find(struct ma_state *mas, unsigned long max);
void *mas_find_range(struct ma_state *mas, unsigned long max);
void *mas_find_rev(struct ma_state *mas, unsigned long min);
void *mas_find_range_rev(struct ma_state *mas, unsigned long max);
int mas_preallocate(struct ma_state *mas, void *entry, gfp_t gfp);
bool mas_is_err(struct ma_state *mas);

bool mas_nomem(struct ma_state *mas, gfp_t gfp);
void mas_pause(struct ma_state *mas);
void maple_tree_init(void);
void mas_destroy(struct ma_state *mas);
int mas_expected_entries(struct ma_state *mas, unsigned long nr_entries);

void *mas_prev(struct ma_state *mas, unsigned long min);
void *mas_prev_range(struct ma_state *mas, unsigned long max);
void *mas_next(struct ma_state *mas, unsigned long max);
void *mas_next_range(struct ma_state *mas, unsigned long max);

int mas_empty_area(struct ma_state *mas, unsigned long min, unsigned long max,
     unsigned long size);




int mas_empty_area_rev(struct ma_state *mas, unsigned long min,
         unsigned long max, unsigned long size);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void mas_init(struct ma_state *mas, struct maple_tree *tree,
       unsigned long addr)
{
 ({ size_t __fortify_size = (size_t)(sizeof(struct ma_state)); fortify_memset_chk(__fortify_size, __builtin_dynamic_object_size(mas, 0), __builtin_dynamic_object_size(mas, 1)), __builtin_memset(mas, 0, __fortify_size); });
 mas->tree = tree;
 mas->index = mas->last = addr;
 mas->max = (~0UL);
 mas->node = ((struct maple_enode *)1UL);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool mas_is_none(const struct ma_state *mas)
{
 return mas->node == ((struct maple_enode *)9UL);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool mas_is_paused(const struct ma_state *mas)
{
 return mas->node == ((struct maple_enode *)17UL);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void mas_reset(struct ma_state *mas)
{
 mas->node = ((struct maple_enode *)1UL);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __mas_set_range(struct ma_state *mas, unsigned long start,
  unsigned long last)
{
 mas->index = start;
 mas->last = last;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0)))
void mas_set_range(struct ma_state *mas, unsigned long start, unsigned long last)
{
 __mas_set_range(mas, start, last);
 mas->node = ((struct maple_enode *)1UL);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void mas_set(struct ma_state *mas, unsigned long index)
{

 mas_set_range(mas, index, index);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool mt_external_lock(const struct maple_tree *mt)
{
 return (mt->ma_flags & 0x300) == 0x300;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void mt_init_flags(struct maple_tree *mt, unsigned int flags)
{
 mt->ma_flags = flags;
 if (!mt_external_lock(mt))
  do { static struct lock_class_key __key; __raw_spin_lock_init(spinlock_check(&mt->ma_lock), "&mt->ma_lock", &__key, LD_WAIT_CONFIG); } while (0);
 do { uintptr_t _r_a_p__v = (uintptr_t)(((void *)0)); ; if (__builtin_constant_p(((void *)0)) && (_r_a_p__v) == (uintptr_t)((void *)0)) do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_86(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof((mt->ma_root)) == sizeof(char) || sizeof((mt->ma_root)) == sizeof(short) || sizeof((mt->ma_root)) == sizeof(int) || sizeof((mt->ma_root)) == sizeof(long)) || sizeof((mt->ma_root)) == sizeof(long long))) __compiletime_assert_86(); } while (0); do { *(volatile typeof((mt->ma_root)) *)&((mt->ma_root)) = ((typeof(mt->ma_root))(_r_a_p__v)); } while (0); } while (0); else do { do { } while (0); do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_87(void) __attribute__((__error__("Need native word sized stores/loads for atomicity."))); if (!((sizeof(*&mt->ma_root) == sizeof(char) || sizeof(*&mt->ma_root) == sizeof(short) || sizeof(*&mt->ma_root) == sizeof(int) || sizeof(*&mt->ma_root) == sizeof(long)))) __compiletime_assert_87(); } while (0); __asm__ __volatile__("dbar %0 " : : "I"(0b10010) : "memory"); do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_88(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(*&mt->ma_root) == sizeof(char) || sizeof(*&mt->ma_root) == sizeof(short) || sizeof(*&mt->ma_root) == sizeof(int) || sizeof(*&mt->ma_root) == sizeof(long)) || sizeof(*&mt->ma_root) == sizeof(long long))) __compiletime_assert_88(); } while (0); do { *(volatile typeof(*&mt->ma_root) *)&(*&mt->ma_root) = ((typeof(*((typeof(mt->ma_root))_r_a_p__v)) *)((typeof(mt->ma_root))_r_a_p__v)); } while (0); } while (0); } while (0); } while (0); } while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void mt_init(struct maple_tree *mt)
{
 mt_init_flags(mt, 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool mt_in_rcu(struct maple_tree *mt)
{



 return mt->ma_flags & 0x02;
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void mt_clear_in_rcu(struct maple_tree *mt)
{
 if (!mt_in_rcu(mt))
  return;

 if (mt_external_lock(mt)) {
  ({ int __ret_warn_on = !!(!(!(mt)->ma_external_lock || lock_is_held((mt)->ma_external_lock))); if (__builtin_expect(!!(__ret_warn_on), 0)) do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/maple_tree.h\"; .popsection; .long 10002b - .; .short 645; .short (1 << 0)|(((9) << 8)); .popsection; 10001: break 1");; do { } while(0); } while (0); __builtin_expect(!!(__ret_warn_on), 0); });
  mt->ma_flags &= ~0x02;
 } else {
  spin_lock((&(mt)->ma_lock));
  mt->ma_flags &= ~0x02;
  spin_unlock((&(mt)->ma_lock));
 }
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void mt_set_in_rcu(struct maple_tree *mt)
{
 if (mt_in_rcu(mt))
  return;

 if (mt_external_lock(mt)) {
  ({ int __ret_warn_on = !!(!(!(mt)->ma_external_lock || lock_is_held((mt)->ma_external_lock))); if (__builtin_expect(!!(__ret_warn_on), 0)) do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/maple_tree.h\"; .popsection; .long 10002b - .; .short 664; .short (1 << 0)|(((9) << 8)); .popsection; 10001: break 1");; do { } while(0); } while (0); __builtin_expect(!!(__ret_warn_on), 0); });
  mt->ma_flags |= 0x02;
 } else {
  spin_lock((&(mt)->ma_lock));
  mt->ma_flags |= 0x02;
  spin_unlock((&(mt)->ma_lock));
 }
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned int mt_height(const struct maple_tree *mt)
{
 return (mt->ma_flags & 0x7C) >> 0x02;
}

void *mt_find(struct maple_tree *mt, unsigned long *index, unsigned long max);
void *mt_find_after(struct maple_tree *mt, unsigned long *index,
      unsigned long max);
void *mt_prev(struct maple_tree *mt, unsigned long index, unsigned long min);
void *mt_next(struct maple_tree *mt, unsigned long index, unsigned long max);
enum mt_dump_format {
 mt_dump_dec,
 mt_dump_hex,
};

extern atomic_t maple_tree_tests_run;
extern atomic_t maple_tree_tests_passed;

void mt_dump(const struct maple_tree *mt, enum mt_dump_format format);
void mas_dump(const struct ma_state *mas);
void mas_wr_dump(const struct ma_wr_state *wr_mas);
void mt_validate(struct maple_tree *mt);
void mt_cache_shrink(void);







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void * __attribute__((__warn_unused_result__)) ERR_PTR(long error)
{
 return (void *) error;
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) long __attribute__((__warn_unused_result__)) PTR_ERR( const void *ptr)
{
 return (long) ptr;
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool __attribute__((__warn_unused_result__)) IS_ERR( const void *ptr)
{
 return __builtin_expect(!!((unsigned long)(void *)((unsigned long)ptr) >= (unsigned long)-4095), 0);
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool __attribute__((__warn_unused_result__)) IS_ERR_OR_NULL( const void *ptr)
{
 return __builtin_expect(!!(!ptr), 0) || __builtin_expect(!!((unsigned long)(void *)((unsigned long)ptr) >= (unsigned long)-4095), 0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void * __attribute__((__warn_unused_result__)) ERR_CAST( const void *ptr)
{

 return (void *) ptr;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int __attribute__((__warn_unused_result__)) PTR_ERR_OR_ZERO( const void *ptr)
{
 if (IS_ERR(ptr))
  return PTR_ERR(ptr);
 else
  return 0;
}
struct rw_semaphore {
 atomic_long_t count;





 atomic_long_t owner;

 struct optimistic_spin_queue osq;

 raw_spinlock_t wait_lock;
 struct list_head wait_list;

 void *magic;


 struct lockdep_map dep_map;

};


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int rwsem_is_locked(struct rw_semaphore *sem)
{
 return atomic_long_read(&sem->count) != 0;
}
extern void __init_rwsem(struct rw_semaphore *sem, const char *name,
    struct lock_class_key *key);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int rwsem_is_contended(struct rw_semaphore *sem)
{
 return !list_empty(&sem->wait_list);
}
extern void down_read(struct rw_semaphore *sem);
extern int __attribute__((__warn_unused_result__)) down_read_interruptible(struct rw_semaphore *sem);
extern int __attribute__((__warn_unused_result__)) down_read_killable(struct rw_semaphore *sem);




extern int down_read_trylock(struct rw_semaphore *sem);




extern void down_write(struct rw_semaphore *sem);
extern int __attribute__((__warn_unused_result__)) down_write_killable(struct rw_semaphore *sem);




extern int down_write_trylock(struct rw_semaphore *sem);




extern void up_read(struct rw_semaphore *sem);




extern void up_write(struct rw_semaphore *sem);

typedef struct rw_semaphore * class_rwsem_read_t; static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void class_rwsem_read_destructor(struct rw_semaphore * *p) { struct rw_semaphore * _T = *p; up_read(_T); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) struct rw_semaphore * class_rwsem_read_constructor(struct rw_semaphore * _T) { struct rw_semaphore * t = ({ down_read(_T); _T; }); return t; }
typedef struct rw_semaphore * class_rwsem_write_t; static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void class_rwsem_write_destructor(struct rw_semaphore * *p) { struct rw_semaphore * _T = *p; up_write(_T); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) struct rw_semaphore * class_rwsem_write_constructor(struct rw_semaphore * _T) { struct rw_semaphore * t = ({ down_write(_T); _T; }); return t; }

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __free_up_read(void *p) { struct rw_semaphore * _T = *(struct rw_semaphore * *)p; if (_T) up_read(_T); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __free_up_write(void *p) { struct rw_semaphore * _T = *(struct rw_semaphore * *)p; if (_T) up_write(_T); }





extern void downgrade_write(struct rw_semaphore *sem);
extern void down_read_nested(struct rw_semaphore *sem, int subclass);
extern int __attribute__((__warn_unused_result__)) down_read_killable_nested(struct rw_semaphore *sem, int subclass);
extern void down_write_nested(struct rw_semaphore *sem, int subclass);
extern int down_write_killable_nested(struct rw_semaphore *sem, int subclass);
extern void _down_write_nest_lock(struct rw_semaphore *sem, struct lockdep_map *nest_lock);
extern void down_read_non_owner(struct rw_semaphore *sem);
extern void up_read_non_owner(struct rw_semaphore *sem);








struct task_struct;

struct swait_queue_head {
 raw_spinlock_t lock;
 struct list_head task_list;
};

struct swait_queue {
 struct task_struct *task;
 struct list_head task_list;
};
extern void __init_swait_queue_head(struct swait_queue_head *q, const char *name,
        struct lock_class_key *key);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int swait_active(struct swait_queue_head *wq)
{
 return !list_empty(&wq->task_list);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool swq_has_sleeper(struct swait_queue_head *wq)
{







 do { do { } while (0); __asm__ __volatile__("dbar %0 " : : "I"(0b10000) : "memory"); } while (0);
 return swait_active(wq);
}

extern void swake_up_one(struct swait_queue_head *q);
extern void swake_up_all(struct swait_queue_head *q);
extern void swake_up_locked(struct swait_queue_head *q, int wake_flags);

extern void prepare_to_swait_exclusive(struct swait_queue_head *q, struct swait_queue *wait, int state);
extern long prepare_to_swait_event(struct swait_queue_head *q, struct swait_queue *wait, int state);

extern void __finish_swait(struct swait_queue_head *q, struct swait_queue *wait);
extern void finish_swait(struct swait_queue_head *q, struct swait_queue *wait);
struct completion {
 unsigned int done;
 struct swait_queue_head wait;
};


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void complete_acquire(struct completion *x) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void complete_release(struct completion *x) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void init_completion(struct completion *x)
{
 x->done = 0;
 do { static struct lock_class_key __key; __init_swait_queue_head((&x->wait), "&x->wait", &__key); } while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void reinit_completion(struct completion *x)
{
 x->done = 0;
}

extern void wait_for_completion(struct completion *);
extern void wait_for_completion_io(struct completion *);
extern int wait_for_completion_interruptible(struct completion *x);
extern int wait_for_completion_killable(struct completion *x);
extern int wait_for_completion_state(struct completion *x, unsigned int state);
extern unsigned long wait_for_completion_timeout(struct completion *x,
         unsigned long timeout);
extern unsigned long wait_for_completion_io_timeout(struct completion *x,
          unsigned long timeout);
extern long wait_for_completion_interruptible_timeout(
 struct completion *x, unsigned long timeout);
extern long wait_for_completion_killable_timeout(
 struct completion *x, unsigned long timeout);
extern bool try_wait_for_completion(struct completion *x);
extern bool completion_done(struct completion *x);

extern void complete(struct completion *);
extern void complete_on_current_cpu(struct completion *x);
extern void complete_all(struct completion *);

struct vm_area_struct;
struct mm_struct;
struct inode;
struct notifier_block;
struct page;






enum uprobe_filter_ctx {
 UPROBE_FILTER_REGISTER,
 UPROBE_FILTER_UNREGISTER,
 UPROBE_FILTER_MMAP,
};

struct uprobe_consumer {
 int (*handler)(struct uprobe_consumer *self, struct pt_regs *regs);
 int (*ret_handler)(struct uprobe_consumer *self,
    unsigned long func,
    struct pt_regs *regs);
 bool (*filter)(struct uprobe_consumer *self,
    enum uprobe_filter_ctx ctx,
    struct mm_struct *mm);

 struct uprobe_consumer *next;
};






struct pt_regs {

 unsigned long regs[32];


 unsigned long orig_a0;


 unsigned long csr_era;
 unsigned long csr_badvaddr;
 unsigned long csr_crmd;
 unsigned long csr_prmd;
 unsigned long csr_euen;
 unsigned long csr_ecfg;
 unsigned long csr_estat;
 unsigned long __last[];
} __attribute__((__aligned__(8)));

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int regs_irqs_disabled(struct pt_regs *regs)
{
 return arch_irqs_disabled_flags(regs->csr_prmd);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long kernel_stack_pointer(struct pt_regs *regs)
{
 return regs->regs[3];
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void instruction_pointer_set(struct pt_regs *regs, unsigned long val)
{
 regs->csr_era = val;
}


extern int regs_query_register_offset(const char *name);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long regs_get_register(struct pt_regs *regs, unsigned int offset)
{
 if (__builtin_expect(!!(offset > (__builtin_offsetof(struct pt_regs, __last))), 0))
  return 0;

 return *(unsigned long *)((unsigned long)regs + offset);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int regs_within_kernel_stack(struct pt_regs *regs, unsigned long addr)
{
 return ((addr & ~(0x00004000 - 1)) ==
  (kernel_stack_pointer(regs) & ~(0x00004000 - 1)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long regs_get_kernel_stack_nth(struct pt_regs *regs, unsigned int n)
{
 unsigned long *addr = (unsigned long *)kernel_stack_pointer(regs);

 addr += n;
 if (regs_within_kernel_stack(regs, (unsigned long)addr))
  return *addr;
 else
  return 0;
}

struct task_struct;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long regs_get_kernel_argument(struct pt_regs *regs,
           unsigned int n)
{

 static const unsigned int args[] = {
  __builtin_offsetof(struct pt_regs, regs[4]),
  __builtin_offsetof(struct pt_regs, regs[5]),
  __builtin_offsetof(struct pt_regs, regs[6]),
  __builtin_offsetof(struct pt_regs, regs[7]),
  __builtin_offsetof(struct pt_regs, regs[8]),
  __builtin_offsetof(struct pt_regs, regs[9]),
  __builtin_offsetof(struct pt_regs, regs[10]),
  __builtin_offsetof(struct pt_regs, regs[11]),
 };

 if (n < 8)
  return regs_get_register(regs, args[n]);
 else {
  n -= 8;
  return regs_get_kernel_stack_nth(regs, n);
 }
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) long regs_return_value(struct pt_regs *regs)
{
 return regs->regs[4];
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void regs_set_return_value(struct pt_regs *regs, unsigned long val)
{
 regs->regs[4] = val;
}




extern void die(const char *, struct pt_regs *) __attribute__((__noreturn__));

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void die_if_kernel(const char *str, struct pt_regs *regs)
{
 if (__builtin_expect(!!(!(((regs)->csr_prmd & 0x3) == 3)), 0))
  die(str, regs);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long user_stack_pointer(struct pt_regs *regs)
{
 return regs->regs[3];
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void user_stack_pointer_set(struct pt_regs *regs,
 unsigned long val)
{
 regs->regs[3] = val;
}
enum reg0i15_op {
 break_op = 0x54,
};

enum reg0i26_op {
 b_op = 0x14,
 bl_op = 0x15,
};

enum reg1i20_op {
 lu12iw_op = 0x0a,
 lu32id_op = 0x0b,
 pcaddi_op = 0x0c,
 pcalau12i_op = 0x0d,
 pcaddu12i_op = 0x0e,
 pcaddu18i_op = 0x0f,
};

enum reg1i21_op {
 beqz_op = 0x10,
 bnez_op = 0x11,
 bceqz_op = 0x12,
 bcnez_op = 0x12,
};

enum reg2_op {
 revb2h_op = 0x0c,
 revb4h_op = 0x0d,
 revb2w_op = 0x0e,
 revbd_op = 0x0f,
 revh2w_op = 0x10,
 revhd_op = 0x11,
};

enum reg2i5_op {
 slliw_op = 0x81,
 srliw_op = 0x89,
 sraiw_op = 0x91,
};

enum reg2i6_op {
 sllid_op = 0x41,
 srlid_op = 0x45,
 sraid_op = 0x49,
};

enum reg2i12_op {
 addiw_op = 0x0a,
 addid_op = 0x0b,
 lu52id_op = 0x0c,
 andi_op = 0x0d,
 ori_op = 0x0e,
 xori_op = 0x0f,
 ldb_op = 0xa0,
 ldh_op = 0xa1,
 ldw_op = 0xa2,
 ldd_op = 0xa3,
 stb_op = 0xa4,
 sth_op = 0xa5,
 stw_op = 0xa6,
 std_op = 0xa7,
 ldbu_op = 0xa8,
 ldhu_op = 0xa9,
 ldwu_op = 0xaa,
 flds_op = 0xac,
 fsts_op = 0xad,
 fldd_op = 0xae,
 fstd_op = 0xaf,
};

enum reg2i14_op {
 llw_op = 0x20,
 scw_op = 0x21,
 lld_op = 0x22,
 scd_op = 0x23,
 ldptrw_op = 0x24,
 stptrw_op = 0x25,
 ldptrd_op = 0x26,
 stptrd_op = 0x27,
};

enum reg2i16_op {
 jirl_op = 0x13,
 beq_op = 0x16,
 bne_op = 0x17,
 blt_op = 0x18,
 bge_op = 0x19,
 bltu_op = 0x1a,
 bgeu_op = 0x1b,
};

enum reg2bstrd_op {
 bstrinsd_op = 0x2,
 bstrpickd_op = 0x3,
};

enum reg3_op {
 asrtle_op = 0x02,
 asrtgt_op = 0x03,
 addw_op = 0x20,
 addd_op = 0x21,
 subw_op = 0x22,
 subd_op = 0x23,
 nor_op = 0x28,
 and_op = 0x29,
 or_op = 0x2a,
 xor_op = 0x2b,
 orn_op = 0x2c,
 andn_op = 0x2d,
 sllw_op = 0x2e,
 srlw_op = 0x2f,
 sraw_op = 0x30,
 slld_op = 0x31,
 srld_op = 0x32,
 srad_op = 0x33,
 mulw_op = 0x38,
 mulhw_op = 0x39,
 mulhwu_op = 0x3a,
 muld_op = 0x3b,
 mulhd_op = 0x3c,
 mulhdu_op = 0x3d,
 divw_op = 0x40,
 modw_op = 0x41,
 divwu_op = 0x42,
 modwu_op = 0x43,
 divd_op = 0x44,
 modd_op = 0x45,
 divdu_op = 0x46,
 moddu_op = 0x47,
 ldxb_op = 0x7000,
 ldxh_op = 0x7008,
 ldxw_op = 0x7010,
 ldxd_op = 0x7018,
 stxb_op = 0x7020,
 stxh_op = 0x7028,
 stxw_op = 0x7030,
 stxd_op = 0x7038,
 ldxbu_op = 0x7040,
 ldxhu_op = 0x7048,
 ldxwu_op = 0x7050,
 fldxs_op = 0x7060,
 fldxd_op = 0x7068,
 fstxs_op = 0x7070,
 fstxd_op = 0x7078,
 amswapw_op = 0x70c0,
 amswapd_op = 0x70c1,
 amaddw_op = 0x70c2,
 amaddd_op = 0x70c3,
 amandw_op = 0x70c4,
 amandd_op = 0x70c5,
 amorw_op = 0x70c6,
 amord_op = 0x70c7,
 amxorw_op = 0x70c8,
 amxord_op = 0x70c9,
 ammaxw_op = 0x70ca,
 ammaxd_op = 0x70cb,
 amminw_op = 0x70cc,
 ammind_op = 0x70cd,
 ammaxwu_op = 0x70ce,
 ammaxdu_op = 0x70cf,
 amminwu_op = 0x70d0,
 ammindu_op = 0x70d1,
 amswapdbw_op = 0x70d2,
 amswapdbd_op = 0x70d3,
 amadddbw_op = 0x70d4,
 amadddbd_op = 0x70d5,
 amanddbw_op = 0x70d6,
 amanddbd_op = 0x70d7,
 amordbw_op = 0x70d8,
 amordbd_op = 0x70d9,
 amxordbw_op = 0x70da,
 amxordbd_op = 0x70db,
 ammaxdbw_op = 0x70dc,
 ammaxdbd_op = 0x70dd,
 ammindbw_op = 0x70de,
 ammindbd_op = 0x70df,
 ammaxdbwu_op = 0x70e0,
 ammaxdbdu_op = 0x70e1,
 ammindbwu_op = 0x70e2,
 ammindbdu_op = 0x70e3,
 fldgts_op = 0x70e8,
 fldgtd_op = 0x70e9,
 fldles_op = 0x70ea,
 fldled_op = 0x70eb,
 fstgts_op = 0x70ec,
 fstgtd_op = 0x70ed,
 fstles_op = 0x70ee,
 fstled_op = 0x70ef,
 ldgtb_op = 0x70f0,
 ldgth_op = 0x70f1,
 ldgtw_op = 0x70f2,
 ldgtd_op = 0x70f3,
 ldleb_op = 0x70f4,
 ldleh_op = 0x70f5,
 ldlew_op = 0x70f6,
 ldled_op = 0x70f7,
 stgtb_op = 0x70f8,
 stgth_op = 0x70f9,
 stgtw_op = 0x70fa,
 stgtd_op = 0x70fb,
 stleb_op = 0x70fc,
 stleh_op = 0x70fd,
 stlew_op = 0x70fe,
 stled_op = 0x70ff,
};

enum reg3sa2_op {
 alslw_op = 0x02,
 alslwu_op = 0x03,
 alsld_op = 0x16,
};

struct reg0i15_format {
 unsigned int immediate : 15;
 unsigned int opcode : 17;
};

struct reg0i26_format {
 unsigned int immediate_h : 10;
 unsigned int immediate_l : 16;
 unsigned int opcode : 6;
};

struct reg1i20_format {
 unsigned int rd : 5;
 unsigned int immediate : 20;
 unsigned int opcode : 7;
};

struct reg1i21_format {
 unsigned int immediate_h : 5;
 unsigned int rj : 5;
 unsigned int immediate_l : 16;
 unsigned int opcode : 6;
};

struct reg2_format {
 unsigned int rd : 5;
 unsigned int rj : 5;
 unsigned int opcode : 22;
};

struct reg2i5_format {
 unsigned int rd : 5;
 unsigned int rj : 5;
 unsigned int immediate : 5;
 unsigned int opcode : 17;
};

struct reg2i6_format {
 unsigned int rd : 5;
 unsigned int rj : 5;
 unsigned int immediate : 6;
 unsigned int opcode : 16;
};

struct reg2i12_format {
 unsigned int rd : 5;
 unsigned int rj : 5;
 unsigned int immediate : 12;
 unsigned int opcode : 10;
};

struct reg2i14_format {
 unsigned int rd : 5;
 unsigned int rj : 5;
 unsigned int immediate : 14;
 unsigned int opcode : 8;
};

struct reg2i16_format {
 unsigned int rd : 5;
 unsigned int rj : 5;
 unsigned int immediate : 16;
 unsigned int opcode : 6;
};

struct reg2bstrd_format {
 unsigned int rd : 5;
 unsigned int rj : 5;
 unsigned int lsbd : 6;
 unsigned int msbd : 6;
 unsigned int opcode : 10;
};

struct reg3_format {
 unsigned int rd : 5;
 unsigned int rj : 5;
 unsigned int rk : 5;
 unsigned int opcode : 17;
};

struct reg3sa2_format {
 unsigned int rd : 5;
 unsigned int rj : 5;
 unsigned int rk : 5;
 unsigned int immediate : 2;
 unsigned int opcode : 15;
};

union loongarch_instruction {
 unsigned int word;
 struct reg0i15_format reg0i15_format;
 struct reg0i26_format reg0i26_format;
 struct reg1i20_format reg1i20_format;
 struct reg1i21_format reg1i21_format;
 struct reg2_format reg2_format;
 struct reg2i5_format reg2i5_format;
 struct reg2i6_format reg2i6_format;
 struct reg2i12_format reg2i12_format;
 struct reg2i14_format reg2i14_format;
 struct reg2i16_format reg2i16_format;
 struct reg2bstrd_format reg2bstrd_format;
 struct reg3_format reg3_format;
 struct reg3sa2_format reg3sa2_format;
};



enum loongarch_gpr {
 LOONGARCH_GPR_ZERO = 0,
 LOONGARCH_GPR_RA = 1,
 LOONGARCH_GPR_TP = 2,
 LOONGARCH_GPR_SP = 3,
 LOONGARCH_GPR_A0 = 4,
 LOONGARCH_GPR_A1,
 LOONGARCH_GPR_A2,
 LOONGARCH_GPR_A3,
 LOONGARCH_GPR_A4,
 LOONGARCH_GPR_A5,
 LOONGARCH_GPR_A6,
 LOONGARCH_GPR_A7,
 LOONGARCH_GPR_T0 = 12,
 LOONGARCH_GPR_T1,
 LOONGARCH_GPR_T2,
 LOONGARCH_GPR_T3,
 LOONGARCH_GPR_T4,
 LOONGARCH_GPR_T5,
 LOONGARCH_GPR_T6,
 LOONGARCH_GPR_T7,
 LOONGARCH_GPR_T8,
 LOONGARCH_GPR_FP = 22,
 LOONGARCH_GPR_S0 = 23,
 LOONGARCH_GPR_S1,
 LOONGARCH_GPR_S2,
 LOONGARCH_GPR_S3,
 LOONGARCH_GPR_S4,
 LOONGARCH_GPR_S5,
 LOONGARCH_GPR_S6,
 LOONGARCH_GPR_S7,
 LOONGARCH_GPR_S8,
 LOONGARCH_GPR_MAX
};



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool is_imm_negative(unsigned long val, unsigned int bit)
{
 return val & (1UL << (bit - 1));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool is_break_ins(union loongarch_instruction *ip)
{
 return ip->reg0i15_format.opcode == break_op;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool is_pc_ins(union loongarch_instruction *ip)
{
 return ip->reg1i20_format.opcode >= pcaddi_op &&
   ip->reg1i20_format.opcode <= pcaddu18i_op;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool is_branch_ins(union loongarch_instruction *ip)
{
 return ip->reg1i21_format.opcode >= beqz_op &&
  ip->reg1i21_format.opcode <= bgeu_op;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool is_ra_save_ins(union loongarch_instruction *ip)
{

 return ip->reg2i12_format.opcode == std_op &&
  ip->reg2i12_format.rj == LOONGARCH_GPR_SP &&
  ip->reg2i12_format.rd == LOONGARCH_GPR_RA &&
  !is_imm_negative(ip->reg2i12_format.immediate, 12);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool is_stack_alloc_ins(union loongarch_instruction *ip)
{

 return ip->reg2i12_format.opcode == addid_op &&
  ip->reg2i12_format.rj == LOONGARCH_GPR_SP &&
  ip->reg2i12_format.rd == LOONGARCH_GPR_SP &&
  is_imm_negative(ip->reg2i12_format.immediate, 12);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool is_self_loop_ins(union loongarch_instruction *ip, struct pt_regs *regs)
{
 switch (ip->reg0i26_format.opcode) {
 case b_op:
 case bl_op:
  if (ip->reg0i26_format.immediate_l == 0
      && ip->reg0i26_format.immediate_h == 0)
   return true;
 }

 switch (ip->reg1i21_format.opcode) {
 case beqz_op:
 case bnez_op:
 case bceqz_op:
  if (ip->reg1i21_format.immediate_l == 0
      && ip->reg1i21_format.immediate_h == 0)
   return true;
 }

 switch (ip->reg2i16_format.opcode) {
 case beq_op:
 case bne_op:
 case blt_op:
 case bge_op:
 case bltu_op:
 case bgeu_op:
  if (ip->reg2i16_format.immediate == 0)
   return true;
  break;
 case jirl_op:
  if (regs->regs[ip->reg2i16_format.rj] +
      ((unsigned long)ip->reg2i16_format.immediate << 2) == (unsigned long)ip)
   return true;
 }

 return false;
}

void simu_pc(struct pt_regs *regs, union loongarch_instruction insn);
void simu_branch(struct pt_regs *regs, union loongarch_instruction insn);

bool insns_not_supported(union loongarch_instruction insn);
bool insns_need_simulation(union loongarch_instruction insn);
void arch_simulate_insn(union loongarch_instruction insn, struct pt_regs *regs);

int larch_insn_read(void *addr, u32 *insnp);
int larch_insn_write(void *addr, u32 insn);
int larch_insn_patch_text(void *addr, u32 insn);

u32 larch_insn_gen_nop(void);
u32 larch_insn_gen_b(unsigned long pc, unsigned long dest);
u32 larch_insn_gen_bl(unsigned long pc, unsigned long dest);

u32 larch_insn_gen_break(int imm);

u32 larch_insn_gen_or(enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk);
u32 larch_insn_gen_move(enum loongarch_gpr rd, enum loongarch_gpr rj);

u32 larch_insn_gen_lu12iw(enum loongarch_gpr rd, int imm);
u32 larch_insn_gen_lu32id(enum loongarch_gpr rd, int imm);
u32 larch_insn_gen_lu52id(enum loongarch_gpr rd, enum loongarch_gpr rj, int imm);
u32 larch_insn_gen_jirl(enum loongarch_gpr rd, enum loongarch_gpr rj, int imm);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool signed_imm_check(long val, unsigned int bit)
{
 return -(1L << (bit - 1)) <= val && val < (1L << (bit - 1));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool unsigned_imm_check(unsigned long val, unsigned int bit)
{
 return val < (1UL << bit);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_break(union loongarch_instruction *insn, int imm) { insn->reg0i15_format.opcode = break_op; insn->reg0i15_format.immediate = imm; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_b(union loongarch_instruction *insn, int offset) { unsigned int immediate_l, immediate_h; immediate_l = offset & 0xffff; offset >>= 16; immediate_h = offset & 0x3ff; insn->reg0i26_format.opcode = b_op; insn->reg0i26_format.immediate_l = immediate_l; insn->reg0i26_format.immediate_h = immediate_h; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_bl(union loongarch_instruction *insn, int offset) { unsigned int immediate_l, immediate_h; immediate_l = offset & 0xffff; offset >>= 16; immediate_h = offset & 0x3ff; insn->reg0i26_format.opcode = bl_op; insn->reg0i26_format.immediate_l = immediate_l; insn->reg0i26_format.immediate_h = immediate_h; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_lu12iw(union loongarch_instruction *insn, enum loongarch_gpr rd, int imm) { insn->reg1i20_format.opcode = lu12iw_op; insn->reg1i20_format.immediate = imm; insn->reg1i20_format.rd = rd; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_lu32id(union loongarch_instruction *insn, enum loongarch_gpr rd, int imm) { insn->reg1i20_format.opcode = lu32id_op; insn->reg1i20_format.immediate = imm; insn->reg1i20_format.rd = rd; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_pcaddu18i(union loongarch_instruction *insn, enum loongarch_gpr rd, int imm) { insn->reg1i20_format.opcode = pcaddu18i_op; insn->reg1i20_format.immediate = imm; insn->reg1i20_format.rd = rd; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_revb2h(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj) { insn->reg2_format.opcode = revb2h_op; insn->reg2_format.rd = rd; insn->reg2_format.rj = rj; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_revb2w(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj) { insn->reg2_format.opcode = revb2w_op; insn->reg2_format.rd = rd; insn->reg2_format.rj = rj; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_revbd(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj) { insn->reg2_format.opcode = revbd_op; insn->reg2_format.rd = rd; insn->reg2_format.rj = rj; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_slliw(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, int imm) { insn->reg2i5_format.opcode = slliw_op; insn->reg2i5_format.immediate = imm; insn->reg2i5_format.rd = rd; insn->reg2i5_format.rj = rj; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_srliw(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, int imm) { insn->reg2i5_format.opcode = srliw_op; insn->reg2i5_format.immediate = imm; insn->reg2i5_format.rd = rd; insn->reg2i5_format.rj = rj; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_sraiw(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, int imm) { insn->reg2i5_format.opcode = sraiw_op; insn->reg2i5_format.immediate = imm; insn->reg2i5_format.rd = rd; insn->reg2i5_format.rj = rj; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_sllid(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, int imm) { insn->reg2i6_format.opcode = sllid_op; insn->reg2i6_format.immediate = imm; insn->reg2i6_format.rd = rd; insn->reg2i6_format.rj = rj; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_srlid(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, int imm) { insn->reg2i6_format.opcode = srlid_op; insn->reg2i6_format.immediate = imm; insn->reg2i6_format.rd = rd; insn->reg2i6_format.rj = rj; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_sraid(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, int imm) { insn->reg2i6_format.opcode = sraid_op; insn->reg2i6_format.immediate = imm; insn->reg2i6_format.rd = rd; insn->reg2i6_format.rj = rj; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_addiw(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, int imm) { insn->reg2i12_format.opcode = addiw_op; insn->reg2i12_format.immediate = imm; insn->reg2i12_format.rd = rd; insn->reg2i12_format.rj = rj; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_addid(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, int imm) { insn->reg2i12_format.opcode = addid_op; insn->reg2i12_format.immediate = imm; insn->reg2i12_format.rd = rd; insn->reg2i12_format.rj = rj; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_lu52id(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, int imm) { insn->reg2i12_format.opcode = lu52id_op; insn->reg2i12_format.immediate = imm; insn->reg2i12_format.rd = rd; insn->reg2i12_format.rj = rj; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_andi(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, int imm) { insn->reg2i12_format.opcode = andi_op; insn->reg2i12_format.immediate = imm; insn->reg2i12_format.rd = rd; insn->reg2i12_format.rj = rj; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_ori(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, int imm) { insn->reg2i12_format.opcode = ori_op; insn->reg2i12_format.immediate = imm; insn->reg2i12_format.rd = rd; insn->reg2i12_format.rj = rj; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_xori(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, int imm) { insn->reg2i12_format.opcode = xori_op; insn->reg2i12_format.immediate = imm; insn->reg2i12_format.rd = rd; insn->reg2i12_format.rj = rj; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_ldbu(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, int imm) { insn->reg2i12_format.opcode = ldbu_op; insn->reg2i12_format.immediate = imm; insn->reg2i12_format.rd = rd; insn->reg2i12_format.rj = rj; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_ldhu(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, int imm) { insn->reg2i12_format.opcode = ldhu_op; insn->reg2i12_format.immediate = imm; insn->reg2i12_format.rd = rd; insn->reg2i12_format.rj = rj; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_ldwu(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, int imm) { insn->reg2i12_format.opcode = ldwu_op; insn->reg2i12_format.immediate = imm; insn->reg2i12_format.rd = rd; insn->reg2i12_format.rj = rj; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_ldd(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, int imm) { insn->reg2i12_format.opcode = ldd_op; insn->reg2i12_format.immediate = imm; insn->reg2i12_format.rd = rd; insn->reg2i12_format.rj = rj; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_stb(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, int imm) { insn->reg2i12_format.opcode = stb_op; insn->reg2i12_format.immediate = imm; insn->reg2i12_format.rd = rd; insn->reg2i12_format.rj = rj; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_sth(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, int imm) { insn->reg2i12_format.opcode = sth_op; insn->reg2i12_format.immediate = imm; insn->reg2i12_format.rd = rd; insn->reg2i12_format.rj = rj; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_stw(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, int imm) { insn->reg2i12_format.opcode = stw_op; insn->reg2i12_format.immediate = imm; insn->reg2i12_format.rd = rd; insn->reg2i12_format.rj = rj; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_std(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, int imm) { insn->reg2i12_format.opcode = std_op; insn->reg2i12_format.immediate = imm; insn->reg2i12_format.rd = rd; insn->reg2i12_format.rj = rj; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_llw(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, int imm) { insn->reg2i14_format.opcode = llw_op; insn->reg2i14_format.immediate = imm; insn->reg2i14_format.rd = rd; insn->reg2i14_format.rj = rj; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_scw(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, int imm) { insn->reg2i14_format.opcode = scw_op; insn->reg2i14_format.immediate = imm; insn->reg2i14_format.rd = rd; insn->reg2i14_format.rj = rj; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_lld(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, int imm) { insn->reg2i14_format.opcode = lld_op; insn->reg2i14_format.immediate = imm; insn->reg2i14_format.rd = rd; insn->reg2i14_format.rj = rj; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_scd(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, int imm) { insn->reg2i14_format.opcode = scd_op; insn->reg2i14_format.immediate = imm; insn->reg2i14_format.rd = rd; insn->reg2i14_format.rj = rj; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_ldptrw(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, int imm) { insn->reg2i14_format.opcode = ldptrw_op; insn->reg2i14_format.immediate = imm; insn->reg2i14_format.rd = rd; insn->reg2i14_format.rj = rj; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_stptrw(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, int imm) { insn->reg2i14_format.opcode = stptrw_op; insn->reg2i14_format.immediate = imm; insn->reg2i14_format.rd = rd; insn->reg2i14_format.rj = rj; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_ldptrd(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, int imm) { insn->reg2i14_format.opcode = ldptrd_op; insn->reg2i14_format.immediate = imm; insn->reg2i14_format.rd = rd; insn->reg2i14_format.rj = rj; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_stptrd(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, int imm) { insn->reg2i14_format.opcode = stptrd_op; insn->reg2i14_format.immediate = imm; insn->reg2i14_format.rd = rd; insn->reg2i14_format.rj = rj; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_beq(union loongarch_instruction *insn, enum loongarch_gpr rj, enum loongarch_gpr rd, int offset) { insn->reg2i16_format.opcode = beq_op; insn->reg2i16_format.immediate = offset; insn->reg2i16_format.rj = rj; insn->reg2i16_format.rd = rd; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_bne(union loongarch_instruction *insn, enum loongarch_gpr rj, enum loongarch_gpr rd, int offset) { insn->reg2i16_format.opcode = bne_op; insn->reg2i16_format.immediate = offset; insn->reg2i16_format.rj = rj; insn->reg2i16_format.rd = rd; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_blt(union loongarch_instruction *insn, enum loongarch_gpr rj, enum loongarch_gpr rd, int offset) { insn->reg2i16_format.opcode = blt_op; insn->reg2i16_format.immediate = offset; insn->reg2i16_format.rj = rj; insn->reg2i16_format.rd = rd; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_bge(union loongarch_instruction *insn, enum loongarch_gpr rj, enum loongarch_gpr rd, int offset) { insn->reg2i16_format.opcode = bge_op; insn->reg2i16_format.immediate = offset; insn->reg2i16_format.rj = rj; insn->reg2i16_format.rd = rd; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_bltu(union loongarch_instruction *insn, enum loongarch_gpr rj, enum loongarch_gpr rd, int offset) { insn->reg2i16_format.opcode = bltu_op; insn->reg2i16_format.immediate = offset; insn->reg2i16_format.rj = rj; insn->reg2i16_format.rd = rd; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_bgeu(union loongarch_instruction *insn, enum loongarch_gpr rj, enum loongarch_gpr rd, int offset) { insn->reg2i16_format.opcode = bgeu_op; insn->reg2i16_format.immediate = offset; insn->reg2i16_format.rj = rj; insn->reg2i16_format.rd = rd; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_jirl(union loongarch_instruction *insn, enum loongarch_gpr rj, enum loongarch_gpr rd, int offset) { insn->reg2i16_format.opcode = jirl_op; insn->reg2i16_format.immediate = offset; insn->reg2i16_format.rj = rj; insn->reg2i16_format.rd = rd; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_bstrpickd(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, int msbd, int lsbd) { insn->reg2bstrd_format.opcode = bstrpickd_op; insn->reg2bstrd_format.msbd = msbd; insn->reg2bstrd_format.lsbd = lsbd; insn->reg2bstrd_format.rj = rj; insn->reg2bstrd_format.rd = rd; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_addd(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = addd_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_subd(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = subd_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_muld(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = muld_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_divdu(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = divdu_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_moddu(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = moddu_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_and(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = and_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_or(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = or_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_xor(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = xor_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_sllw(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = sllw_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_slld(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = slld_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_srlw(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = srlw_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_srld(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = srld_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_sraw(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = sraw_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_srad(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = srad_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_ldxbu(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = ldxbu_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_ldxhu(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = ldxhu_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_ldxwu(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = ldxwu_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_ldxd(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = ldxd_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_stxb(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = stxb_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_stxh(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = stxh_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_stxw(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = stxw_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_stxd(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = stxd_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_amaddw(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = amaddw_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_amaddd(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = amaddd_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_amandw(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = amandw_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_amandd(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = amandd_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_amorw(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = amorw_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_amord(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = amord_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_amxorw(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = amxorw_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_amxord(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = amxord_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_amswapw(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = amswapw_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_amswapd(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk) { insn->reg3_format.opcode = amswapd_op; insn->reg3_format.rd = rd; insn->reg3_format.rj = rj; insn->reg3_format.rk = rk; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void emit_alsld(union loongarch_instruction *insn, enum loongarch_gpr rd, enum loongarch_gpr rj, enum loongarch_gpr rk, int imm) { insn->reg3sa2_format.opcode = alsld_op; insn->reg3sa2_format.immediate = imm; insn->reg3sa2_format.rd = rd; insn->reg3sa2_format.rj = rj; insn->reg3sa2_format.rk = rk; }

struct pt_regs;

void emulate_load_store_insn(struct pt_regs *regs, void *addr, unsigned int *pc);
unsigned long unaligned_read(void *addr, void *value, unsigned long n, bool sign);
unsigned long unaligned_write(void *addr, unsigned long value, unsigned long n);

typedef u32 uprobe_opcode_t;
struct arch_uprobe {
 unsigned long resume_era;
 u32 insn[2];
 u32 ixol[2];
 bool simulate;
};

struct arch_uprobe_task {
 unsigned long saved_trap_nr;
};


bool uprobe_breakpoint_handler(struct pt_regs *regs);
bool uprobe_singlestep_handler(struct pt_regs *regs);

enum uprobe_task_state {
 UTASK_RUNNING,
 UTASK_SSTEP,
 UTASK_SSTEP_ACK,
 UTASK_SSTEP_TRAPPED,
};




struct uprobe_task {
 enum uprobe_task_state state;

 union {
  struct {
   struct arch_uprobe_task autask;
   unsigned long vaddr;
  };

  struct {
   struct callback_head dup_xol_work;
   unsigned long dup_xol_addr;
  };
 };

 struct uprobe *active_uprobe;
 unsigned long xol_vaddr;

 struct return_instance *return_instances;
 unsigned int depth;
};

struct return_instance {
 struct uprobe *uprobe;
 unsigned long func;
 unsigned long stack;
 unsigned long orig_ret_vaddr;
 bool chained;

 struct return_instance *next;
};

enum rp_check {
 RP_CHECK_CALL,
 RP_CHECK_CHAIN_CALL,
 RP_CHECK_RET,
};

struct xol_area;

struct uprobes_state {
 struct xol_area *xol_area;
};

extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) uprobes_init(void);
extern int set_swbp(struct arch_uprobe *aup, struct mm_struct *mm, unsigned long vaddr);
extern int set_orig_insn(struct arch_uprobe *aup, struct mm_struct *mm, unsigned long vaddr);
extern bool is_swbp_insn(uprobe_opcode_t *insn);
extern bool is_trap_insn(uprobe_opcode_t *insn);
extern unsigned long uprobe_get_swbp_addr(struct pt_regs *regs);
extern unsigned long uprobe_get_trap_addr(struct pt_regs *regs);
extern int uprobe_write_opcode(struct arch_uprobe *auprobe, struct mm_struct *mm, unsigned long vaddr, uprobe_opcode_t);
extern int uprobe_register(struct inode *inode, loff_t offset, struct uprobe_consumer *uc);
extern int uprobe_register_refctr(struct inode *inode, loff_t offset, loff_t ref_ctr_offset, struct uprobe_consumer *uc);
extern int uprobe_apply(struct inode *inode, loff_t offset, struct uprobe_consumer *uc, bool);
extern void uprobe_unregister(struct inode *inode, loff_t offset, struct uprobe_consumer *uc);
extern int uprobe_mmap(struct vm_area_struct *vma);
extern void uprobe_munmap(struct vm_area_struct *vma, unsigned long start, unsigned long end);
extern void uprobe_start_dup_mmap(void);
extern void uprobe_end_dup_mmap(void);
extern void uprobe_dup_mmap(struct mm_struct *oldmm, struct mm_struct *newmm);
extern void uprobe_free_utask(struct task_struct *t);
extern void uprobe_copy_process(struct task_struct *t, unsigned long flags);
extern int uprobe_post_sstep_notifier(struct pt_regs *regs);
extern int uprobe_pre_sstep_notifier(struct pt_regs *regs);
extern void uprobe_notify_resume(struct pt_regs *regs);
extern bool uprobe_deny_signal(void);
extern bool arch_uprobe_skip_sstep(struct arch_uprobe *aup, struct pt_regs *regs);
extern void uprobe_clear_state(struct mm_struct *mm);
extern int arch_uprobe_analyze_insn(struct arch_uprobe *aup, struct mm_struct *mm, unsigned long addr);
extern int arch_uprobe_pre_xol(struct arch_uprobe *aup, struct pt_regs *regs);
extern int arch_uprobe_post_xol(struct arch_uprobe *aup, struct pt_regs *regs);
extern bool arch_uprobe_xol_was_trapped(struct task_struct *tsk);
extern int arch_uprobe_exception_notify(struct notifier_block *self, unsigned long val, void *data);
extern void arch_uprobe_abort_xol(struct arch_uprobe *aup, struct pt_regs *regs);
extern unsigned long arch_uretprobe_hijack_return_addr(unsigned long trampoline_vaddr, struct pt_regs *regs);
extern bool arch_uretprobe_is_alive(struct return_instance *ret, enum rp_check ctx, struct pt_regs *regs);
extern bool arch_uprobe_ignore(struct arch_uprobe *aup, struct pt_regs *regs);
extern void arch_uprobe_copy_ixol(struct page *page, unsigned long vaddr,
      void *src, unsigned long len);























extern struct timezone sys_tz;

int get_timespec64(struct timespec64 *ts,
  const struct __kernel_timespec *uts);
int put_timespec64(const struct timespec64 *ts,
  struct __kernel_timespec *uts);
int get_itimerspec64(struct itimerspec64 *it,
   const struct __kernel_itimerspec *uit);
int put_itimerspec64(const struct itimerspec64 *it,
   struct __kernel_itimerspec *uit);

extern time64_t mktime64(const unsigned int year, const unsigned int mon,
   const unsigned int day, const unsigned int hour,
   const unsigned int min, const unsigned int sec);


extern void clear_itimer(void);




extern long do_utimes(int dfd, const char *filename, struct timespec64 *times, int flags);





struct tm {




 int tm_sec;

 int tm_min;

 int tm_hour;

 int tm_mday;

 int tm_mon;

 long tm_year;

 int tm_wday;

 int tm_yday;
};

void time64_to_tm(time64_t totalsecs, int offset, struct tm *result);

struct __kernel_timex_timeval {
 __kernel_time64_t tv_sec;
 long long tv_usec;
};

struct __kernel_timex {
 unsigned int modes;
 int :32;
 long long offset;
 long long freq;
 long long maxerror;
 long long esterror;
 int status;
 int :32;
 long long constant;
 long long precision;
 long long tolerance;


 struct __kernel_timex_timeval time;
 long long tick;

 long long ppsfreq;
 long long jitter;
 int shift;
 int :32;
 long long stabil;
 long long jitcnt;
 long long calcnt;
 long long errcnt;
 long long stbcnt;

 int tai;

 int :32; int :32; int :32; int :32;
 int :32; int :32; int :32; int :32;
 int :32; int :32; int :32;
};








unsigned long random_get_entropy_fallback(void);


typedef unsigned long cycles_t;



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) cycles_t get_cycles(void)
{
 return drdtime();
}
extern unsigned long tick_usec;
extern unsigned long tick_nsec;
extern int do_adjtimex(struct __kernel_timex *);
extern int do_clock_adjtime(const clockid_t which_clock, struct __kernel_timex * ktx);

extern void hardpps(const struct timespec64 *, const struct timespec64 *);

int read_current_timer(unsigned long *timer_val);





typedef s32 old_time32_t;

struct old_timespec32 {
 old_time32_t tv_sec;
 s32 tv_nsec;
};

struct old_timeval32 {
 old_time32_t tv_sec;
 s32 tv_usec;
};

struct old_itimerspec32 {
 struct old_timespec32 it_interval;
 struct old_timespec32 it_value;
};

struct old_utimbuf32 {
 old_time32_t actime;
 old_time32_t modtime;
};

struct old_timex32 {
 u32 modes;
 s32 offset;
 s32 freq;
 s32 maxerror;
 s32 esterror;
 s32 status;
 s32 constant;
 s32 precision;
 s32 tolerance;
 struct old_timeval32 time;
 s32 tick;
 s32 ppsfreq;
 s32 jitter;
 s32 shift;
 s32 stabil;
 s32 jitcnt;
 s32 calcnt;
 s32 errcnt;
 s32 stbcnt;
 s32 tai;

 s32:32; s32:32; s32:32; s32:32;
 s32:32; s32:32; s32:32; s32:32;
 s32:32; s32:32; s32:32;
};

extern int get_old_timespec32(struct timespec64 *, const void *);
extern int put_old_timespec32(const struct timespec64 *, void *);
extern int get_old_itimerspec32(struct itimerspec64 *its,
   const struct old_itimerspec32 *uits);
extern int put_old_itimerspec32(const struct itimerspec64 *its,
   struct old_itimerspec32 *uits);
struct __kernel_timex;
int get_old_timex32(struct __kernel_timex *, const struct old_timex32 *);
int put_old_timex32(struct old_timex32 *, const struct __kernel_timex *);







extern struct __kernel_old_timeval ns_to_kernel_old_timeval(s64 nsec);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool itimerspec64_valid(const struct itimerspec64 *its)
{
 if (!timespec64_valid(&(its->it_interval)) ||
  !timespec64_valid(&(its->it_value)))
  return false;

 return true;
}






struct timens_offset {
 s64 sec;
 u64 nsec;
};




extern int register_refined_jiffies(long clock_tick_rate);
extern u64 __attribute__((__aligned__((1 << 6)), __section__(".data..cacheline_aligned"))) jiffies_64;
extern unsigned long volatile __attribute__((__aligned__((1 << 6)), __section__(".data..cacheline_aligned"))) jiffies;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u64 get_jiffies_64(void)
{
 return (u64)jiffies;
}
extern unsigned long preset_lpj;
extern unsigned int jiffies_to_msecs(const unsigned long j);
extern unsigned int jiffies_to_usecs(const unsigned long j);







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u64 jiffies_to_nsecs(const unsigned long j)
{
 return (u64)jiffies_to_usecs(j) * 1000L;
}

extern u64 jiffies64_to_nsecs(u64 j);
extern u64 jiffies64_to_msecs(u64 j);

extern unsigned long __msecs_to_jiffies(const unsigned int m);






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long _msecs_to_jiffies(const unsigned int m)
{
 return (m + (1000L / 250) - 1) / (1000L / 250);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) unsigned long msecs_to_jiffies(const unsigned int m)
{
 if (__builtin_constant_p(m)) {
  if ((int)m < 0)
   return ((((long)(~0UL >> 1)) >> 1)-1);
  return _msecs_to_jiffies(m);
 } else {
  return __msecs_to_jiffies(m);
 }
}

extern unsigned long __usecs_to_jiffies(const unsigned int u);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long _usecs_to_jiffies(const unsigned int u)
{
 return (u + (1000000L / 250) - 1) / (1000000L / 250);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) unsigned long usecs_to_jiffies(const unsigned int u)
{
 if (__builtin_constant_p(u)) {
  if (u > jiffies_to_usecs(((((long)(~0UL >> 1)) >> 1)-1)))
   return ((((long)(~0UL >> 1)) >> 1)-1);
  return _usecs_to_jiffies(u);
 } else {
  return __usecs_to_jiffies(u);
 }
}

extern unsigned long timespec64_to_jiffies(const struct timespec64 *value);
extern void jiffies_to_timespec64(const unsigned long jiffies,
      struct timespec64 *value);
extern clock_t jiffies_to_clock_t(unsigned long x);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) clock_t jiffies_delta_to_clock_t(long delta)
{
 return jiffies_to_clock_t(__builtin_choose_expr(((!!(sizeof((typeof(0L) *)1 == (typeof(delta) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)(0L) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)(delta) * 0l)) : (int *)8))))), ((0L) > (delta) ? (0L) : (delta)), ({ typeof(0L) __UNIQUE_ID___x89 = (0L); typeof(delta) __UNIQUE_ID___y90 = (delta); ((__UNIQUE_ID___x89) > (__UNIQUE_ID___y90) ? (__UNIQUE_ID___x89) : (__UNIQUE_ID___y90)); })));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned int jiffies_delta_to_msecs(long delta)
{
 return jiffies_to_msecs(__builtin_choose_expr(((!!(sizeof((typeof(0L) *)1 == (typeof(delta) *)1))) && ((sizeof(int) == sizeof(*(8 ? ((void *)((long)(0L) * 0l)) : (int *)8))) && (sizeof(int) == sizeof(*(8 ? ((void *)((long)(delta) * 0l)) : (int *)8))))), ((0L) > (delta) ? (0L) : (delta)), ({ typeof(0L) __UNIQUE_ID___x91 = (0L); typeof(delta) __UNIQUE_ID___y92 = (delta); ((__UNIQUE_ID___x91) > (__UNIQUE_ID___y92) ? (__UNIQUE_ID___x91) : (__UNIQUE_ID___y92)); })));
}

extern unsigned long clock_t_to_jiffies(unsigned long x);
extern u64 jiffies_64_to_clock_t(u64 x);
extern u64 nsec_to_clock_t(u64 x);
extern u64 nsecs_to_jiffies64(u64 n);
extern unsigned long nsecs_to_jiffies(u64 n);



typedef s64 ktime_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) ktime_t ktime_set(const s64 secs, const unsigned long nsecs)
{
 if (__builtin_expect(!!(secs >= (((s64)~((u64)1 << 63)) / 1000000000L)), 0))
  return ((s64)~((u64)1 << 63));

 return secs * 1000000000L + (s64)nsecs;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) ktime_t timespec64_to_ktime(struct timespec64 ts)
{
 return ktime_set(ts.tv_sec, ts.tv_nsec);
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) s64 ktime_to_ns(const ktime_t kt)
{
 return kt;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int ktime_compare(const ktime_t cmp1, const ktime_t cmp2)
{
 if (cmp1 < cmp2)
  return -1;
 if (cmp1 > cmp2)
  return 1;
 return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool ktime_after(const ktime_t cmp1, const ktime_t cmp2)
{
 return ktime_compare(cmp1, cmp2) > 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool ktime_before(const ktime_t cmp1, const ktime_t cmp2)
{
 return ktime_compare(cmp1, cmp2) < 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) s64 ktime_divns(const ktime_t kt, s64 div)
{




 ({ int __ret_warn_on = !!(div < 0); if (__builtin_expect(!!(__ret_warn_on), 0)) do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/ktime.h\"; .popsection; .long 10002b - .; .short 154; .short (1 << 0)|(((9) << 8)); .popsection; 10001: break 1");; do { } while(0); } while (0); __builtin_expect(!!(__ret_warn_on), 0); });
 return kt / div;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) s64 ktime_to_us(const ktime_t kt)
{
 return ktime_divns(kt, 1000L);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) s64 ktime_to_ms(const ktime_t kt)
{
 return ktime_divns(kt, 1000000L);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) s64 ktime_us_delta(const ktime_t later, const ktime_t earlier)
{
       return ktime_to_us(((later) - (earlier)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) s64 ktime_ms_delta(const ktime_t later, const ktime_t earlier)
{
 return ktime_to_ms(((later) - (earlier)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) ktime_t ktime_add_us(const ktime_t kt, const u64 usec)
{
 return ((kt) + (usec * 1000L));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) ktime_t ktime_add_ms(const ktime_t kt, const u64 msec)
{
 return ((kt) + (msec * 1000000L));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) ktime_t ktime_sub_us(const ktime_t kt, const u64 usec)
{
 return ((kt) - (usec * 1000L));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) ktime_t ktime_sub_ms(const ktime_t kt, const u64 msec)
{
 return ((kt) - (msec * 1000000L));
}

extern ktime_t ktime_add_safe(const ktime_t lhs, const ktime_t rhs);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__warn_unused_result__)) bool ktime_to_timespec64_cond(const ktime_t kt,
             struct timespec64 *ts)
{
 if (kt) {
  *ts = ns_to_timespec64((kt));
  return true;
 } else {
  return false;
 }
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) ktime_t ns_to_ktime(u64 ns)
{
 return ns;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) ktime_t ms_to_ktime(u64 ms)
{
 return ms * 1000000L;
}











enum clocksource_ids {
 CSID_GENERIC = 0,
 CSID_ARM_ARCH_COUNTER,
 CSID_MAX,
};



void timekeeping_init(void);
extern int timekeeping_suspended;


extern void legacy_timer_tick(unsigned long ticks);




extern int do_settimeofday64(const struct timespec64 *ts);
extern int do_sys_settimeofday64(const struct timespec64 *tv,
     const struct timezone *tz);
extern void ktime_get_raw_ts64(struct timespec64 *ts);
extern void ktime_get_ts64(struct timespec64 *ts);
extern void ktime_get_real_ts64(struct timespec64 *tv);
extern void ktime_get_coarse_ts64(struct timespec64 *ts);
extern void ktime_get_coarse_real_ts64(struct timespec64 *ts);

void getboottime64(struct timespec64 *ts);




extern time64_t ktime_get_seconds(void);
extern time64_t __ktime_get_real_seconds(void);
extern time64_t ktime_get_real_seconds(void);





enum tk_offsets {
 TK_OFFS_REAL,
 TK_OFFS_BOOT,
 TK_OFFS_TAI,
 TK_OFFS_MAX,
};

extern ktime_t ktime_get(void);
extern ktime_t ktime_get_with_offset(enum tk_offsets offs);
extern ktime_t ktime_get_coarse_with_offset(enum tk_offsets offs);
extern ktime_t ktime_mono_to_any(ktime_t tmono, enum tk_offsets offs);
extern ktime_t ktime_get_raw(void);
extern u32 ktime_get_resolution_ns(void);




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) ktime_t ktime_get_real(void)
{
 return ktime_get_with_offset(TK_OFFS_REAL);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) ktime_t ktime_get_coarse_real(void)
{
 return ktime_get_coarse_with_offset(TK_OFFS_REAL);
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) ktime_t ktime_get_boottime(void)
{
 return ktime_get_with_offset(TK_OFFS_BOOT);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) ktime_t ktime_get_coarse_boottime(void)
{
 return ktime_get_coarse_with_offset(TK_OFFS_BOOT);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) ktime_t ktime_get_clocktai(void)
{
 return ktime_get_with_offset(TK_OFFS_TAI);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) ktime_t ktime_get_coarse_clocktai(void)
{
 return ktime_get_coarse_with_offset(TK_OFFS_TAI);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) ktime_t ktime_get_coarse(void)
{
 struct timespec64 ts;

 ktime_get_coarse_ts64(&ts);
 return timespec64_to_ktime(ts);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u64 ktime_get_coarse_ns(void)
{
 return ktime_to_ns(ktime_get_coarse());
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u64 ktime_get_coarse_real_ns(void)
{
 return ktime_to_ns(ktime_get_coarse_real());
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u64 ktime_get_coarse_boottime_ns(void)
{
 return ktime_to_ns(ktime_get_coarse_boottime());
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u64 ktime_get_coarse_clocktai_ns(void)
{
 return ktime_to_ns(ktime_get_coarse_clocktai());
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) ktime_t ktime_mono_to_real(ktime_t mono)
{
 return ktime_mono_to_any(mono, TK_OFFS_REAL);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u64 ktime_get_ns(void)
{
 return ktime_to_ns(ktime_get());
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u64 ktime_get_real_ns(void)
{
 return ktime_to_ns(ktime_get_real());
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u64 ktime_get_boottime_ns(void)
{
 return ktime_to_ns(ktime_get_boottime());
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u64 ktime_get_clocktai_ns(void)
{
 return ktime_to_ns(ktime_get_clocktai());
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u64 ktime_get_raw_ns(void)
{
 return ktime_to_ns(ktime_get_raw());
}

extern u64 ktime_get_mono_fast_ns(void);
extern u64 ktime_get_raw_fast_ns(void);
extern u64 ktime_get_boot_fast_ns(void);
extern u64 ktime_get_tai_fast_ns(void);
extern u64 ktime_get_real_fast_ns(void);






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void ktime_get_boottime_ts64(struct timespec64 *ts)
{
 *ts = ns_to_timespec64((ktime_get_boottime()));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void ktime_get_coarse_boottime_ts64(struct timespec64 *ts)
{
 *ts = ns_to_timespec64((ktime_get_coarse_boottime()));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) time64_t ktime_get_boottime_seconds(void)
{
 return ktime_divns(ktime_get_coarse_boottime(), 1000000000L);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void ktime_get_clocktai_ts64(struct timespec64 *ts)
{
 *ts = ns_to_timespec64((ktime_get_clocktai()));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void ktime_get_coarse_clocktai_ts64(struct timespec64 *ts)
{
 *ts = ns_to_timespec64((ktime_get_coarse_clocktai()));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) time64_t ktime_get_clocktai_seconds(void)
{
 return ktime_divns(ktime_get_coarse_clocktai(), 1000000000L);
}




extern bool timekeeping_rtc_skipsuspend(void);
extern bool timekeeping_rtc_skipresume(void);

extern void timekeeping_inject_sleeptime64(const struct timespec64 *delta);







struct ktime_timestamps {
 u64 mono;
 u64 boot;
 u64 real;
};
struct system_time_snapshot {
 u64 cycles;
 ktime_t real;
 ktime_t raw;
 enum clocksource_ids cs_id;
 unsigned int clock_was_set_seq;
 u8 cs_was_changed_seq;
};
struct system_device_crosststamp {
 ktime_t device;
 ktime_t sys_realtime;
 ktime_t sys_monoraw;
};
struct system_counterval_t {
 u64 cycles;
 struct clocksource *cs;
};




extern int get_device_system_crosststamp(
   int (*get_time_fn)(ktime_t *device_time,
    struct system_counterval_t *system_counterval,
    void *ctx),
   void *ctx,
   struct system_time_snapshot *history,
   struct system_device_crosststamp *xtstamp);




extern void ktime_get_snapshot(struct system_time_snapshot *systime_snapshot);


extern void ktime_get_fast_timestamps(struct ktime_timestamps *snap);




extern int persistent_clock_is_local;

extern void read_persistent_clock64(struct timespec64 *ts);
void read_persistent_wall_and_boot_offset(struct timespec64 *wall_clock,
       struct timespec64 *boot_offset);

extern int update_persistent_clock64(struct timespec64 now);








enum debug_obj_state {
 ODEBUG_STATE_NONE,
 ODEBUG_STATE_INIT,
 ODEBUG_STATE_INACTIVE,
 ODEBUG_STATE_ACTIVE,
 ODEBUG_STATE_DESTROYED,
 ODEBUG_STATE_NOTAVAILABLE,
 ODEBUG_STATE_MAX,
};

struct debug_obj_descr;
struct debug_obj {
 struct hlist_node node;
 enum debug_obj_state state;
 unsigned int astate;
 void *object;
 const struct debug_obj_descr *descr;
};
struct debug_obj_descr {
 const char *name;
 void *(*debug_hint)(void *addr);
 bool (*is_static_object)(void *addr);
 bool (*fixup_init)(void *addr, enum debug_obj_state state);
 bool (*fixup_activate)(void *addr, enum debug_obj_state state);
 bool (*fixup_destroy)(void *addr, enum debug_obj_state state);
 bool (*fixup_free)(void *addr, enum debug_obj_state state);
 bool (*fixup_assert_init)(void *addr, enum debug_obj_state state);
};


extern void debug_object_init (void *addr, const struct debug_obj_descr *descr);
extern void
debug_object_init_on_stack(void *addr, const struct debug_obj_descr *descr);
extern int debug_object_activate (void *addr, const struct debug_obj_descr *descr);
extern void debug_object_deactivate(void *addr, const struct debug_obj_descr *descr);
extern void debug_object_destroy (void *addr, const struct debug_obj_descr *descr);
extern void debug_object_free (void *addr, const struct debug_obj_descr *descr);
extern void debug_object_assert_init(void *addr, const struct debug_obj_descr *descr);






extern void
debug_object_active_state(void *addr, const struct debug_obj_descr *descr,
     unsigned int expect, unsigned int next);

extern void debug_objects_early_init(void);
extern void debug_objects_mem_init(void);
extern void debug_check_no_obj_freed(const void *address, unsigned long size);


struct timer_list {




 struct hlist_node entry;
 unsigned long expires;
 void (*function)(struct timer_list *);
 u32 flags;


 struct lockdep_map lockdep_map;

};
void init_timer_key(struct timer_list *timer,
      void (*func)(struct timer_list *), unsigned int flags,
      const char *name, struct lock_class_key *key);


extern void init_timer_on_stack_key(struct timer_list *timer,
        void (*func)(struct timer_list *),
        unsigned int flags, const char *name,
        struct lock_class_key *key);
extern void destroy_timer_on_stack(struct timer_list *timer);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int timer_pending(const struct timer_list * timer)
{
 return !hlist_unhashed_lockless(&timer->entry);
}

extern void add_timer_on(struct timer_list *timer, int cpu);
extern int mod_timer(struct timer_list *timer, unsigned long expires);
extern int mod_timer_pending(struct timer_list *timer, unsigned long expires);
extern int timer_reduce(struct timer_list *timer, unsigned long expires);







extern void add_timer(struct timer_list *timer);

extern int try_to_del_timer_sync(struct timer_list *timer);
extern int timer_delete_sync(struct timer_list *timer);
extern int timer_delete(struct timer_list *timer);
extern int timer_shutdown_sync(struct timer_list *timer);
extern int timer_shutdown(struct timer_list *timer);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int del_timer_sync(struct timer_list *timer)
{
 return timer_delete_sync(timer);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int del_timer(struct timer_list *timer)
{
 return timer_delete(timer);
}

extern void init_timers(void);
struct hrtimer;
extern enum hrtimer_restart it_real_fn(struct hrtimer *);

unsigned long __round_jiffies(unsigned long j, int cpu);
unsigned long __round_jiffies_relative(unsigned long j, int cpu);
unsigned long round_jiffies(unsigned long j);
unsigned long round_jiffies_relative(unsigned long j);

unsigned long __round_jiffies_up(unsigned long j, int cpu);
unsigned long __round_jiffies_up_relative(unsigned long j, int cpu);
unsigned long round_jiffies_up(unsigned long j);
unsigned long round_jiffies_up_relative(unsigned long j);


int timers_prepare_cpu(unsigned int cpu);
int timers_dead_cpu(unsigned int cpu);








struct workqueue_struct;

struct work_struct;
typedef void (*work_func_t)(struct work_struct *work);
void delayed_work_timer_fn(struct timer_list *t);







enum {
 WORK_STRUCT_PENDING_BIT = 0,
 WORK_STRUCT_INACTIVE_BIT= 1,
 WORK_STRUCT_PWQ_BIT = 2,
 WORK_STRUCT_LINKED_BIT = 3,

 WORK_STRUCT_STATIC_BIT = 4,
 WORK_STRUCT_COLOR_SHIFT = 5,




 WORK_STRUCT_COLOR_BITS = 4,

 WORK_STRUCT_PENDING = 1 << WORK_STRUCT_PENDING_BIT,
 WORK_STRUCT_INACTIVE = 1 << WORK_STRUCT_INACTIVE_BIT,
 WORK_STRUCT_PWQ = 1 << WORK_STRUCT_PWQ_BIT,
 WORK_STRUCT_LINKED = 1 << WORK_STRUCT_LINKED_BIT,

 WORK_STRUCT_STATIC = 1 << WORK_STRUCT_STATIC_BIT,




 WORK_NR_COLORS = (1 << WORK_STRUCT_COLOR_BITS),


 WORK_CPU_UNBOUND = 64,






 WORK_STRUCT_FLAG_BITS = WORK_STRUCT_COLOR_SHIFT +
      WORK_STRUCT_COLOR_BITS,


 WORK_OFFQ_FLAG_BASE = WORK_STRUCT_COLOR_SHIFT,

 __WORK_OFFQ_CANCELING = WORK_OFFQ_FLAG_BASE,






 WORK_OFFQ_FLAG_BITS = 1,
 WORK_OFFQ_POOL_SHIFT = WORK_OFFQ_FLAG_BASE + WORK_OFFQ_FLAG_BITS,
 WORK_OFFQ_LEFT = 64 - WORK_OFFQ_POOL_SHIFT,
 WORK_OFFQ_POOL_BITS = WORK_OFFQ_LEFT <= 31 ? WORK_OFFQ_LEFT : 31,


 WORK_BUSY_PENDING = 1 << 0,
 WORK_BUSY_RUNNING = 1 << 1,


 WORKER_DESC_LEN = 24,
};
struct work_struct {
 atomic_long_t data;
 struct list_head entry;
 work_func_t func;

 struct lockdep_map lockdep_map;

};





struct delayed_work {
 struct work_struct work;
 struct timer_list timer;


 struct workqueue_struct *wq;
 int cpu;
};

struct rcu_work {
 struct work_struct work;
 struct callback_head rcu;


 struct workqueue_struct *wq;
};






struct workqueue_attrs {



 int nice;




 cpumask_var_t cpumask;
 bool no_numa;
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) struct delayed_work *to_delayed_work(struct work_struct *work)
{
 return ({ void *__mptr = (void *)(work); _Static_assert(__builtin_types_compatible_p(typeof(*(work)), typeof(((struct delayed_work *)0)->work)) || __builtin_types_compatible_p(typeof(*(work)), typeof(void)), "pointer type mismatch in container_of()"); ((struct delayed_work *)(__mptr - __builtin_offsetof(struct delayed_work, work))); });
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) struct rcu_work *to_rcu_work(struct work_struct *work)
{
 return ({ void *__mptr = (void *)(work); _Static_assert(__builtin_types_compatible_p(typeof(*(work)), typeof(((struct rcu_work *)0)->work)) || __builtin_types_compatible_p(typeof(*(work)), typeof(void)), "pointer type mismatch in container_of()"); ((struct rcu_work *)(__mptr - __builtin_offsetof(struct rcu_work, work))); });
}

struct execute_work {
 struct work_struct work;
};
extern void __init_work(struct work_struct *work, int onstack);
extern void destroy_work_on_stack(struct work_struct *work);
extern void destroy_delayed_work_on_stack(struct delayed_work *work);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned int work_static(struct work_struct *work)
{
 return *((unsigned long *)(&(work)->data)) & WORK_STRUCT_STATIC;
}
enum {
 WQ_UNBOUND = 1 << 1,
 WQ_FREEZABLE = 1 << 2,
 WQ_MEM_RECLAIM = 1 << 3,
 WQ_HIGHPRI = 1 << 4,
 WQ_CPU_INTENSIVE = 1 << 5,
 WQ_SYSFS = 1 << 6,
 WQ_POWER_EFFICIENT = 1 << 7,

 __WQ_DESTROYING = 1 << 15,
 __WQ_DRAINING = 1 << 16,
 __WQ_ORDERED = 1 << 17,
 __WQ_LEGACY = 1 << 18,
 __WQ_ORDERED_EXPLICIT = 1 << 19,

 WQ_MAX_ACTIVE = 512,
 WQ_MAX_UNBOUND_PER_CPU = 4,
 WQ_DFL_ACTIVE = WQ_MAX_ACTIVE / 2,
};
extern struct workqueue_struct *system_wq;
extern struct workqueue_struct *system_highpri_wq;
extern struct workqueue_struct *system_long_wq;
extern struct workqueue_struct *system_unbound_wq;
extern struct workqueue_struct *system_freezable_wq;
extern struct workqueue_struct *system_power_efficient_wq;
extern struct workqueue_struct *system_freezable_power_efficient_wq;
__attribute__((__format__(printf, 1, 4))) struct workqueue_struct *
alloc_workqueue(const char *fmt, unsigned int flags, int max_active, ...);
extern void destroy_workqueue(struct workqueue_struct *wq);

struct workqueue_attrs *alloc_workqueue_attrs(void);
void free_workqueue_attrs(struct workqueue_attrs *attrs);
int apply_workqueue_attrs(struct workqueue_struct *wq,
     const struct workqueue_attrs *attrs);
int workqueue_set_unbound_cpumask(cpumask_var_t cpumask);

extern bool queue_work_on(int cpu, struct workqueue_struct *wq,
   struct work_struct *work);
extern bool queue_work_node(int node, struct workqueue_struct *wq,
       struct work_struct *work);
extern bool queue_delayed_work_on(int cpu, struct workqueue_struct *wq,
   struct delayed_work *work, unsigned long delay);
extern bool mod_delayed_work_on(int cpu, struct workqueue_struct *wq,
   struct delayed_work *dwork, unsigned long delay);
extern bool queue_rcu_work(struct workqueue_struct *wq, struct rcu_work *rwork);

extern void __flush_workqueue(struct workqueue_struct *wq);
extern void drain_workqueue(struct workqueue_struct *wq);

extern int schedule_on_each_cpu(work_func_t func);

int execute_in_process_context(work_func_t fn, struct execute_work *);

extern bool flush_work(struct work_struct *work);
extern bool cancel_work(struct work_struct *work);
extern bool cancel_work_sync(struct work_struct *work);

extern bool flush_delayed_work(struct delayed_work *dwork);
extern bool cancel_delayed_work(struct delayed_work *dwork);
extern bool cancel_delayed_work_sync(struct delayed_work *dwork);

extern bool flush_rcu_work(struct rcu_work *rwork);

extern void workqueue_set_max_active(struct workqueue_struct *wq,
         int max_active);
extern struct work_struct *current_work(void);
extern bool current_is_workqueue_rescuer(void);
extern bool workqueue_congested(int cpu, struct workqueue_struct *wq);
extern unsigned int work_busy(struct work_struct *work);
extern __attribute__((__format__(printf, 1, 2))) void set_worker_desc(const char *fmt, ...);
extern void print_worker_info(const char *log_lvl, struct task_struct *task);
extern void show_all_workqueues(void);
extern void show_freezable_workqueues(void);
extern void show_one_workqueue(struct workqueue_struct *wq);
extern void wq_worker_comm(char *buf, size_t size, struct task_struct *task);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool queue_work(struct workqueue_struct *wq,
         struct work_struct *work)
{
 return queue_work_on(WORK_CPU_UNBOUND, wq, work);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool queue_delayed_work(struct workqueue_struct *wq,
          struct delayed_work *dwork,
          unsigned long delay)
{
 return queue_delayed_work_on(WORK_CPU_UNBOUND, wq, dwork, delay);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool mod_delayed_work(struct workqueue_struct *wq,
        struct delayed_work *dwork,
        unsigned long delay)
{
 return mod_delayed_work_on(WORK_CPU_UNBOUND, wq, dwork, delay);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool schedule_work_on(int cpu, struct work_struct *work)
{
 return queue_work_on(cpu, system_wq, work);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool schedule_work(struct work_struct *work)
{
 return queue_work(system_wq, work);
}
extern void __warn_flushing_systemwide_wq(void)
 __attribute__((__warning__("Please avoid flushing system-wide workqueues.")));
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool schedule_delayed_work_on(int cpu, struct delayed_work *dwork,
         unsigned long delay)
{
 return queue_delayed_work_on(cpu, system_wq, dwork, delay);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool schedule_delayed_work(struct delayed_work *dwork,
      unsigned long delay)
{
 return queue_delayed_work(system_wq, dwork, delay);
}
long work_on_cpu(int cpu, long (*fn)(void *), void *arg);
long work_on_cpu_safe(int cpu, long (*fn)(void *), void *arg);



extern void freeze_workqueues_begin(void);
extern bool freeze_workqueues_busy(void);
extern void thaw_workqueues(void);



int workqueue_sysfs_register(struct workqueue_struct *wq);






void wq_watchdog_touch(int cpu);





int workqueue_prepare_cpu(unsigned int cpu);
int workqueue_online_cpu(unsigned int cpu);
int workqueue_offline_cpu(unsigned int cpu);


void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) workqueue_init_early(void);
void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) workqueue_init(void);

struct percpu_counter {
 raw_spinlock_t lock;
 s64 count;

 struct list_head list;

 s32 *counters;
};

extern int percpu_counter_batch;

int __percpu_counter_init(struct percpu_counter *fbc, s64 amount, gfp_t gfp,
     struct lock_class_key *key);
void percpu_counter_destroy(struct percpu_counter *fbc);
void percpu_counter_set(struct percpu_counter *fbc, s64 amount);
void percpu_counter_add_batch(struct percpu_counter *fbc, s64 amount,
         s32 batch);
s64 __percpu_counter_sum(struct percpu_counter *fbc);
int __percpu_counter_compare(struct percpu_counter *fbc, s64 rhs, s32 batch);
void percpu_counter_sync(struct percpu_counter *fbc);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int percpu_counter_compare(struct percpu_counter *fbc, s64 rhs)
{
 return __percpu_counter_compare(fbc, rhs, percpu_counter_batch);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void percpu_counter_add(struct percpu_counter *fbc, s64 amount)
{
 percpu_counter_add_batch(fbc, amount, percpu_counter_batch);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void
percpu_counter_add_local(struct percpu_counter *fbc, s64 amount)
{
 percpu_counter_add_batch(fbc, amount, ((int)(~0U >> 1)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) s64 percpu_counter_sum_positive(struct percpu_counter *fbc)
{
 s64 ret = __percpu_counter_sum(fbc);
 return ret < 0 ? 0 : ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) s64 percpu_counter_sum(struct percpu_counter *fbc)
{
 return __percpu_counter_sum(fbc);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) s64 percpu_counter_read(struct percpu_counter *fbc)
{
 return fbc->count;
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) s64 percpu_counter_read_positive(struct percpu_counter *fbc)
{

 s64 ret = ({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_93(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(fbc->count) == sizeof(char) || sizeof(fbc->count) == sizeof(short) || sizeof(fbc->count) == sizeof(int) || sizeof(fbc->count) == sizeof(long)) || sizeof(fbc->count) == sizeof(long long))) __compiletime_assert_93(); } while (0); (*(const volatile typeof( _Generic((fbc->count), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (fbc->count))) *)&(fbc->count)); });

 if (ret >= 0)
  return ret;
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool percpu_counter_initialized(struct percpu_counter *fbc)
{
 return (fbc->counters != ((void *)0));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void percpu_counter_inc(struct percpu_counter *fbc)
{
 percpu_counter_add(fbc, 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void percpu_counter_dec(struct percpu_counter *fbc)
{
 percpu_counter_add(fbc, -1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void percpu_counter_sub(struct percpu_counter *fbc, s64 amount)
{
 percpu_counter_add(fbc, -amount);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void
percpu_counter_sub_local(struct percpu_counter *fbc, s64 amount)
{
 percpu_counter_add_local(fbc, -amount);
}

typedef struct {
 u64 asid[64];
 void *vdso;
} mm_context_t;








struct address_space;
struct mem_cgroup;
struct page {
 unsigned long flags;







 union {
  struct {





   union {
    struct list_head lru;


    struct {

     void *__filler;

     unsigned int mlock_count;
    };


    struct list_head buddy_list;
    struct list_head pcp_list;
   };

   struct address_space *mapping;
   union {
    unsigned long index;
    unsigned long share;
   };






   unsigned long private;
  };
  struct {




   unsigned long pp_magic;
   struct page_pool *pp;
   unsigned long _pp_mapping_pad;
   unsigned long dma_addr;
   union {




    unsigned long dma_addr_upper;




    atomic_long_t pp_frag_count;
   };
  };
  struct {
   unsigned long compound_head;
  };
  struct {
   unsigned long _pt_pad_1;
   pgtable_t pmd_huge_pte;




   unsigned long _pt_pad_2;
   union {
    struct mm_struct *pt_mm;
    atomic_t pt_frag_refcount;
   };

   spinlock_t *ptl;



  };
  struct {

   struct dev_pagemap *pgmap;
   void *zone_device_data;
  };


  struct callback_head callback_head;
 };

 union {




  atomic_t _mapcount;







  unsigned int page_type;
 };


 atomic_t _refcount;


 unsigned long memcg_data;
} __attribute__((__aligned__(sizeof(unsigned long))));
struct encoded_page;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) struct encoded_page *encode_page(struct page *page, unsigned long flags)
{
 do { __attribute__((__noreturn__)) extern void __compiletime_assert_94(void) __attribute__((__error__("BUILD_BUG_ON failed: " "flags > ENCODE_PAGE_BITS"))); if (!(!(flags > 3ul))) __compiletime_assert_94(); } while (0);
 return (struct encoded_page *)(flags | (unsigned long)page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long encoded_page_flags(struct encoded_page *page)
{
 return 3ul & (unsigned long)page;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) struct page *encoded_page_ptr(struct encoded_page *page)
{
 return (struct page *)(~3ul & (unsigned long)page);
}
struct folio {

 union {
  struct {

   unsigned long flags;
   union {
    struct list_head lru;

    struct {
     void *__filler;

     unsigned int mlock_count;

    };

   };
   struct address_space *mapping;
   unsigned long index;
   void *private;
   atomic_t _mapcount;
   atomic_t _refcount;

   unsigned long memcg_data;


  };
  struct page page;
 };
 union {
  struct {
   unsigned long _flags_1;
   unsigned long _head_1;

   unsigned char _folio_dtor;
   unsigned char _folio_order;
   atomic_t _entire_mapcount;
   atomic_t _nr_pages_mapped;
   atomic_t _pincount;

   unsigned int _folio_nr_pages;


  };
  struct page __page_1;
 };
 union {
  struct {
   unsigned long _flags_2;
   unsigned long _head_2;

   void *_hugetlb_subpool;
   void *_hugetlb_cgroup;
   void *_hugetlb_cgroup_rsvd;
   void *_hugetlb_hwpoison;

  };
  struct {
   unsigned long _flags_2a;
   unsigned long _head_2a;

   struct list_head _deferred_list;

  };
  struct page __page_2;
 };
};



_Static_assert(__builtin_offsetof(struct page, flags) == __builtin_offsetof(struct folio, flags), "offsetof(struct page, flags) == offsetof(struct folio, flags)");
_Static_assert(__builtin_offsetof(struct page, lru) == __builtin_offsetof(struct folio, lru), "offsetof(struct page, lru) == offsetof(struct folio, lru)");
_Static_assert(__builtin_offsetof(struct page, mapping) == __builtin_offsetof(struct folio, mapping), "offsetof(struct page, mapping) == offsetof(struct folio, mapping)");
_Static_assert(__builtin_offsetof(struct page, compound_head) == __builtin_offsetof(struct folio, lru), "offsetof(struct page, compound_head) == offsetof(struct folio, lru)");
_Static_assert(__builtin_offsetof(struct page, index) == __builtin_offsetof(struct folio, index), "offsetof(struct page, index) == offsetof(struct folio, index)");
_Static_assert(__builtin_offsetof(struct page, private) == __builtin_offsetof(struct folio, private), "offsetof(struct page, private) == offsetof(struct folio, private)");
_Static_assert(__builtin_offsetof(struct page, _mapcount) == __builtin_offsetof(struct folio, _mapcount), "offsetof(struct page, _mapcount) == offsetof(struct folio, _mapcount)");
_Static_assert(__builtin_offsetof(struct page, _refcount) == __builtin_offsetof(struct folio, _refcount), "offsetof(struct page, _refcount) == offsetof(struct folio, _refcount)");

_Static_assert(__builtin_offsetof(struct page, memcg_data) == __builtin_offsetof(struct folio, memcg_data), "offsetof(struct page, memcg_data) == offsetof(struct folio, memcg_data)");





_Static_assert(__builtin_offsetof(struct folio, _flags_1) == __builtin_offsetof(struct page, flags) + sizeof(struct page), "offsetof(struct folio, _flags_1) == offsetof(struct page, flags) + sizeof(struct page)");
_Static_assert(__builtin_offsetof(struct folio, _head_1) == __builtin_offsetof(struct page, compound_head) + sizeof(struct page), "offsetof(struct folio, _head_1) == offsetof(struct page, compound_head) + sizeof(struct page)");




_Static_assert(__builtin_offsetof(struct folio, _flags_2) == __builtin_offsetof(struct page, flags) + 2 * sizeof(struct page), "offsetof(struct folio, _flags_2) == offsetof(struct page, flags) + 2 * sizeof(struct page)");
_Static_assert(__builtin_offsetof(struct folio, _head_2) == __builtin_offsetof(struct page, compound_head) + 2 * sizeof(struct page), "offsetof(struct folio, _head_2) == offsetof(struct page, compound_head) + 2 * sizeof(struct page)");
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void set_page_private(struct page *page, unsigned long private)
{
 page->private = private;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void *folio_get_private(struct folio *folio)
{
 return folio->private;
}

struct page_frag_cache {
 void * va;

 __u16 offset;
 __u16 size;






 unsigned int pagecnt_bias;
 bool pfmemalloc;
};

typedef unsigned long vm_flags_t;






struct vm_region {
 struct rb_node vm_rb;
 vm_flags_t vm_flags;
 unsigned long vm_start;
 unsigned long vm_end;
 unsigned long vm_top;
 unsigned long vm_pgoff;
 struct file *vm_file;

 int vm_usage;
 bool vm_icache_flushed : 1;

};



struct vm_userfaultfd_ctx {
 struct userfaultfd_ctx *ctx;
};





struct anon_vma_name {
 struct kref kref;

 char name[];
};

struct vma_lock {
 struct rw_semaphore lock;
};

struct vma_numab_state {
 unsigned long next_scan;
 unsigned long next_pid_reset;
 unsigned long access_pids[2];
};







struct vm_area_struct {


 union {
  struct {

   unsigned long vm_start;
   unsigned long vm_end;
  };



 };

 struct mm_struct *vm_mm;
 pgprot_t vm_page_prot;





 union {
  const vm_flags_t vm_flags;
  vm_flags_t __vm_flags;
 };
 struct {
  struct rb_node rb;
  unsigned long rb_subtree_last;
 } shared;







 struct list_head anon_vma_chain;

 struct anon_vma *anon_vma;


 const struct vm_operations_struct *vm_ops;


 unsigned long vm_pgoff;

 struct file * vm_file;
 void * vm_private_data;







 struct anon_vma_name *anon_name;


 atomic_long_t swap_readahead_info;





 struct mempolicy *vm_policy;


 struct vma_numab_state *numab_state;

 struct vm_userfaultfd_ctx vm_userfaultfd_ctx;
} __attribute__((randomize_layout));


struct mm_cid {
 u64 time;
 int cid;
};


struct kioctx_table;
struct mm_struct {
 struct {




  struct {







   atomic_t mm_count;
  } __attribute__((__aligned__((1 << 6))));

  struct maple_tree mm_mt;

  unsigned long (*get_unmapped_area) (struct file *filp,
    unsigned long addr, unsigned long len,
    unsigned long pgoff, unsigned long flags);

  unsigned long mmap_base;
  unsigned long mmap_legacy_base;





  unsigned long task_size;
  pgd_t * pgd;
  atomic_t membarrier_state;
  atomic_t mm_users;
  struct mm_cid *pcpu_cid;





  unsigned long mm_cid_next_scan;


  atomic_long_t pgtables_bytes;

  int map_count;

  spinlock_t page_table_lock;
  struct rw_semaphore mmap_lock;

  struct list_head mmlist;
  unsigned long hiwater_rss;
  unsigned long hiwater_vm;

  unsigned long total_vm;
  unsigned long locked_vm;
  atomic64_t pinned_vm;
  unsigned long data_vm;
  unsigned long exec_vm;
  unsigned long stack_vm;
  unsigned long def_flags;






  seqcount_t write_protect_seq;

  spinlock_t arg_lock;

  unsigned long start_code, end_code, start_data, end_data;
  unsigned long start_brk, brk, start_stack;
  unsigned long arg_start, arg_end, env_start, env_end;

  unsigned long saved_auxv[(2*(1 + 22 + 1))];

  struct percpu_counter rss_stat[NR_MM_COUNTERS];

  struct linux_binfmt *binfmt;


  mm_context_t context;

  unsigned long flags;


  spinlock_t ioctx_lock;
  struct kioctx_table *ioctx_table;
  struct task_struct *owner;

  struct user_namespace *user_ns;


  struct file *exe_file;

  struct mmu_notifier_subscriptions *notifier_subscriptions;


  pgtable_t pmd_huge_pte;







  unsigned long numa_next_scan;


  unsigned long numa_scan_offset;


  int numa_scan_seq;






  atomic_t tlb_flush_pending;




  struct uprobes_state uprobes_state;




  atomic_long_t hugetlb_usage;

  struct work_struct async_put_work;
  unsigned long ksm_merging_pages;




  unsigned long ksm_rmap_items;




  unsigned long ksm_zero_pages;


  struct {

   struct list_head list;





   unsigned long bitmap;


   struct mem_cgroup *memcg;

  } lru_gen;

 } __attribute__((randomize_layout));





 unsigned long cpu_bitmap[];
};



extern struct mm_struct init_mm;


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void mm_init_cpumask(struct mm_struct *mm)
{
 unsigned long cpu_bitmap = (unsigned long)mm;

 cpu_bitmap += __builtin_offsetof(struct mm_struct, cpu_bitmap);
 cpumask_clear((struct cpumask *)cpu_bitmap);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) cpumask_t *mm_cpumask(struct mm_struct *mm)
{
 return (struct cpumask *)&mm->cpu_bitmap;
}



struct lru_gen_mm_list {

 struct list_head fifo;

 spinlock_t lock;
};

void lru_gen_add_mm(struct mm_struct *mm);
void lru_gen_del_mm(struct mm_struct *mm);

void lru_gen_migrate_mm(struct mm_struct *mm);


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void lru_gen_init_mm(struct mm_struct *mm)
{
 INIT_LIST_HEAD(&mm->lru_gen.list);
 mm->lru_gen.bitmap = 0;

 mm->lru_gen.memcg = ((void *)0);

}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void lru_gen_use_mm(struct mm_struct *mm)
{





 do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_95(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(mm->lru_gen.bitmap) == sizeof(char) || sizeof(mm->lru_gen.bitmap) == sizeof(short) || sizeof(mm->lru_gen.bitmap) == sizeof(int) || sizeof(mm->lru_gen.bitmap) == sizeof(long)) || sizeof(mm->lru_gen.bitmap) == sizeof(long long))) __compiletime_assert_95(); } while (0); do { *(volatile typeof(mm->lru_gen.bitmap) *)&(mm->lru_gen.bitmap) = (-1); } while (0); } while (0);
}
struct vma_iterator {
 struct ma_state mas;
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void vma_iter_init(struct vma_iterator *vmi,
  struct mm_struct *mm, unsigned long addr)
{
 mas_init(&vmi->mas, &mm->mm_mt, addr);
}



enum mm_cid_state {
 MM_CID_UNSET = -1U,
 MM_CID_LAZY_PUT = (1U << 31),
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool mm_cid_is_unset(int cid)
{
 return cid == MM_CID_UNSET;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool mm_cid_is_lazy_put(int cid)
{
 return !mm_cid_is_unset(cid) && (cid & MM_CID_LAZY_PUT);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool mm_cid_is_valid(int cid)
{
 return !(cid & MM_CID_LAZY_PUT);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int mm_cid_set_lazy_put(int cid)
{
 return cid | MM_CID_LAZY_PUT;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int mm_cid_clear_lazy_put(int cid)
{
 return cid & ~MM_CID_LAZY_PUT;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) cpumask_t *mm_cidmask(struct mm_struct *mm)
{
 unsigned long cid_bitmap = (unsigned long)mm;

 cid_bitmap += __builtin_offsetof(struct mm_struct, cpu_bitmap);

 cid_bitmap += cpumask_size();
 return (struct cpumask *)cid_bitmap;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void mm_init_cid(struct mm_struct *mm)
{
 int i;

 for (((i)) = 0; ((i)) = find_next_bit((((((const struct cpumask *)&__cpu_possible_mask))->bits)), (((unsigned int)64)), ((i))), ((i)) < (((unsigned int)64)); ((i))++) {
  struct mm_cid *pcpu_cid = ({ do { const void *__vpp_verify = (typeof((mm->pcpu_cid) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*((mm->pcpu_cid))) *)((mm->pcpu_cid))); (typeof((typeof(*((mm->pcpu_cid))) *)((mm->pcpu_cid)))) (__ptr + (((__per_cpu_offset[(i)])))); }); });

  pcpu_cid->cid = MM_CID_UNSET;
  pcpu_cid->time = 0;
 }
 cpumask_clear(mm_cidmask(mm));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int mm_alloc_cid(struct mm_struct *mm)
{
 mm->pcpu_cid = (typeof(struct mm_cid) *)__alloc_percpu(sizeof(struct mm_cid), __alignof__(struct mm_cid));
 if (!mm->pcpu_cid)
  return -12;
 mm_init_cid(mm);
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void mm_destroy_cid(struct mm_struct *mm)
{
 free_percpu(mm->pcpu_cid);
 mm->pcpu_cid = ((void *)0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned int mm_cid_size(void)
{
 return cpumask_size();
}
struct mmu_gather;
extern void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm);
extern void tlb_gather_mmu_fullmm(struct mmu_gather *tlb, struct mm_struct *mm);
extern void tlb_finish_mmu(struct mmu_gather *tlb);

struct vm_fault;






typedef unsigned int vm_fault_t;
enum vm_fault_reason {
 VM_FAULT_OOM = ( vm_fault_t)0x000001,
 VM_FAULT_SIGBUS = ( vm_fault_t)0x000002,
 VM_FAULT_MAJOR = ( vm_fault_t)0x000004,
 VM_FAULT_HWPOISON = ( vm_fault_t)0x000010,
 VM_FAULT_HWPOISON_LARGE = ( vm_fault_t)0x000020,
 VM_FAULT_SIGSEGV = ( vm_fault_t)0x000040,
 VM_FAULT_NOPAGE = ( vm_fault_t)0x000100,
 VM_FAULT_LOCKED = ( vm_fault_t)0x000200,
 VM_FAULT_RETRY = ( vm_fault_t)0x000400,
 VM_FAULT_FALLBACK = ( vm_fault_t)0x000800,
 VM_FAULT_DONE_COW = ( vm_fault_t)0x001000,
 VM_FAULT_NEEDDSYNC = ( vm_fault_t)0x002000,
 VM_FAULT_COMPLETED = ( vm_fault_t)0x004000,
 VM_FAULT_HINDEX_MASK = ( vm_fault_t)0x0f0000,
};
struct vm_special_mapping {
 const char *name;







 struct page **pages;





 vm_fault_t (*fault)(const struct vm_special_mapping *sm,
    struct vm_area_struct *vma,
    struct vm_fault *vmf);

 int (*mremap)(const struct vm_special_mapping *sm,
       struct vm_area_struct *new_vma);
};

enum tlb_flush_reason {
 TLB_FLUSH_ON_TASK_SWITCH,
 TLB_REMOTE_SHOOTDOWN,
 TLB_LOCAL_SHOOTDOWN,
 TLB_LOCAL_MM_SHOOTDOWN,
 TLB_REMOTE_SEND_IPI,
 NR_TLB_FLUSH_REASONS,
};





typedef struct {
 unsigned long val;
} swp_entry_t;
enum fault_flag {
 FAULT_FLAG_WRITE = 1 << 0,
 FAULT_FLAG_MKWRITE = 1 << 1,
 FAULT_FLAG_ALLOW_RETRY = 1 << 2,
 FAULT_FLAG_RETRY_NOWAIT = 1 << 3,
 FAULT_FLAG_KILLABLE = 1 << 4,
 FAULT_FLAG_TRIED = 1 << 5,
 FAULT_FLAG_USER = 1 << 6,
 FAULT_FLAG_REMOTE = 1 << 7,
 FAULT_FLAG_INSTRUCTION = 1 << 8,
 FAULT_FLAG_INTERRUPTIBLE = 1 << 9,
 FAULT_FLAG_UNSHARE = 1 << 10,
 FAULT_FLAG_ORIG_PTE_VALID = 1 << 11,
 FAULT_FLAG_VMA_LOCK = 1 << 12,
};

typedef unsigned int zap_flags_t;
enum {

 FOLL_WRITE = 1 << 0,

 FOLL_GET = 1 << 1,

 FOLL_DUMP = 1 << 2,

 FOLL_FORCE = 1 << 3,




 FOLL_NOWAIT = 1 << 4,

 FOLL_NOFAULT = 1 << 5,

 FOLL_HWPOISON = 1 << 6,

 FOLL_ANON = 1 << 7,





 FOLL_LONGTERM = 1 << 8,

 FOLL_SPLIT_PMD = 1 << 9,

 FOLL_PCI_P2PDMA = 1 << 10,

 FOLL_INTERRUPTIBLE = 1 << 11,


};
enum pageflags {
 PG_locked,
 PG_referenced,
 PG_uptodate,
 PG_dirty,
 PG_lru,
 PG_active,
 PG_workingset,
 PG_waiters,
 PG_error,
 PG_slab,
 PG_owner_priv_1,
 PG_arch_1,
 PG_reserved,
 PG_private,
 PG_private_2,
 PG_writeback,
 PG_head,
 PG_mappedtodisk,
 PG_reclaim,
 PG_swapbacked,
 PG_unevictable,

 PG_mlocked,
 PG_young,
 PG_idle,





 __NR_PAGEFLAGS,

 PG_readahead = PG_reclaim,
 PG_anon_exclusive = PG_mappedtodisk,


 PG_checked = PG_owner_priv_1,


 PG_swapcache = PG_owner_priv_1,





 PG_fscache = PG_private_2,



 PG_pinned = PG_owner_priv_1,

 PG_savepinned = PG_dirty,

 PG_foreign = PG_owner_priv_1,

 PG_xen_remapped = PG_owner_priv_1,
 PG_isolated = PG_reclaim,


 PG_reported = PG_uptodate,



 PG_vmemmap_self_hosted = PG_owner_priv_1,

};






extern struct static_key_false hugetlb_optimize_vmemmap_key;





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) const struct page *page_fixed_fake_head(const struct page *page)
{
 if (!({ bool branch; if (__builtin_types_compatible_p(typeof(*&hugetlb_optimize_vmemmap_key), struct static_key_true)) branch = arch_static_branch_jump(&(&hugetlb_optimize_vmemmap_key)->key, false); else if (__builtin_types_compatible_p(typeof(*&hugetlb_optimize_vmemmap_key), struct static_key_false)) branch = arch_static_branch(&(&hugetlb_optimize_vmemmap_key)->key, false); else branch = ____wrong_branch_error(); __builtin_expect(!!(branch), 0); }))
  return page;







 if (((((unsigned long)page) & ((typeof((unsigned long)page))(((1UL) << 14)) - 1)) == 0) &&
     ((__builtin_constant_p(PG_head) && __builtin_constant_p((uintptr_t)(&page->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&page->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&page->flags))) ? const_test_bit(PG_head, &page->flags) : generic_test_bit(PG_head, &page->flags))) {





  unsigned long head = ({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_96(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(page[1].compound_head) == sizeof(char) || sizeof(page[1].compound_head) == sizeof(short) || sizeof(page[1].compound_head) == sizeof(int) || sizeof(page[1].compound_head) == sizeof(long)) || sizeof(page[1].compound_head) == sizeof(long long))) __compiletime_assert_96(); } while (0); (*(const volatile typeof( _Generic((page[1].compound_head), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (page[1].compound_head))) *)&(page[1].compound_head)); });

  if (__builtin_expect(!!(head & 1), 1))
   return (const struct page *)(head - 1);
 }
 return page;
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int page_is_fake_head(struct page *page)
{
 return page_fixed_fake_head(page) != page;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long _compound_head(const struct page *page)
{
 unsigned long head = ({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_97(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(page->compound_head) == sizeof(char) || sizeof(page->compound_head) == sizeof(short) || sizeof(page->compound_head) == sizeof(int) || sizeof(page->compound_head) == sizeof(long)) || sizeof(page->compound_head) == sizeof(long long))) __compiletime_assert_97(); } while (0); (*(const volatile typeof( _Generic((page->compound_head), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (page->compound_head))) *)&(page->compound_head)); });

 if (__builtin_expect(!!(head & 1), 0))
  return head - 1;
 return (unsigned long)page_fixed_fake_head(page);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageTail(struct page *page)
{
 return ({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_98(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(page->compound_head) == sizeof(char) || sizeof(page->compound_head) == sizeof(short) || sizeof(page->compound_head) == sizeof(int) || sizeof(page->compound_head) == sizeof(long)) || sizeof(page->compound_head) == sizeof(long long))) __compiletime_assert_98(); } while (0); (*(const volatile typeof( _Generic((page->compound_head), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (page->compound_head))) *)&(page->compound_head)); }) & 1 || page_is_fake_head(page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageCompound(struct page *page)
{
 return ((__builtin_constant_p(PG_head) && __builtin_constant_p((uintptr_t)(&page->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&page->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&page->flags))) ? const_test_bit(PG_head, &page->flags) : generic_test_bit(PG_head, &page->flags)) ||
        ({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_99(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(page->compound_head) == sizeof(char) || sizeof(page->compound_head) == sizeof(short) || sizeof(page->compound_head) == sizeof(int) || sizeof(page->compound_head) == sizeof(long)) || sizeof(page->compound_head) == sizeof(long long))) __compiletime_assert_99(); } while (0); (*(const volatile typeof( _Generic((page->compound_head), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (page->compound_head))) *)&(page->compound_head)); }) & 1;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int PagePoisoned(const struct page *page)
{
 return ({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_100(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(page->flags) == sizeof(char) || sizeof(page->flags) == sizeof(short) || sizeof(page->flags) == sizeof(int) || sizeof(page->flags) == sizeof(long)) || sizeof(page->flags) == sizeof(long long))) __compiletime_assert_100(); } while (0); (*(const volatile typeof( _Generic((page->flags), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (page->flags))) *)&(page->flags)); }) == -1l;
}


void page_init_poison(struct page *page, size_t size);






static unsigned long *folio_flags(struct folio *folio, unsigned n)
{
 struct page *page = &folio->page;

 do { if (__builtin_expect(!!(PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 311; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0);
 do { if (__builtin_expect(!!(n > 0 && !((__builtin_constant_p(PG_head) && __builtin_constant_p((uintptr_t)(&page->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&page->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&page->flags))) ? const_test_bit(PG_head, &page->flags) : generic_test_bit(PG_head, &page->flags))), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "n > 0 && !((__builtin_constant_p(PG_head) && __builtin_constant_p((uintptr_t)(&page->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&page->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&page->flags))) ? const_test_bit(PG_head, &page->flags) : generic_test_bit(PG_head, &page->flags))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 312; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0);
 return &page[n].flags;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_locked(struct folio *folio) { return ((__builtin_constant_p(PG_locked) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_locked, folio_flags(folio, 0)) : generic_test_bit(PG_locked, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageLocked(struct page *page) { return ((__builtin_constant_p(PG_locked) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 465; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 465; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 465; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 465; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 465; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 465; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags))) ? const_test_bit(PG_locked, &({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 465; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 465; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) : generic_test_bit(PG_locked, &({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 465; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 465; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __folio_set_locked(struct folio *folio) { ((__builtin_constant_p(PG_locked) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___set_bit(PG_locked, folio_flags(folio, 0)) : generic___set_bit(PG_locked, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __SetPageLocked(struct page *page) { ((__builtin_constant_p(PG_locked) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 465; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 465; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 465; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 465; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 465; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 465; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags))) ? generic___set_bit(PG_locked, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 465; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 465; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) : generic___set_bit(PG_locked, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 465; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 465; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __folio_clear_locked(struct folio *folio) { ((__builtin_constant_p(PG_locked) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___clear_bit(PG_locked, folio_flags(folio, 0)) : generic___clear_bit(PG_locked, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __ClearPageLocked(struct page *page) { ((__builtin_constant_p(PG_locked) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 465; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 465; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 465; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 465; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 465; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 465; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags))) ? generic___clear_bit(PG_locked, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 465; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 465; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) : generic___clear_bit(PG_locked, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 465; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 465; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags)); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_waiters(struct folio *folio) { return ((__builtin_constant_p(PG_waiters) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_waiters, folio_flags(folio, 0)) : generic_test_bit(PG_waiters, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageWaiters(struct page *page) { return ((__builtin_constant_p(PG_waiters) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 466; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 466; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 466; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 466; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 466; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 466; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags))) ? const_test_bit(PG_waiters, &({ do { if (__builtin_expect(!!(PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 466; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 466; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) : generic_test_bit(PG_waiters, &({ do { if (__builtin_expect(!!(PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 466; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 466; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_set_waiters(struct folio *folio) { set_bit(PG_waiters, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void SetPageWaiters(struct page *page) { set_bit(PG_waiters, &({ do { if (__builtin_expect(!!(PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 466; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 466; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_clear_waiters(struct folio *folio) { clear_bit(PG_waiters, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void ClearPageWaiters(struct page *page) { clear_bit(PG_waiters, &({ do { if (__builtin_expect(!!(PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 466; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 466; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_error(struct folio *folio) { return ((__builtin_constant_p(PG_error) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_error, folio_flags(folio, 0)) : generic_test_bit(PG_error, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageError(struct page *page) { return ((__builtin_constant_p(PG_error) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 467; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 467; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 467; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 467; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 467; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 467; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags))) ? const_test_bit(PG_error, &({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 467; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 467; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) : generic_test_bit(PG_error, &({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 467; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 467; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_set_error(struct folio *folio) { set_bit(PG_error, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void SetPageError(struct page *page) { set_bit(PG_error, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 467; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 467; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_clear_error(struct folio *folio) { clear_bit(PG_error, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void ClearPageError(struct page *page) { clear_bit(PG_error, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 467; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 467; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_clear_error(struct folio *folio) { return test_and_clear_bit(PG_error, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int TestClearPageError(struct page *page) { return test_and_clear_bit(PG_error, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 467; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 467; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_referenced(struct folio *folio) { return ((__builtin_constant_p(PG_referenced) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_referenced, folio_flags(folio, 0)) : generic_test_bit(PG_referenced, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageReferenced(struct page *page) { return ((__builtin_constant_p(PG_referenced) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 468; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 468; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 468; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags))) ? const_test_bit(PG_referenced, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 468; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) : generic_test_bit(PG_referenced, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 468; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_set_referenced(struct folio *folio) { set_bit(PG_referenced, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void SetPageReferenced(struct page *page) { set_bit(PG_referenced, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 468; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_clear_referenced(struct folio *folio) { clear_bit(PG_referenced, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void ClearPageReferenced(struct page *page) { clear_bit(PG_referenced, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 468; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_clear_referenced(struct folio *folio) { return test_and_clear_bit(PG_referenced, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int TestClearPageReferenced(struct page *page) { return test_and_clear_bit(PG_referenced, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 469; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __folio_set_referenced(struct folio *folio) { ((__builtin_constant_p(PG_referenced) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___set_bit(PG_referenced, folio_flags(folio, 0)) : generic___set_bit(PG_referenced, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __SetPageReferenced(struct page *page) { ((__builtin_constant_p(PG_referenced) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 470; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 470; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 470; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags))) ? generic___set_bit(PG_referenced, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 470; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) : generic___set_bit(PG_referenced, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 470; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags)); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_dirty(struct folio *folio) { return ((__builtin_constant_p(PG_dirty) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_dirty, folio_flags(folio, 0)) : generic_test_bit(PG_dirty, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageDirty(struct page *page) { return ((__builtin_constant_p(PG_dirty) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 471; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 471; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 471; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags))) ? const_test_bit(PG_dirty, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 471; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) : generic_test_bit(PG_dirty, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 471; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_set_dirty(struct folio *folio) { set_bit(PG_dirty, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void SetPageDirty(struct page *page) { set_bit(PG_dirty, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 471; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_clear_dirty(struct folio *folio) { clear_bit(PG_dirty, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void ClearPageDirty(struct page *page) { clear_bit(PG_dirty, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 471; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_set_dirty(struct folio *folio) { return test_and_set_bit(PG_dirty, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int TestSetPageDirty(struct page *page) { return test_and_set_bit(PG_dirty, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 471; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_clear_dirty(struct folio *folio) { return test_and_clear_bit(PG_dirty, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int TestClearPageDirty(struct page *page) { return test_and_clear_bit(PG_dirty, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 471; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __folio_clear_dirty(struct folio *folio) { ((__builtin_constant_p(PG_dirty) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___clear_bit(PG_dirty, folio_flags(folio, 0)) : generic___clear_bit(PG_dirty, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __ClearPageDirty(struct page *page) { ((__builtin_constant_p(PG_dirty) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 472; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 472; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 472; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags))) ? generic___clear_bit(PG_dirty, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 472; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) : generic___clear_bit(PG_dirty, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 472; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags)); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_lru(struct folio *folio) { return ((__builtin_constant_p(PG_lru) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_lru, folio_flags(folio, 0)) : generic_test_bit(PG_lru, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageLRU(struct page *page) { return ((__builtin_constant_p(PG_lru) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 473; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 473; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 473; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags))) ? const_test_bit(PG_lru, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 473; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) : generic_test_bit(PG_lru, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 473; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_set_lru(struct folio *folio) { set_bit(PG_lru, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void SetPageLRU(struct page *page) { set_bit(PG_lru, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 473; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_clear_lru(struct folio *folio) { clear_bit(PG_lru, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void ClearPageLRU(struct page *page) { clear_bit(PG_lru, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 473; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __folio_clear_lru(struct folio *folio) { ((__builtin_constant_p(PG_lru) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___clear_bit(PG_lru, folio_flags(folio, 0)) : generic___clear_bit(PG_lru, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __ClearPageLRU(struct page *page) { ((__builtin_constant_p(PG_lru) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 473; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 473; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 473; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags))) ? generic___clear_bit(PG_lru, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 473; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) : generic___clear_bit(PG_lru, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 473; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags)); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_clear_lru(struct folio *folio) { return test_and_clear_bit(PG_lru, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int TestClearPageLRU(struct page *page) { return test_and_clear_bit(PG_lru, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 474; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_active(struct folio *folio) { return ((__builtin_constant_p(PG_active) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_active, folio_flags(folio, 0)) : generic_test_bit(PG_active, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageActive(struct page *page) { return ((__builtin_constant_p(PG_active) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 475; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 475; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 475; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags))) ? const_test_bit(PG_active, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 475; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) : generic_test_bit(PG_active, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 475; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_set_active(struct folio *folio) { set_bit(PG_active, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void SetPageActive(struct page *page) { set_bit(PG_active, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 475; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_clear_active(struct folio *folio) { clear_bit(PG_active, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void ClearPageActive(struct page *page) { clear_bit(PG_active, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 475; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __folio_clear_active(struct folio *folio) { ((__builtin_constant_p(PG_active) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___clear_bit(PG_active, folio_flags(folio, 0)) : generic___clear_bit(PG_active, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __ClearPageActive(struct page *page) { ((__builtin_constant_p(PG_active) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 475; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 475; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 475; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags))) ? generic___clear_bit(PG_active, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 475; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) : generic___clear_bit(PG_active, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 475; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags)); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_clear_active(struct folio *folio) { return test_and_clear_bit(PG_active, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int TestClearPageActive(struct page *page) { return test_and_clear_bit(PG_active, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 476; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_workingset(struct folio *folio) { return ((__builtin_constant_p(PG_workingset) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_workingset, folio_flags(folio, 0)) : generic_test_bit(PG_workingset, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageWorkingset(struct page *page) { return ((__builtin_constant_p(PG_workingset) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 477; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 477; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 477; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags))) ? const_test_bit(PG_workingset, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 477; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) : generic_test_bit(PG_workingset, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 477; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_set_workingset(struct folio *folio) { set_bit(PG_workingset, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void SetPageWorkingset(struct page *page) { set_bit(PG_workingset, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 477; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_clear_workingset(struct folio *folio) { clear_bit(PG_workingset, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void ClearPageWorkingset(struct page *page) { clear_bit(PG_workingset, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 477; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_clear_workingset(struct folio *folio) { return test_and_clear_bit(PG_workingset, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int TestClearPageWorkingset(struct page *page) { return test_and_clear_bit(PG_workingset, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 478; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_slab(struct folio *folio) { return ((__builtin_constant_p(PG_slab) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_slab, folio_flags(folio, 0)) : generic_test_bit(PG_slab, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageSlab(struct page *page) { return ((__builtin_constant_p(PG_slab) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 479; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 479; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 479; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 479; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 479; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 479; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags))) ? const_test_bit(PG_slab, &({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 479; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 479; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) : generic_test_bit(PG_slab, &({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 479; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 479; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __folio_set_slab(struct folio *folio) { ((__builtin_constant_p(PG_slab) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___set_bit(PG_slab, folio_flags(folio, 0)) : generic___set_bit(PG_slab, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __SetPageSlab(struct page *page) { ((__builtin_constant_p(PG_slab) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 479; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 479; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 479; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 479; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 479; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 479; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags))) ? generic___set_bit(PG_slab, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 479; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 479; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) : generic___set_bit(PG_slab, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 479; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 479; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __folio_clear_slab(struct folio *folio) { ((__builtin_constant_p(PG_slab) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___clear_bit(PG_slab, folio_flags(folio, 0)) : generic___clear_bit(PG_slab, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __ClearPageSlab(struct page *page) { ((__builtin_constant_p(PG_slab) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 479; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 479; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 479; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 479; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 479; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 479; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags))) ? generic___clear_bit(PG_slab, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 479; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 479; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) : generic___clear_bit(PG_slab, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 479; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 479; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags)); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_checked(struct folio *folio) { return ((__builtin_constant_p(PG_checked) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_checked, folio_flags(folio, 0)) : generic_test_bit(PG_checked, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageChecked(struct page *page) { return ((__builtin_constant_p(PG_checked) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 480; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 480; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 480; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 480; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 480; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 480; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags))) ? const_test_bit(PG_checked, &({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 480; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 480; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) : generic_test_bit(PG_checked, &({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 480; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 480; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_set_checked(struct folio *folio) { set_bit(PG_checked, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void SetPageChecked(struct page *page) { set_bit(PG_checked, &({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 480; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 480; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_clear_checked(struct folio *folio) { clear_bit(PG_checked, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void ClearPageChecked(struct page *page) { clear_bit(PG_checked, &({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 480; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 480; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags); }


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_pinned(struct folio *folio) { return ((__builtin_constant_p(PG_pinned) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_pinned, folio_flags(folio, 0)) : generic_test_bit(PG_pinned, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PagePinned(struct page *page) { return ((__builtin_constant_p(PG_pinned) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 483; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 483; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 483; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 483; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 483; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 483; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags))) ? const_test_bit(PG_pinned, &({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 483; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 483; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) : generic_test_bit(PG_pinned, &({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 483; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 483; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_set_pinned(struct folio *folio) { set_bit(PG_pinned, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void SetPagePinned(struct page *page) { set_bit(PG_pinned, &({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 483; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 483; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_clear_pinned(struct folio *folio) { clear_bit(PG_pinned, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void ClearPagePinned(struct page *page) { clear_bit(PG_pinned, &({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 483; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 483; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_set_pinned(struct folio *folio) { return test_and_set_bit(PG_pinned, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int TestSetPagePinned(struct page *page) { return test_and_set_bit(PG_pinned, &({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 484; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 484; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_clear_pinned(struct folio *folio) { return test_and_clear_bit(PG_pinned, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int TestClearPagePinned(struct page *page) { return test_and_clear_bit(PG_pinned, &({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 484; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 484; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_savepinned(struct folio *folio) { return ((__builtin_constant_p(PG_savepinned) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_savepinned, folio_flags(folio, 0)) : generic_test_bit(PG_savepinned, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageSavePinned(struct page *page) { return ((__builtin_constant_p(PG_savepinned) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 485; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 485; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 485; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 485; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 485; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 485; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags))) ? const_test_bit(PG_savepinned, &({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 485; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 485; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) : generic_test_bit(PG_savepinned, &({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 485; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 485; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_set_savepinned(struct folio *folio) { set_bit(PG_savepinned, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void SetPageSavePinned(struct page *page) { set_bit(PG_savepinned, &({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 485; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 485; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_clear_savepinned(struct folio *folio) { clear_bit(PG_savepinned, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void ClearPageSavePinned(struct page *page) { clear_bit(PG_savepinned, &({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 485; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 485; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags); };
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_foreign(struct folio *folio) { return ((__builtin_constant_p(PG_foreign) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_foreign, folio_flags(folio, 0)) : generic_test_bit(PG_foreign, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageForeign(struct page *page) { return ((__builtin_constant_p(PG_foreign) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 486; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 486; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 486; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 486; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 486; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 486; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags))) ? const_test_bit(PG_foreign, &({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 486; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 486; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) : generic_test_bit(PG_foreign, &({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 486; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 486; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_set_foreign(struct folio *folio) { set_bit(PG_foreign, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void SetPageForeign(struct page *page) { set_bit(PG_foreign, &({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 486; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 486; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_clear_foreign(struct folio *folio) { clear_bit(PG_foreign, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void ClearPageForeign(struct page *page) { clear_bit(PG_foreign, &({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 486; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 486; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags); };
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_xen_remapped(struct folio *folio) { return ((__builtin_constant_p(PG_xen_remapped) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_xen_remapped, folio_flags(folio, 0)) : generic_test_bit(PG_xen_remapped, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageXenRemapped(struct page *page) { return ((__builtin_constant_p(PG_xen_remapped) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 487; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 487; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 487; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 487; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 487; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 487; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags))) ? const_test_bit(PG_xen_remapped, &({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 487; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 487; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) : generic_test_bit(PG_xen_remapped, &({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 487; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 487; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_set_xen_remapped(struct folio *folio) { set_bit(PG_xen_remapped, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void SetPageXenRemapped(struct page *page) { set_bit(PG_xen_remapped, &({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 487; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 487; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_clear_xen_remapped(struct folio *folio) { clear_bit(PG_xen_remapped, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void ClearPageXenRemapped(struct page *page) { clear_bit(PG_xen_remapped, &({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 487; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 487; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_clear_xen_remapped(struct folio *folio) { return test_and_clear_bit(PG_xen_remapped, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int TestClearPageXenRemapped(struct page *page) { return test_and_clear_bit(PG_xen_remapped, &({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 488; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 488; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags); }

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_reserved(struct folio *folio) { return ((__builtin_constant_p(PG_reserved) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_reserved, folio_flags(folio, 0)) : generic_test_bit(PG_reserved, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageReserved(struct page *page) { return ((__builtin_constant_p(PG_reserved) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 490; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 490; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 490; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 490; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 490; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 490; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags))) ? const_test_bit(PG_reserved, &({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 490; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 490; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) : generic_test_bit(PG_reserved, &({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 490; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 490; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_set_reserved(struct folio *folio) { set_bit(PG_reserved, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void SetPageReserved(struct page *page) { set_bit(PG_reserved, &({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 490; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 490; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_clear_reserved(struct folio *folio) { clear_bit(PG_reserved, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void ClearPageReserved(struct page *page) { clear_bit(PG_reserved, &({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 490; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 490; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __folio_clear_reserved(struct folio *folio) { ((__builtin_constant_p(PG_reserved) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___clear_bit(PG_reserved, folio_flags(folio, 0)) : generic___clear_bit(PG_reserved, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __ClearPageReserved(struct page *page) { ((__builtin_constant_p(PG_reserved) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 491; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 491; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 491; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 491; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 491; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 491; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags))) ? generic___clear_bit(PG_reserved, &({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 491; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 491; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) : generic___clear_bit(PG_reserved, &({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 491; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 491; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags)); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __folio_set_reserved(struct folio *folio) { ((__builtin_constant_p(PG_reserved) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___set_bit(PG_reserved, folio_flags(folio, 0)) : generic___set_bit(PG_reserved, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __SetPageReserved(struct page *page) { ((__builtin_constant_p(PG_reserved) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 492; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 492; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 492; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 492; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 492; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 492; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags))) ? generic___set_bit(PG_reserved, &({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 492; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 492; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) : generic___set_bit(PG_reserved, &({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 492; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 492; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags)); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_swapbacked(struct folio *folio) { return ((__builtin_constant_p(PG_swapbacked) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_swapbacked, folio_flags(folio, 0)) : generic_test_bit(PG_swapbacked, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageSwapBacked(struct page *page) { return ((__builtin_constant_p(PG_swapbacked) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 493; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 493; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 493; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 493; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 493; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 493; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags))) ? const_test_bit(PG_swapbacked, &({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 493; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 493; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) : generic_test_bit(PG_swapbacked, &({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 493; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 493; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_set_swapbacked(struct folio *folio) { set_bit(PG_swapbacked, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void SetPageSwapBacked(struct page *page) { set_bit(PG_swapbacked, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 493; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 493; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_clear_swapbacked(struct folio *folio) { clear_bit(PG_swapbacked, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void ClearPageSwapBacked(struct page *page) { clear_bit(PG_swapbacked, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 493; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 493; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __folio_clear_swapbacked(struct folio *folio) { ((__builtin_constant_p(PG_swapbacked) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___clear_bit(PG_swapbacked, folio_flags(folio, 0)) : generic___clear_bit(PG_swapbacked, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __ClearPageSwapBacked(struct page *page) { ((__builtin_constant_p(PG_swapbacked) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 494; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 494; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 494; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 494; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 494; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 494; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags))) ? generic___clear_bit(PG_swapbacked, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 494; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 494; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) : generic___clear_bit(PG_swapbacked, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 494; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 494; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags)); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __folio_set_swapbacked(struct folio *folio) { ((__builtin_constant_p(PG_swapbacked) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___set_bit(PG_swapbacked, folio_flags(folio, 0)) : generic___set_bit(PG_swapbacked, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __SetPageSwapBacked(struct page *page) { ((__builtin_constant_p(PG_swapbacked) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 495; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 495; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 495; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 495; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 495; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 495; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags))) ? generic___set_bit(PG_swapbacked, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 495; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 495; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) : generic___set_bit(PG_swapbacked, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 495; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 495; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags)); }






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_private(struct folio *folio) { return ((__builtin_constant_p(PG_private) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_private, folio_flags(folio, 0)) : generic_test_bit(PG_private, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PagePrivate(struct page *page) { return ((__builtin_constant_p(PG_private) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 502; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 502; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 502; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags))) ? const_test_bit(PG_private, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 502; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) : generic_test_bit(PG_private, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 502; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_set_private(struct folio *folio) { set_bit(PG_private, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void SetPagePrivate(struct page *page) { set_bit(PG_private, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 502; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_clear_private(struct folio *folio) { clear_bit(PG_private, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void ClearPagePrivate(struct page *page) { clear_bit(PG_private, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 502; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_private_2(struct folio *folio) { return ((__builtin_constant_p(PG_private_2) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_private_2, folio_flags(folio, 0)) : generic_test_bit(PG_private_2, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PagePrivate2(struct page *page) { return ((__builtin_constant_p(PG_private_2) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 503; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 503; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 503; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags))) ? const_test_bit(PG_private_2, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 503; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) : generic_test_bit(PG_private_2, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 503; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_set_private_2(struct folio *folio) { set_bit(PG_private_2, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void SetPagePrivate2(struct page *page) { set_bit(PG_private_2, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 503; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_clear_private_2(struct folio *folio) { clear_bit(PG_private_2, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void ClearPagePrivate2(struct page *page) { clear_bit(PG_private_2, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 503; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_set_private_2(struct folio *folio) { return test_and_set_bit(PG_private_2, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int TestSetPagePrivate2(struct page *page) { return test_and_set_bit(PG_private_2, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 503; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_clear_private_2(struct folio *folio) { return test_and_clear_bit(PG_private_2, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int TestClearPagePrivate2(struct page *page) { return test_and_clear_bit(PG_private_2, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 503; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_owner_priv_1(struct folio *folio) { return ((__builtin_constant_p(PG_owner_priv_1) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_owner_priv_1, folio_flags(folio, 0)) : generic_test_bit(PG_owner_priv_1, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageOwnerPriv1(struct page *page) { return ((__builtin_constant_p(PG_owner_priv_1) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 504; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 504; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 504; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags))) ? const_test_bit(PG_owner_priv_1, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 504; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) : generic_test_bit(PG_owner_priv_1, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 504; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_set_owner_priv_1(struct folio *folio) { set_bit(PG_owner_priv_1, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void SetPageOwnerPriv1(struct page *page) { set_bit(PG_owner_priv_1, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 504; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_clear_owner_priv_1(struct folio *folio) { clear_bit(PG_owner_priv_1, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void ClearPageOwnerPriv1(struct page *page) { clear_bit(PG_owner_priv_1, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 504; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_clear_owner_priv_1(struct folio *folio) { return test_and_clear_bit(PG_owner_priv_1, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int TestClearPageOwnerPriv1(struct page *page) { return test_and_clear_bit(PG_owner_priv_1, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 505; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags); }





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_writeback(struct folio *folio) { return ((__builtin_constant_p(PG_writeback) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_writeback, folio_flags(folio, 0)) : generic_test_bit(PG_writeback, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageWriteback(struct page *page) { return ((__builtin_constant_p(PG_writeback) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 511; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 511; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 511; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 511; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 511; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 511; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags))) ? const_test_bit(PG_writeback, &({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 511; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 511; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) : generic_test_bit(PG_writeback, &({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 511; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 511; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags)); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_set_writeback(struct folio *folio) { return test_and_set_bit(PG_writeback, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int TestSetPageWriteback(struct page *page) { return test_and_set_bit(PG_writeback, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 512; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 512; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_clear_writeback(struct folio *folio) { return test_and_clear_bit(PG_writeback, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int TestClearPageWriteback(struct page *page) { return test_and_clear_bit(PG_writeback, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 512; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 512; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_mappedtodisk(struct folio *folio) { return ((__builtin_constant_p(PG_mappedtodisk) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_mappedtodisk, folio_flags(folio, 0)) : generic_test_bit(PG_mappedtodisk, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageMappedToDisk(struct page *page) { return ((__builtin_constant_p(PG_mappedtodisk) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 513; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 513; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 513; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 513; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 513; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 513; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags))) ? const_test_bit(PG_mappedtodisk, &({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 513; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 513; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) : generic_test_bit(PG_mappedtodisk, &({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 513; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 513; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_set_mappedtodisk(struct folio *folio) { set_bit(PG_mappedtodisk, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void SetPageMappedToDisk(struct page *page) { set_bit(PG_mappedtodisk, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 513; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 513; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_clear_mappedtodisk(struct folio *folio) { clear_bit(PG_mappedtodisk, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void ClearPageMappedToDisk(struct page *page) { clear_bit(PG_mappedtodisk, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 513; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 513; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags); }


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_reclaim(struct folio *folio) { return ((__builtin_constant_p(PG_reclaim) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_reclaim, folio_flags(folio, 0)) : generic_test_bit(PG_reclaim, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageReclaim(struct page *page) { return ((__builtin_constant_p(PG_reclaim) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 516; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 516; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 516; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 516; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 516; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 516; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags))) ? const_test_bit(PG_reclaim, &({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 516; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 516; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) : generic_test_bit(PG_reclaim, &({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 516; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 516; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_set_reclaim(struct folio *folio) { set_bit(PG_reclaim, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void SetPageReclaim(struct page *page) { set_bit(PG_reclaim, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 516; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 516; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_clear_reclaim(struct folio *folio) { clear_bit(PG_reclaim, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void ClearPageReclaim(struct page *page) { clear_bit(PG_reclaim, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 516; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 516; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_clear_reclaim(struct folio *folio) { return test_and_clear_bit(PG_reclaim, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int TestClearPageReclaim(struct page *page) { return test_and_clear_bit(PG_reclaim, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 517; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 517; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_readahead(struct folio *folio) { return ((__builtin_constant_p(PG_readahead) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_readahead, folio_flags(folio, 0)) : generic_test_bit(PG_readahead, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageReadahead(struct page *page) { return ((__builtin_constant_p(PG_readahead) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 518; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 518; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 518; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 518; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 518; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 518; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags))) ? const_test_bit(PG_readahead, &({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 518; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 518; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) : generic_test_bit(PG_readahead, &({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 518; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 518; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_set_readahead(struct folio *folio) { set_bit(PG_readahead, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void SetPageReadahead(struct page *page) { set_bit(PG_readahead, &({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 518; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 518; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_clear_readahead(struct folio *folio) { clear_bit(PG_readahead, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void ClearPageReadahead(struct page *page) { clear_bit(PG_readahead, &({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 518; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 518; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_clear_readahead(struct folio *folio) { return test_and_clear_bit(PG_readahead, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int TestClearPageReadahead(struct page *page) { return test_and_clear_bit(PG_readahead, &({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 519; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 519; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool folio_test_highmem(const struct folio *folio) { return false; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int PageHighMem(const struct page *page) { return 0; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void folio_set_highmem(struct folio *folio) { } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void SetPageHighMem(struct page *page) { } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void folio_clear_highmem(struct folio *folio) { } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void ClearPageHighMem(struct page *page) { }



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_swapcache(struct folio *folio)
{
 return folio_test_swapbacked(folio) &&
   ((__builtin_constant_p(PG_swapcache) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_swapcache, folio_flags(folio, 0)) : generic_test_bit(PG_swapcache, folio_flags(folio, 0)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool PageSwapCache(struct page *page)
{
 return folio_test_swapcache((_Generic((page), const struct page *: (const struct folio *)_compound_head(page), struct page *: (struct folio *)_compound_head(page))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_set_swapcache(struct folio *folio) { set_bit(PG_swapcache, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void SetPageSwapCache(struct page *page) { set_bit(PG_swapcache, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 544; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 544; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_clear_swapcache(struct folio *folio) { clear_bit(PG_swapcache, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void ClearPageSwapCache(struct page *page) { clear_bit(PG_swapcache, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 545; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 545; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags); }




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_unevictable(struct folio *folio) { return ((__builtin_constant_p(PG_unevictable) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_unevictable, folio_flags(folio, 0)) : generic_test_bit(PG_unevictable, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageUnevictable(struct page *page) { return ((__builtin_constant_p(PG_unevictable) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 550; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 550; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 550; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags))) ? const_test_bit(PG_unevictable, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 550; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) : generic_test_bit(PG_unevictable, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 550; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_set_unevictable(struct folio *folio) { set_bit(PG_unevictable, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void SetPageUnevictable(struct page *page) { set_bit(PG_unevictable, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 550; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_clear_unevictable(struct folio *folio) { clear_bit(PG_unevictable, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void ClearPageUnevictable(struct page *page) { clear_bit(PG_unevictable, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 550; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __folio_clear_unevictable(struct folio *folio) { ((__builtin_constant_p(PG_unevictable) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___clear_bit(PG_unevictable, folio_flags(folio, 0)) : generic___clear_bit(PG_unevictable, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __ClearPageUnevictable(struct page *page) { ((__builtin_constant_p(PG_unevictable) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 551; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 551; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 551; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags))) ? generic___clear_bit(PG_unevictable, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 551; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags) : generic___clear_bit(PG_unevictable, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 551; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags)); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_clear_unevictable(struct folio *folio) { return test_and_clear_bit(PG_unevictable, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int TestClearPageUnevictable(struct page *page) { return test_and_clear_bit(PG_unevictable, &({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 552; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); })->flags); }


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_mlocked(struct folio *folio) { return ((__builtin_constant_p(PG_mlocked) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_mlocked, folio_flags(folio, 0)) : generic_test_bit(PG_mlocked, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageMlocked(struct page *page) { return ((__builtin_constant_p(PG_mlocked) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 555; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 555; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 555; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 555; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 555; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 555; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags))) ? const_test_bit(PG_mlocked, &({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 555; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 555; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) : generic_test_bit(PG_mlocked, &({ do { if (__builtin_expect(!!(0 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 555; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 555; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_set_mlocked(struct folio *folio) { set_bit(PG_mlocked, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void SetPageMlocked(struct page *page) { set_bit(PG_mlocked, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 555; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 555; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_clear_mlocked(struct folio *folio) { clear_bit(PG_mlocked, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void ClearPageMlocked(struct page *page) { clear_bit(PG_mlocked, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 555; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 555; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __folio_clear_mlocked(struct folio *folio) { ((__builtin_constant_p(PG_mlocked) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___clear_bit(PG_mlocked, folio_flags(folio, 0)) : generic___clear_bit(PG_mlocked, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __ClearPageMlocked(struct page *page) { ((__builtin_constant_p(PG_mlocked) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 556; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 556; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 556; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 556; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 556; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 556; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags))) ? generic___clear_bit(PG_mlocked, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 556; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 556; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags) : generic___clear_bit(PG_mlocked, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 556; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 556; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags)); }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_set_mlocked(struct folio *folio) { return test_and_set_bit(PG_mlocked, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int TestSetPageMlocked(struct page *page) { return test_and_set_bit(PG_mlocked, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 557; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 557; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_clear_mlocked(struct folio *folio) { return test_and_clear_bit(PG_mlocked, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int TestClearPageMlocked(struct page *page) { return test_and_clear_bit(PG_mlocked, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 557; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 557; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool folio_test_uncached(const struct folio *folio) { return false; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int PageUncached(const struct page *page) { return 0; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void folio_set_uncached(struct folio *folio) { } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void SetPageUncached(struct page *page) { } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void folio_clear_uncached(struct folio *folio) { } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void ClearPageUncached(struct page *page) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool folio_test_hwpoison(const struct folio *folio) { return false; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int PageHWPoison(const struct page *page) { return 0; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void folio_set_hwpoison(struct folio *folio) { } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void SetPageHWPoison(struct page *page) { } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void folio_clear_hwpoison(struct folio *folio) { } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void ClearPageHWPoison(struct page *page) { }




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_young(struct folio *folio) { return ((__builtin_constant_p(PG_young) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_young, folio_flags(folio, 0)) : generic_test_bit(PG_young, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageYoung(struct page *page) { return ((__builtin_constant_p(PG_young) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 584; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 584; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 584; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags))) ? const_test_bit(PG_young, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 584; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) : generic_test_bit(PG_young, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 584; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags)); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_set_young(struct folio *folio) { set_bit(PG_young, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void SetPageYoung(struct page *page) { set_bit(PG_young, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 585; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_clear_young(struct folio *folio) { return test_and_clear_bit(PG_young, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int TestClearPageYoung(struct page *page) { return test_and_clear_bit(PG_young, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 586; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_idle(struct folio *folio) { return ((__builtin_constant_p(PG_idle) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_idle, folio_flags(folio, 0)) : generic_test_bit(PG_idle, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageIdle(struct page *page) { return ((__builtin_constant_p(PG_idle) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 587; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 587; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 587; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags))) ? const_test_bit(PG_idle, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 587; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) : generic_test_bit(PG_idle, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 587; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_set_idle(struct folio *folio) { set_bit(PG_idle, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void SetPageIdle(struct page *page) { set_bit(PG_idle, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 587; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_clear_idle(struct folio *folio) { clear_bit(PG_idle, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void ClearPageIdle(struct page *page) { clear_bit(PG_idle, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 587; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_reported(struct folio *folio) { return ((__builtin_constant_p(PG_reported) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_reported, folio_flags(folio, 0)) : generic_test_bit(PG_reported, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageReported(struct page *page) { return ((__builtin_constant_p(PG_reported) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 596; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 596; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 596; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 596; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 596; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 596; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags))) ? const_test_bit(PG_reported, &({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 596; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 596; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) : generic_test_bit(PG_reported, &({ do { if (__builtin_expect(!!(0 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "0 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 596; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 596; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __folio_set_reported(struct folio *folio) { ((__builtin_constant_p(PG_reported) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___set_bit(PG_reported, folio_flags(folio, 0)) : generic___set_bit(PG_reported, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __SetPageReported(struct page *page) { ((__builtin_constant_p(PG_reported) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 596; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 596; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 596; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 596; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 596; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 596; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags))) ? generic___set_bit(PG_reported, &({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 596; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 596; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) : generic___set_bit(PG_reported, &({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 596; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 596; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __folio_clear_reported(struct folio *folio) { ((__builtin_constant_p(PG_reported) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___clear_bit(PG_reported, folio_flags(folio, 0)) : generic___clear_bit(PG_reported, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __ClearPageReported(struct page *page) { ((__builtin_constant_p(PG_reported) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 596; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 596; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 596; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 596; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 596; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 596; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags))) ? generic___clear_bit(PG_reported, &({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 596; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 596; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags) : generic___clear_bit(PG_reported, &({ do { if (__builtin_expect(!!(1 && PageCompound(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageCompound(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 596; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 596; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; }); })->flags)); }


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_vmemmap_self_hosted(struct folio *folio) { return ((__builtin_constant_p(PG_vmemmap_self_hosted) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_vmemmap_self_hosted, folio_flags(folio, 0)) : generic_test_bit(PG_vmemmap_self_hosted, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageVmemmapSelfHosted(struct page *page) { return ((__builtin_constant_p(PG_vmemmap_self_hosted) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 599; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 599; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 599; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags))) ? const_test_bit(PG_vmemmap_self_hosted, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 599; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) : generic_test_bit(PG_vmemmap_self_hosted, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 599; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_set_vmemmap_self_hosted(struct folio *folio) { set_bit(PG_vmemmap_self_hosted, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void SetPageVmemmapSelfHosted(struct page *page) { set_bit(PG_vmemmap_self_hosted, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 599; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_clear_vmemmap_self_hosted(struct folio *folio) { clear_bit(PG_vmemmap_self_hosted, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void ClearPageVmemmapSelfHosted(struct page *page) { clear_bit(PG_vmemmap_self_hosted, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 599; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_mapping_flags(struct folio *folio)
{
 return ((unsigned long)folio->mapping & (0x1 | 0x2)) != 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageMappingFlags(struct page *page)
{
 return ((unsigned long)page->mapping & (0x1 | 0x2)) != 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_anon(struct folio *folio)
{
 return ((unsigned long)folio->mapping & 0x1) != 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool PageAnon(struct page *page)
{
 return folio_test_anon((_Generic((page), const struct page *: (const struct folio *)_compound_head(page), struct page *: (struct folio *)_compound_head(page))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool __folio_test_movable(const struct folio *folio)
{
 return ((unsigned long)folio->mapping & (0x1 | 0x2)) ==
   0x2;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int __PageMovable(struct page *page)
{
 return ((unsigned long)page->mapping & (0x1 | 0x2)) ==
    0x2;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_ksm(struct folio *folio)
{
 return ((unsigned long)folio->mapping & (0x1 | 0x2)) ==
    (0x1 | 0x2);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool PageKsm(struct page *page)
{
 return folio_test_ksm((_Generic((page), const struct page *: (const struct folio *)_compound_head(page), struct page *: (struct folio *)_compound_head(page))));
}




u64 stable_page_flags(struct page *page);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool folio_test_uptodate(struct folio *folio)
{
 bool ret = ((__builtin_constant_p(PG_uptodate) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_uptodate, folio_flags(folio, 0)) : generic_test_bit(PG_uptodate, folio_flags(folio, 0)));
 if (ret)
  do { do { } while (0); __asm__ __volatile__("dbar %0 " : : "I"(0b10101) : "memory"); } while (0);

 return ret;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int PageUptodate(struct page *page)
{
 return folio_test_uptodate((_Generic((page), const struct page *: (const struct folio *)_compound_head(page), struct page *: (struct folio *)_compound_head(page))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __folio_mark_uptodate(struct folio *folio)
{
 do { do { } while (0); __asm__ __volatile__("dbar %0 " : : "I"(0b11010) : "memory"); } while (0);
 ((__builtin_constant_p(PG_uptodate) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___set_bit(PG_uptodate, folio_flags(folio, 0)) : generic___set_bit(PG_uptodate, folio_flags(folio, 0)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_mark_uptodate(struct folio *folio)
{





 do { do { } while (0); __asm__ __volatile__("dbar %0 " : : "I"(0b11010) : "memory"); } while (0);
 set_bit(PG_uptodate, folio_flags(folio, 0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __SetPageUptodate(struct page *page)
{
 __folio_mark_uptodate((struct folio *)page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void SetPageUptodate(struct page *page)
{
 folio_mark_uptodate((struct folio *)page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_clear_uptodate(struct folio *folio) { clear_bit(PG_uptodate, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void ClearPageUptodate(struct page *page) { clear_bit(PG_uptodate, &({ do { if (__builtin_expect(!!(1 && PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "1 && PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 752; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ({ do { if (__builtin_expect(!!(PagePoisoned(((typeof(page))_compound_head(page)))), 0)) { dump_page(((typeof(page))_compound_head(page)), "VM_BUG_ON_PAGE(" "PagePoisoned(((typeof(page))_compound_head(page)))"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 752; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); ((typeof(page))_compound_head(page)); }); })->flags); }

bool __folio_start_writeback(struct folio *folio, bool keep_write);
bool set_page_writeback(struct page *page);






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool test_set_page_writeback(struct page *page)
{
 return set_page_writeback(page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_head(struct folio *folio)
{
 return ((__builtin_constant_p(PG_head) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_head, folio_flags(folio, 0)) : generic_test_bit(PG_head, folio_flags(folio, 0)));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageHead(struct page *page)
{
 ({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 774; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; });
 return ((__builtin_constant_p(PG_head) && __builtin_constant_p((uintptr_t)(&page->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&page->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&page->flags))) ? const_test_bit(PG_head, &page->flags) : generic_test_bit(PG_head, &page->flags)) && !page_is_fake_head(page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __folio_set_head(struct folio *folio) { ((__builtin_constant_p(PG_head) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___set_bit(PG_head, folio_flags(folio, 0)) : generic___set_bit(PG_head, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __SetPageHead(struct page *page) { ((__builtin_constant_p(PG_head) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 778; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 778; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 778; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags))) ? generic___set_bit(PG_head, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 778; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) : generic___set_bit(PG_head, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 778; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags)); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __folio_clear_head(struct folio *folio) { ((__builtin_constant_p(PG_head) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? generic___clear_bit(PG_head, folio_flags(folio, 0)) : generic___clear_bit(PG_head, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __ClearPageHead(struct page *page) { ((__builtin_constant_p(PG_head) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 779; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 779; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 779; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags))) ? generic___clear_bit(PG_head, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 779; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) : generic___clear_bit(PG_head, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 779; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags)); }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_clear_head(struct folio *folio) { clear_bit(PG_head, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void ClearPageHead(struct page *page) { clear_bit(PG_head, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 780; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags); }







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool folio_test_large(struct folio *folio)
{
 return folio_test_head(folio);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void set_compound_head(struct page *page, struct page *head)
{
 do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_101(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(page->compound_head) == sizeof(char) || sizeof(page->compound_head) == sizeof(short) || sizeof(page->compound_head) == sizeof(int) || sizeof(page->compound_head) == sizeof(long)) || sizeof(page->compound_head) == sizeof(long long))) __compiletime_assert_101(); } while (0); do { *(volatile typeof(page->compound_head) *)&(page->compound_head) = ((unsigned long)head + 1); } while (0); } while (0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void clear_compound_head(struct page *page)
{
 do { do { __attribute__((__noreturn__)) extern void __compiletime_assert_102(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(page->compound_head) == sizeof(char) || sizeof(page->compound_head) == sizeof(short) || sizeof(page->compound_head) == sizeof(int) || sizeof(page->compound_head) == sizeof(long)) || sizeof(page->compound_head) == sizeof(long long))) __compiletime_assert_102(); } while (0); do { *(volatile typeof(page->compound_head) *)&(page->compound_head) = (0); } while (0); } while (0);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void ClearPageCompound(struct page *page)
{
 do { if (__builtin_expect(!!(!PageHead(page)), 0)) do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 806; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } while (0);
 ClearPageHead(page);
}





int PageHuge(struct page *page);
bool folio_test_hugetlb(struct folio *folio);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int PageTransHuge(struct page *page)
{
 do { if (__builtin_expect(!!(PageTail(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PageTail(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 831; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0);
 return PageHead(page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool folio_test_transhuge(struct folio *folio)
{
 return folio_test_head(folio);
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int PageTransCompound(struct page *page)
{
 return PageCompound(page);
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int PageTransTail(struct page *page)
{
 return PageTail(page);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool folio_test_has_hwpoisoned(const struct folio *folio) { return false; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int PageHasHWPoisoned(const struct page *page) { return 0; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void folio_set_has_hwpoisoned(struct folio *folio) { } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void SetPageHasHWPoisoned(struct page *page) { } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void folio_clear_has_hwpoisoned(struct folio *folio) { } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void ClearPageHasHWPoisoned(struct page *page) { }
 static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool folio_test_set_has_hwpoisoned(struct folio *folio) { return 0; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int TestSetPageHasHWPoisoned(struct page *page) { return 0; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool folio_test_clear_has_hwpoisoned(struct folio *folio) { return 0; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int TestClearPageHasHWPoisoned(struct page *page) { return 0; }







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool is_page_hwpoison(struct page *page)
{
 if (PageHWPoison(page))
  return true;
 return PageHuge(page) && PageHWPoison(((typeof(page))_compound_head(page)));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int page_type_has_type(unsigned int page_type)
{
 return (int)page_type < -128;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int page_has_type(struct page *page)
{
 return page_type_has_type(page->page_type);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageBuddy(struct page *page) { return ((page->page_type & (0xf0000000 | 0x00000080)) == 0xf0000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __SetPageBuddy(struct page *page) { do { if (__builtin_expect(!!(!((page->page_type & (0xf0000000 | 0)) == 0xf0000000)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "!((page->page_type & (0xf0000000 | 0)) == 0xf0000000)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 942; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page->page_type &= ~0x00000080; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __ClearPageBuddy(struct page *page) { do { if (__builtin_expect(!!(!PageBuddy(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "!PageBuddy(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 942; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page->page_type |= 0x00000080; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageOffline(struct page *page) { return ((page->page_type & (0xf0000000 | 0x00000100)) == 0xf0000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __SetPageOffline(struct page *page) { do { if (__builtin_expect(!!(!((page->page_type & (0xf0000000 | 0)) == 0xf0000000)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "!((page->page_type & (0xf0000000 | 0)) == 0xf0000000)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 966; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page->page_type &= ~0x00000100; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __ClearPageOffline(struct page *page) { do { if (__builtin_expect(!!(!PageOffline(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "!PageOffline(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 966; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page->page_type |= 0x00000100; }

extern void page_offline_freeze(void);
extern void page_offline_thaw(void);
extern void page_offline_begin(void);
extern void page_offline_end(void);




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageTable(struct page *page) { return ((page->page_type & (0xf0000000 | 0x00000200)) == 0xf0000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __SetPageTable(struct page *page) { do { if (__builtin_expect(!!(!((page->page_type & (0xf0000000 | 0)) == 0xf0000000)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "!((page->page_type & (0xf0000000 | 0)) == 0xf0000000)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 976; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page->page_type &= ~0x00000200; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __ClearPageTable(struct page *page) { do { if (__builtin_expect(!!(!PageTable(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "!PageTable(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 976; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page->page_type |= 0x00000200; }




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageGuard(struct page *page) { return ((page->page_type & (0xf0000000 | 0x00000400)) == 0xf0000000); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __SetPageGuard(struct page *page) { do { if (__builtin_expect(!!(!((page->page_type & (0xf0000000 | 0)) == 0xf0000000)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "!((page->page_type & (0xf0000000 | 0)) == 0xf0000000)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 981; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page->page_type &= ~0x00000400; } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __ClearPageGuard(struct page *page) { do { if (__builtin_expect(!!(!PageGuard(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "!PageGuard(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 981; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page->page_type |= 0x00000400; }

extern bool is_free_buddy_page(struct page *page);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool folio_test_isolated(struct folio *folio) { return ((__builtin_constant_p(PG_isolated) && __builtin_constant_p((uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0)) && (uintptr_t)(folio_flags(folio, 0)) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(folio_flags(folio, 0)))) ? const_test_bit(PG_isolated, folio_flags(folio, 0)) : generic_test_bit(PG_isolated, folio_flags(folio, 0))); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageIsolated(struct page *page) { return ((__builtin_constant_p(PG_isolated) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 985; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 985; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 985; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags))) ? const_test_bit(PG_isolated, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 985; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) : generic_test_bit(PG_isolated, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 985; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_set_isolated(struct folio *folio) { set_bit(PG_isolated, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void SetPageIsolated(struct page *page) { set_bit(PG_isolated, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 985; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void folio_clear_isolated(struct folio *folio) { clear_bit(PG_isolated, folio_flags(folio, 0)); } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void ClearPageIsolated(struct page *page) { clear_bit(PG_isolated, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 985; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags); };

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) int PageAnonExclusive(struct page *page)
{
 do { if (__builtin_expect(!!(!PageAnon(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "!PageAnon(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 989; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0);
 do { if (__builtin_expect(!!(PageHuge(page) && !PageHead(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PageHuge(page) && !PageHead(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 990; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0);
 return ((__builtin_constant_p(PG_anon_exclusive) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 991; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 991; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 991; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags))) ? const_test_bit(PG_anon_exclusive, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 991; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) : generic_test_bit(PG_anon_exclusive, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 991; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void SetPageAnonExclusive(struct page *page)
{
 do { if (__builtin_expect(!!(!PageAnon(page) || PageKsm(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "!PageAnon(page) || PageKsm(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 996; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0);
 do { if (__builtin_expect(!!(PageHuge(page) && !PageHead(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PageHuge(page) && !PageHead(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 997; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0);
 set_bit(PG_anon_exclusive, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 998; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void ClearPageAnonExclusive(struct page *page)
{
 do { if (__builtin_expect(!!(!PageAnon(page) || PageKsm(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "!PageAnon(page) || PageKsm(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 1003; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0);
 do { if (__builtin_expect(!!(PageHuge(page) && !PageHead(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PageHuge(page) && !PageHead(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 1004; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0);
 clear_bit(PG_anon_exclusive, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 1005; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void __ClearPageAnonExclusive(struct page *page)
{
 do { if (__builtin_expect(!!(!PageAnon(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "!PageAnon(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 1010; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0);
 do { if (__builtin_expect(!!(PageHuge(page) && !PageHead(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PageHuge(page) && !PageHead(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 1011; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0);
 ((__builtin_constant_p(PG_anon_exclusive) && __builtin_constant_p((uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 1012; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) != (uintptr_t)((void *)0)) && (uintptr_t)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 1012; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(&({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 1012; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags))) ? generic___clear_bit(PG_anon_exclusive, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 1012; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags) : generic___clear_bit(PG_anon_exclusive, &({ do { if (__builtin_expect(!!(PagePoisoned(page)), 0)) { dump_page(page, "VM_BUG_ON_PAGE(" "PagePoisoned(page)"")"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/page-flags.h\"; .popsection; .long 10002b - .; .short 1012; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } } while (0); page; })->flags));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int page_has_private(struct page *page)
{
 return !!(page->flags & (1UL << PG_private | 1UL << PG_private_2));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool folio_has_private(struct folio *folio)
{
 return page_has_private(&folio->page);
}




typedef struct {

 struct lockdep_map dep_map;
 struct task_struct *owner;

} local_lock_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void local_lock_acquire(local_lock_t *l)
{
 lock_acquire(&l->dep_map, 0, 0, 0, 1, ((void *)0), ({ __label__ __here; __here: (unsigned long)&&__here; }));
 ({ int __ret = 0; if (!oops_in_progress && __builtin_expect(!!(l->owner), 0)) { do { } while(0); if (debug_locks_off() && !debug_locks_silent) ({ int __ret_warn_on = !!(1); if (__builtin_expect(!!(__ret_warn_on), 0)) do { do { } while(0); __warn_printk("DEBUG_LOCKS_WARN_ON(%s)", "l->owner"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/local_lock_internal.h\"; .popsection; .long 10002b - .; .short 30; .short (1 << 0)|((1 << 3) | ((9) << 8)); .popsection; 10001: break 1");; do { } while(0); } while (0); do { } while(0); } while (0); __builtin_expect(!!(__ret_warn_on), 0); }); do { } while(0); __ret = 1; } __ret; });
 l->owner = (current_thread_info()->task);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void local_lock_release(local_lock_t *l)
{
 ({ int __ret = 0; if (!oops_in_progress && __builtin_expect(!!(l->owner != (current_thread_info()->task)), 0)) { do { } while(0); if (debug_locks_off() && !debug_locks_silent) ({ int __ret_warn_on = !!(1); if (__builtin_expect(!!(__ret_warn_on), 0)) do { do { } while(0); __warn_printk("DEBUG_LOCKS_WARN_ON(%s)", "l->owner != current"); do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/local_lock_internal.h\"; .popsection; .long 10002b - .; .short 36; .short (1 << 0)|((1 << 3) | ((9) << 8)); .popsection; 10001: break 1");; do { } while(0); } while (0); do { } while(0); } while (0); __builtin_expect(!!(__ret_warn_on), 0); }); do { } while(0); __ret = 1; } __ret; });
 l->owner = ((void *)0);
 lock_release(&l->dep_map, ({ __label__ __here; __here: (unsigned long)&&__here; }));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void local_lock_debug_init(local_lock_t *l)
{
 l->owner = ((void *)0);
}
enum migratetype {
 MIGRATE_UNMOVABLE,
 MIGRATE_MOVABLE,
 MIGRATE_RECLAIMABLE,
 MIGRATE_PCPTYPES,
 MIGRATE_HIGHATOMIC = MIGRATE_PCPTYPES,
 MIGRATE_CMA,


 MIGRATE_ISOLATE,

 MIGRATE_TYPES
};


extern const char * const migratetype_names[MIGRATE_TYPES];
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool is_migrate_movable(int mt)
{
 return __builtin_expect(!!((mt) == MIGRATE_CMA), 0) || mt == MIGRATE_MOVABLE;
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool migratetype_is_mergeable(int mt)
{
 return mt < MIGRATE_PCPTYPES;
}





extern int page_group_by_mobility_disabled;
struct free_area {
 struct list_head free_list[MIGRATE_TYPES];
 unsigned long nr_free;
};

struct pglist_data;


enum numa_stat_item {
 NUMA_HIT,
 NUMA_MISS,
 NUMA_FOREIGN,
 NUMA_INTERLEAVE_HIT,
 NUMA_LOCAL,
 NUMA_OTHER,
 NR_VM_NUMA_EVENT_ITEMS
};




enum zone_stat_item {

 NR_FREE_PAGES,
 NR_ZONE_LRU_BASE,
 NR_ZONE_INACTIVE_ANON = NR_ZONE_LRU_BASE,
 NR_ZONE_ACTIVE_ANON,
 NR_ZONE_INACTIVE_FILE,
 NR_ZONE_ACTIVE_FILE,
 NR_ZONE_UNEVICTABLE,
 NR_ZONE_WRITE_PENDING,
 NR_MLOCK,

 NR_BOUNCE,

 NR_ZSPAGES,

 NR_FREE_CMA_PAGES,



 NR_VM_ZONE_STAT_ITEMS };

enum node_stat_item {
 NR_LRU_BASE,
 NR_INACTIVE_ANON = NR_LRU_BASE,
 NR_ACTIVE_ANON,
 NR_INACTIVE_FILE,
 NR_ACTIVE_FILE,
 NR_UNEVICTABLE,
 NR_SLAB_RECLAIMABLE_B,
 NR_SLAB_UNRECLAIMABLE_B,
 NR_ISOLATED_ANON,
 NR_ISOLATED_FILE,
 WORKINGSET_NODES,
 WORKINGSET_REFAULT_BASE,
 WORKINGSET_REFAULT_ANON = WORKINGSET_REFAULT_BASE,
 WORKINGSET_REFAULT_FILE,
 WORKINGSET_ACTIVATE_BASE,
 WORKINGSET_ACTIVATE_ANON = WORKINGSET_ACTIVATE_BASE,
 WORKINGSET_ACTIVATE_FILE,
 WORKINGSET_RESTORE_BASE,
 WORKINGSET_RESTORE_ANON = WORKINGSET_RESTORE_BASE,
 WORKINGSET_RESTORE_FILE,
 WORKINGSET_NODERECLAIM,
 NR_ANON_MAPPED,
 NR_FILE_MAPPED,

 NR_FILE_PAGES,
 NR_FILE_DIRTY,
 NR_WRITEBACK,
 NR_WRITEBACK_TEMP,
 NR_SHMEM,
 NR_SHMEM_THPS,
 NR_SHMEM_PMDMAPPED,
 NR_FILE_THPS,
 NR_FILE_PMDMAPPED,
 NR_ANON_THPS,
 NR_VMSCAN_WRITE,
 NR_VMSCAN_IMMEDIATE,
 NR_DIRTIED,
 NR_WRITTEN,
 NR_THROTTLED_WRITTEN,
 NR_KERNEL_MISC_RECLAIMABLE,
 NR_FOLL_PIN_ACQUIRED,
 NR_FOLL_PIN_RELEASED,
 NR_KERNEL_STACK_KB,



 NR_PAGETABLE,
 NR_SECONDARY_PAGETABLE,

 NR_SWAPCACHE,


 PGPROMOTE_SUCCESS,
 PGPROMOTE_CANDIDATE,

 NR_VM_NODE_STAT_ITEMS
};






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool vmstat_item_print_in_thp(enum node_stat_item item)
{
 if (!1)
  return false;

 return item == NR_ANON_THPS ||
        item == NR_FILE_THPS ||
        item == NR_SHMEM_THPS ||
        item == NR_SHMEM_PMDMAPPED ||
        item == NR_FILE_PMDMAPPED;
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) bool vmstat_item_in_bytes(int idx)
{
 return (idx == NR_SLAB_RECLAIMABLE_B ||
  idx == NR_SLAB_UNRECLAIMABLE_B);
}
enum lru_list {
 LRU_INACTIVE_ANON = 0,
 LRU_ACTIVE_ANON = 0 + 1,
 LRU_INACTIVE_FILE = 0 + 2,
 LRU_ACTIVE_FILE = 0 + 2 + 1,
 LRU_UNEVICTABLE,
 NR_LRU_LISTS
};

enum vmscan_throttle_state {
 VMSCAN_THROTTLE_WRITEBACK,
 VMSCAN_THROTTLE_ISOLATED,
 VMSCAN_THROTTLE_NOPROGRESS,
 VMSCAN_THROTTLE_CONGESTED,
 NR_VMSCAN_THROTTLE,
};





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool is_file_lru(enum lru_list lru)
{
 return (lru == LRU_INACTIVE_FILE || lru == LRU_ACTIVE_FILE);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool is_active_lru(enum lru_list lru)
{
 return (lru == LRU_ACTIVE_ANON || lru == LRU_ACTIVE_FILE);
}





enum lruvec_flags {
 LRUVEC_CGROUP_CONGESTED,
 LRUVEC_NODE_CONGESTED,
};
struct lruvec;
struct page_vma_mapped_walk;






enum {
 LRU_GEN_ANON,
 LRU_GEN_FILE,
};

enum {
 LRU_GEN_CORE,
 LRU_GEN_MM_WALK,
 LRU_GEN_NONLEAF_YOUNG,
 NR_LRU_GEN_CAPS
};
struct lru_gen_folio {

 unsigned long max_seq;

 unsigned long min_seq[2];

 unsigned long timestamps[4U];

 struct list_head folios[4U][2][3];

 long nr_pages[4U][2][3];

 unsigned long avg_refaulted[2][4U];

 unsigned long avg_total[2][4U];

 unsigned long protected[4U][2][4U - 1];

 atomic_long_t evicted[4U][2][4U];
 atomic_long_t refaulted[4U][2][4U];

 bool enabled;


 u8 gen;

 u8 seg;

 struct hlist_nulls_node list;

};

enum {
 MM_LEAF_TOTAL,
 MM_LEAF_OLD,
 MM_LEAF_YOUNG,
 MM_NONLEAF_TOTAL,
 MM_NONLEAF_FOUND,
 MM_NONLEAF_ADDED,
 NR_MM_STATS
};




struct lru_gen_mm_state {

 unsigned long seq;

 struct list_head *head;

 struct list_head *tail;

 unsigned long *filters[2];

 unsigned long stats[4U][NR_MM_STATS];
};

struct lru_gen_mm_walk {

 struct lruvec *lruvec;

 unsigned long max_seq;

 unsigned long next_addr;

 int nr_pages[4U][2][3];

 int mm_stats[NR_MM_STATS];

 int batched;
 bool can_swap;
 bool force_scan;
};

void lru_gen_init_lruvec(struct lruvec *lruvec);
void lru_gen_look_around(struct page_vma_mapped_walk *pvmw);
struct lru_gen_memcg {

 unsigned long seq;

 unsigned long nr_memcgs[2];

 struct hlist_nulls_head fifo[2][8];

 spinlock_t lock;
};

void lru_gen_init_pgdat(struct pglist_data *pgdat);

void lru_gen_init_memcg(struct mem_cgroup *memcg);
void lru_gen_exit_memcg(struct mem_cgroup *memcg);
void lru_gen_online_memcg(struct mem_cgroup *memcg);
void lru_gen_offline_memcg(struct mem_cgroup *memcg);
void lru_gen_release_memcg(struct mem_cgroup *memcg);
void lru_gen_soft_reclaim(struct mem_cgroup *memcg, int nid);
struct lruvec {
 struct list_head lists[NR_LRU_LISTS];

 spinlock_t lru_lock;





 unsigned long anon_cost;
 unsigned long file_cost;

 atomic_long_t nonresident_age;

 unsigned long refaults[2];

 unsigned long flags;


 struct lru_gen_folio lrugen;

 struct lru_gen_mm_state mm_state;


 struct pglist_data *pgdat;

};
typedef unsigned isolate_mode_t;

enum zone_watermarks {
 WMARK_MIN,
 WMARK_LOW,
 WMARK_HIGH,
 WMARK_PROMO,
 NR_WMARK
};
struct per_cpu_pages {
 spinlock_t lock;
 int count;
 int high;
 int batch;
 short free_factor;

 short expire;



 struct list_head lists[((MIGRATE_PCPTYPES * (3 + 1)) + 1)];
} __attribute__((__aligned__((1 << 6))));

struct per_cpu_zonestat {

 s8 vm_stat_diff[NR_VM_ZONE_STAT_ITEMS];
 s8 stat_threshold;







 unsigned long vm_numa_event[NR_VM_NUMA_EVENT_ITEMS];

};

struct per_cpu_nodestat {
 s8 stat_threshold;
 s8 vm_node_stat_diff[NR_VM_NODE_STAT_ITEMS];
};



enum zone_type {
 ZONE_DMA32,






 ZONE_NORMAL,
 ZONE_MOVABLE,



 __MAX_NR_ZONES

};





struct zone {



 unsigned long _watermark[NR_WMARK];
 unsigned long watermark_boost;

 unsigned long nr_reserved_highatomic;
 long lowmem_reserve[3];


 int node;

 struct pglist_data *zone_pgdat;
 struct per_cpu_pages *per_cpu_pageset;
 struct per_cpu_zonestat *per_cpu_zonestats;




 int pageset_high;
 int pageset_batch;
 unsigned long zone_start_pfn;
 atomic_long_t managed_pages;
 unsigned long spanned_pages;
 unsigned long present_pages;

 unsigned long present_early_pages;


 unsigned long cma_pages;


 const char *name;







 unsigned long nr_isolate_pageblock;




 seqlock_t span_seqlock;


 int initialized;


 struct cacheline_padding _pad1_;


 struct free_area free_area[11 + 1];







 unsigned long flags;


 spinlock_t lock;


 struct cacheline_padding _pad2_;






 unsigned long percpu_drift_mark;



 unsigned long compact_cached_free_pfn;

 unsigned long compact_cached_migrate_pfn[2];
 unsigned long compact_init_migrate_pfn;
 unsigned long compact_init_free_pfn;
 unsigned int compact_considered;
 unsigned int compact_defer_shift;
 int compact_order_failed;




 bool compact_blockskip_flush;


 bool contiguous;

 struct cacheline_padding _pad3_;

 atomic_long_t vm_stat[NR_VM_ZONE_STAT_ITEMS];
 atomic_long_t vm_numa_event[NR_VM_NUMA_EVENT_ITEMS];
} __attribute__((__aligned__(1 << (6))));

enum pgdat_flags {
 PGDAT_DIRTY,



 PGDAT_WRITEBACK,


 PGDAT_RECLAIM_LOCKED,
};

enum zone_flags {
 ZONE_BOOSTED_WATERMARK,


 ZONE_RECLAIM_ACTIVE,
};

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long zone_managed_pages(struct zone *zone)
{
 return (unsigned long)atomic_long_read(&zone->managed_pages);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long zone_cma_pages(struct zone *zone)
{

 return zone->cma_pages;



}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long zone_end_pfn(const struct zone *zone)
{
 return zone->zone_start_pfn + zone->spanned_pages;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool zone_spans_pfn(const struct zone *zone, unsigned long pfn)
{
 return zone->zone_start_pfn <= pfn && pfn < zone_end_pfn(zone);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool zone_is_initialized(struct zone *zone)
{
 return zone->initialized;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool zone_is_empty(struct zone *zone)
{
 return zone->spanned_pages == 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) enum zone_type page_zonenum(const struct page *page)
{
 do { kcsan_set_access_mask(((1UL << 2) - 1) << (((((sizeof(unsigned long)*8) - 0) - 6) - 2) * (2 != 0))); __kcsan_check_access(&(page->flags), sizeof(page->flags), (1 << 3)); kcsan_set_access_mask(0); kcsan_atomic_next(1); } while (0);
 return (page->flags >> (((((sizeof(unsigned long)*8) - 0) - 6) - 2) * (2 != 0))) & ((1UL << 2) - 1);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) enum zone_type folio_zonenum(const struct folio *folio)
{
 return page_zonenum(&folio->page);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool is_zone_device_page(const struct page *page)
{
 return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool zone_device_pages_have_same_pgmap(const struct page *a,
           const struct page *b)
{
 return true;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool folio_is_zone_device(const struct folio *folio)
{
 return is_zone_device_page(&folio->page);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool is_zone_movable_page(const struct page *page)
{
 return page_zonenum(page) == ZONE_MOVABLE;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool folio_is_zone_movable(const struct folio *folio)
{
 return folio_zonenum(folio) == ZONE_MOVABLE;
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool zone_intersects(struct zone *zone,
  unsigned long start_pfn, unsigned long nr_pages)
{
 if (zone_is_empty(zone))
  return false;
 if (start_pfn >= zone_end_pfn(zone) ||
     start_pfn + nr_pages <= zone->zone_start_pfn)
  return false;

 return true;
}
enum {
 ZONELIST_FALLBACK,





 ZONELIST_NOFALLBACK,

 MAX_ZONELISTS
};





struct zoneref {
 struct zone *zone;
 int zone_idx;
};
struct zonelist {
 struct zoneref _zonerefs[((1 << 6) * 3) + 1];
};






extern struct page *mem_map;


struct deferred_split {
 spinlock_t split_queue_lock;
 struct list_head split_queue;
 unsigned long split_queue_len;
};
typedef struct pglist_data {





 struct zone node_zones[3];






 struct zonelist node_zonelists[MAX_ZONELISTS];

 int nr_zones;
 spinlock_t node_size_lock;

 unsigned long node_start_pfn;
 unsigned long node_present_pages;
 unsigned long node_spanned_pages;

 int node_id;
 wait_queue_head_t kswapd_wait;
 wait_queue_head_t pfmemalloc_wait;


 wait_queue_head_t reclaim_wait[NR_VMSCAN_THROTTLE];

 atomic_t nr_writeback_throttled;
 unsigned long nr_reclaim_start;


 struct mutex kswapd_lock;

 struct task_struct *kswapd;
 int kswapd_order;
 enum zone_type kswapd_highest_zoneidx;

 int kswapd_failures;


 int kcompactd_max_order;
 enum zone_type kcompactd_highest_zoneidx;
 wait_queue_head_t kcompactd_wait;
 struct task_struct *kcompactd;
 bool proactive_compact_trigger;





 unsigned long totalreserve_pages;





 unsigned long min_unmapped_pages;
 unsigned long min_slab_pages;



 struct cacheline_padding _pad1_;






 unsigned long first_deferred_pfn;



 struct deferred_split deferred_split_queue;




 unsigned int nbp_rl_start;

 unsigned long nbp_rl_nr_cand;

 unsigned int nbp_threshold;

 unsigned int nbp_th_start;




 unsigned long nbp_th_nr_cand;
 struct lruvec __lruvec;

 unsigned long flags;



 struct lru_gen_mm_walk mm_walk;

 struct lru_gen_memcg memcg_lru;


 struct cacheline_padding _pad2_;


 struct per_cpu_nodestat *per_cpu_nodestats;
 atomic_long_t vm_stat[NR_VM_NODE_STAT_ITEMS];

 struct memory_tier *memtier;




} pg_data_t;







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long pgdat_end_pfn(pg_data_t *pgdat)
{
 return pgdat->node_start_pfn + pgdat->node_spanned_pages;
}






struct rcu_cblist {
 struct callback_head *head;
 struct callback_head **tail;
 long len;
};
struct rcu_segcblist {
 struct callback_head *head;
 struct callback_head **tails[4];
 unsigned long gp_seq[4];

 atomic_long_t len;



 long seglen[4];
 u8 flags;
};

struct srcu_struct;



int __init_srcu_struct(struct srcu_struct *ssp, const char *name,
         struct lock_class_key *key);


struct srcu_node;
struct srcu_struct;





struct srcu_data {

 atomic_long_t srcu_lock_count[2];
 atomic_long_t srcu_unlock_count[2];
 int srcu_nmi_safety;


 spinlock_t lock __attribute__((__aligned__(1 << (6))));
 struct rcu_segcblist srcu_cblist;
 unsigned long srcu_gp_seq_needed;
 unsigned long srcu_gp_seq_needed_exp;
 bool srcu_cblist_invoking;
 struct timer_list delay_work;
 struct work_struct work;
 struct callback_head srcu_barrier_head;
 struct srcu_node *mynode;
 unsigned long grpmask;

 int cpu;
 struct srcu_struct *ssp;
};




struct srcu_node {
 spinlock_t lock;
 unsigned long srcu_have_cbs[4];

 unsigned long srcu_data_have_cbs[4];
 unsigned long srcu_gp_seq_needed_exp;
 struct srcu_node *srcu_parent;
 int grplo;
 int grphi;
};




struct srcu_usage {
 struct srcu_node *node;
 struct srcu_node *level[2 + 1];

 int srcu_size_state;
 struct mutex srcu_cb_mutex;
 spinlock_t lock;
 struct mutex srcu_gp_mutex;
 unsigned long srcu_gp_seq;
 unsigned long srcu_gp_seq_needed;
 unsigned long srcu_gp_seq_needed_exp;
 unsigned long srcu_gp_start;
 unsigned long srcu_last_gp_end;
 unsigned long srcu_size_jiffies;
 unsigned long srcu_n_lock_retries;
 unsigned long srcu_n_exp_nodelay;
 bool sda_is_static;
 unsigned long srcu_barrier_seq;
 struct mutex srcu_barrier_mutex;
 struct completion srcu_barrier_completion;

 atomic_t srcu_barrier_cpu_cnt;


 unsigned long reschedule_jiffies;
 unsigned long reschedule_count;
 struct delayed_work work;
 struct srcu_struct *srcu_ssp;
};




struct srcu_struct {
 unsigned int srcu_idx;
 struct srcu_data *sda;
 struct lockdep_map dep_map;
 struct srcu_usage *srcu_sup;
};
void synchronize_srcu_expedited(struct srcu_struct *ssp);
void srcu_barrier(struct srcu_struct *ssp);
void srcu_torture_stats_print(struct srcu_struct *ssp, char *tt, char *tf);




void call_srcu(struct srcu_struct *ssp, struct callback_head *head,
  void (*func)(struct callback_head *head));
void cleanup_srcu_struct(struct srcu_struct *ssp);
int __srcu_read_lock(struct srcu_struct *ssp) ;
void __srcu_read_unlock(struct srcu_struct *ssp, int idx) ;
void synchronize_srcu(struct srcu_struct *ssp);
unsigned long get_state_synchronize_srcu(struct srcu_struct *ssp);
unsigned long start_poll_synchronize_srcu(struct srcu_struct *ssp);
bool poll_state_synchronize_srcu(struct srcu_struct *ssp, unsigned long cookie);





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int __srcu_read_lock_nmisafe(struct srcu_struct *ssp)
{
 return __srcu_read_lock(ssp);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __srcu_read_unlock_nmisafe(struct srcu_struct *ssp, int idx)
{
 __srcu_read_unlock(ssp, idx);
}


void srcu_init(void);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int srcu_read_lock_held(const struct srcu_struct *ssp)
{
 if (!debug_lockdep_rcu_enabled())
  return 1;
 return lock_is_held(&ssp->dep_map);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void srcu_lock_acquire(struct lockdep_map *map)
{
 lock_acquire(map, 0, 0, 2, 1, ((void *)0), ({ __label__ __here; __here: (unsigned long)&&__here; }));
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void srcu_lock_release(struct lockdep_map *map)
{
 lock_release(map, ({ __label__ __here; __here: (unsigned long)&&__here; }));
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void srcu_lock_sync(struct lockdep_map *map)
{
 lock_sync(map, 0, 0, 1, ((void *)0), ({ __label__ __here; __here: (unsigned long)&&__here; }));
}
void srcu_check_nmi_safety(struct srcu_struct *ssp, bool nmi_safe);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int srcu_read_lock(struct srcu_struct *ssp)
{
 int retval;

 srcu_check_nmi_safety(ssp, false);
 retval = __srcu_read_lock(ssp);
 srcu_lock_acquire(&ssp->dep_map);
 return retval;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int srcu_read_lock_nmisafe(struct srcu_struct *ssp)
{
 int retval;

 srcu_check_nmi_safety(ssp, true);
 retval = __srcu_read_lock_nmisafe(ssp);
 rcu_lock_acquire(&ssp->dep_map);
 return retval;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((patchable_function_entry(0, 0))) int
srcu_read_lock_notrace(struct srcu_struct *ssp)
{
 int retval;

 srcu_check_nmi_safety(ssp, false);
 retval = __srcu_read_lock(ssp);
 return retval;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int srcu_down_read(struct srcu_struct *ssp)
{
 ({ int __ret_warn_on = !!(((preempt_count() & (((1UL << (4))-1) << (((0 + 8) + 8) + 4))))); if (__builtin_expect(!!(__ret_warn_on), 0)) do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/srcu.h\"; .popsection; .long 10002b - .; .short 270; .short (1 << 0)|((1 << 1) | ((9) << 8)); .popsection; 10001: break 1");; do { } while(0); } while (0); __builtin_expect(!!(__ret_warn_on), 0); });
 srcu_check_nmi_safety(ssp, false);
 return __srcu_read_lock(ssp);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void srcu_read_unlock(struct srcu_struct *ssp, int idx)

{
 ({ int __ret_warn_on = !!(idx & ~0x1); if (__builtin_expect(!!(__ret_warn_on), 0)) do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/srcu.h\"; .popsection; .long 10002b - .; .short 285; .short (1 << 0)|((1 << 1) | ((9) << 8)); .popsection; 10001: break 1");; do { } while(0); } while (0); __builtin_expect(!!(__ret_warn_on), 0); });
 srcu_check_nmi_safety(ssp, false);
 srcu_lock_release(&ssp->dep_map);
 __srcu_read_unlock(ssp, idx);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void srcu_read_unlock_nmisafe(struct srcu_struct *ssp, int idx)

{
 ({ int __ret_warn_on = !!(idx & ~0x1); if (__builtin_expect(!!(__ret_warn_on), 0)) do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/srcu.h\"; .popsection; .long 10002b - .; .short 301; .short (1 << 0)|((1 << 1) | ((9) << 8)); .popsection; 10001: break 1");; do { } while(0); } while (0); __builtin_expect(!!(__ret_warn_on), 0); });
 srcu_check_nmi_safety(ssp, true);
 rcu_lock_release(&ssp->dep_map);
 __srcu_read_unlock_nmisafe(ssp, idx);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((patchable_function_entry(0, 0))) void
srcu_read_unlock_notrace(struct srcu_struct *ssp, int idx)
{
 srcu_check_nmi_safety(ssp, false);
 __srcu_read_unlock(ssp, idx);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void srcu_up_read(struct srcu_struct *ssp, int idx)

{
 ({ int __ret_warn_on = !!(idx & ~0x1); if (__builtin_expect(!!(__ret_warn_on), 0)) do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/srcu.h\"; .popsection; .long 10002b - .; .short 326; .short (1 << 0)|((1 << 1) | ((9) << 8)); .popsection; 10001: break 1");; do { } while(0); } while (0); __builtin_expect(!!(__ret_warn_on), 0); });
 ({ int __ret_warn_on = !!(((preempt_count() & (((1UL << (4))-1) << (((0 + 8) + 8) + 4))))); if (__builtin_expect(!!(__ret_warn_on), 0)) do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/srcu.h\"; .popsection; .long 10002b - .; .short 327; .short (1 << 0)|((1 << 1) | ((9) << 8)); .popsection; 10001: break 1");; do { } while(0); } while (0); __builtin_expect(!!(__ret_warn_on), 0); });
 srcu_check_nmi_safety(ssp, false);
 __srcu_read_unlock(ssp, idx);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void smp_mb__after_srcu_read_unlock(void)
{

}

typedef struct { struct srcu_struct *lock; int idx; } class_srcu_t; static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void class_srcu_destructor(class_srcu_t *_T) { if (_T->lock) { srcu_read_unlock(_T->lock, _T->idx); } } static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) class_srcu_t class_srcu_constructor(struct srcu_struct *l) { class_srcu_t _t = { .lock = l }, *_T = &_t; _T->idx = srcu_read_lock(_T->lock); return _t; }
struct notifier_block;

typedef int (*notifier_fn_t)(struct notifier_block *nb,
   unsigned long action, void *data);

struct notifier_block {
 notifier_fn_t notifier_call;
 struct notifier_block *next;
 int priority;
};

struct atomic_notifier_head {
 spinlock_t lock;
 struct notifier_block *head;
};

struct blocking_notifier_head {
 struct rw_semaphore rwsem;
 struct notifier_block *head;
};

struct raw_notifier_head {
 struct notifier_block *head;
};

struct srcu_notifier_head {
 struct mutex mutex;
 struct srcu_usage srcuu;
 struct srcu_struct srcu;
 struct notifier_block *head;
};
extern void srcu_init_notifier_head(struct srcu_notifier_head *nh);
extern int atomic_notifier_chain_register(struct atomic_notifier_head *nh,
  struct notifier_block *nb);
extern int blocking_notifier_chain_register(struct blocking_notifier_head *nh,
  struct notifier_block *nb);
extern int raw_notifier_chain_register(struct raw_notifier_head *nh,
  struct notifier_block *nb);
extern int srcu_notifier_chain_register(struct srcu_notifier_head *nh,
  struct notifier_block *nb);

extern int atomic_notifier_chain_register_unique_prio(
  struct atomic_notifier_head *nh, struct notifier_block *nb);
extern int blocking_notifier_chain_register_unique_prio(
  struct blocking_notifier_head *nh, struct notifier_block *nb);

extern int atomic_notifier_chain_unregister(struct atomic_notifier_head *nh,
  struct notifier_block *nb);
extern int blocking_notifier_chain_unregister(struct blocking_notifier_head *nh,
  struct notifier_block *nb);
extern int raw_notifier_chain_unregister(struct raw_notifier_head *nh,
  struct notifier_block *nb);
extern int srcu_notifier_chain_unregister(struct srcu_notifier_head *nh,
  struct notifier_block *nb);

extern int atomic_notifier_call_chain(struct atomic_notifier_head *nh,
  unsigned long val, void *v);
extern int blocking_notifier_call_chain(struct blocking_notifier_head *nh,
  unsigned long val, void *v);
extern int raw_notifier_call_chain(struct raw_notifier_head *nh,
  unsigned long val, void *v);
extern int srcu_notifier_call_chain(struct srcu_notifier_head *nh,
  unsigned long val, void *v);

extern int blocking_notifier_call_chain_robust(struct blocking_notifier_head *nh,
  unsigned long val_up, unsigned long val_down, void *v);
extern int raw_notifier_call_chain_robust(struct raw_notifier_head *nh,
  unsigned long val_up, unsigned long val_down, void *v);

extern bool atomic_notifier_call_chain_is_empty(struct atomic_notifier_head *nh);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int notifier_from_errno(int err)
{
 if (err)
  return 0x8000 | (0x0001 - err);

 return 0x0001;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int notifier_to_errno(int ret)
{
 ret &= ~0x8000;
 return ret > 0x0001 ? 0x0001 - ret : 0;
}
extern struct blocking_notifier_head reboot_notifier_list;


struct page;
struct zone;
struct pglist_data;
struct mem_section;
struct memory_group;
struct resource;
struct vmem_altmap;
struct dev_pagemap;
extern pg_data_t *node_data[];
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void arch_refresh_nodedata(int nid, pg_data_t *pgdat)
{
 node_data[nid] = pgdat;
}
struct page *pfn_to_online_page(unsigned long pfn);


enum {

 MMOP_OFFLINE = 0,

 MMOP_ONLINE,

 MMOP_ONLINE_KERNEL,

 MMOP_ONLINE_MOVABLE,
};


typedef int mhp_t;
struct mhp_params {
 struct vmem_altmap *altmap;
 pgprot_t pgprot;
 struct dev_pagemap *pgmap;
};

bool mhp_range_allowed(u64 start, u64 size, bool need_mapping);
struct range mhp_get_pluggable_range(bool need_mapping);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned zone_span_seqbegin(struct zone *zone)
{
 return read_seqbegin(&zone->span_seqlock);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int zone_span_seqretry(struct zone *zone, unsigned iv)
{
 return read_seqretry(&zone->span_seqlock, iv);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void zone_span_writelock(struct zone *zone)
{
 write_seqlock(&zone->span_seqlock);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void zone_span_writeunlock(struct zone *zone)
{
 write_sequnlock(&zone->span_seqlock);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void zone_seqlock_init(struct zone *zone)
{
 do { do { static struct lock_class_key __key; __raw_spin_lock_init(spinlock_check(&(&zone->span_seqlock)->lock), "&(&zone->span_seqlock)->lock", &__key, LD_WAIT_CONFIG); } while (0); do { seqcount_spinlock_t *____s = (&(&zone->span_seqlock)->seqcount); do { static struct lock_class_key __key; __seqcount_init((&____s->seqcount), "&____s->seqcount", &__key); } while (0); ____s->lock = (&(&zone->span_seqlock)->lock); } while (0); } while (0);
}
extern void adjust_present_page_count(struct page *page,
          struct memory_group *group,
          long nr_pages);

extern int mhp_init_memmap_on_memory(unsigned long pfn, unsigned long nr_pages,
         struct zone *zone);
extern void mhp_deinit_memmap_on_memory(unsigned long pfn, unsigned long nr_pages);
extern int online_pages(unsigned long pfn, unsigned long nr_pages,
   struct zone *zone, struct memory_group *group);
extern void __offline_isolated_pages(unsigned long start_pfn,
         unsigned long end_pfn);

typedef void (*online_page_callback_t)(struct page *page, unsigned int order);

extern void generic_online_page(struct page *page, unsigned int order);
extern int set_online_page_callback(online_page_callback_t callback);
extern int restore_online_page_callback(online_page_callback_t callback);

extern int try_online_node(int nid);

extern int arch_add_memory(int nid, u64 start, u64 size,
      struct mhp_params *params);
extern u64 max_mem_size;

extern int mhp_online_type_from_str(const char *str);


extern int mhp_default_online_type;

extern bool movable_node_enabled;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool movable_node_is_enabled(void)
{
 return movable_node_enabled;
}

extern void arch_remove_memory(u64 start, u64 size, struct vmem_altmap *altmap);
extern void __remove_pages(unsigned long start_pfn, unsigned long nr_pages,
      struct vmem_altmap *altmap);


extern int __add_pages(int nid, unsigned long start_pfn, unsigned long nr_pages,
         struct mhp_params *params);


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int add_pages(int nid, unsigned long start_pfn,
  unsigned long nr_pages, struct mhp_params *params)
{
 return __add_pages(nid, start_pfn, nr_pages, params);
}





void get_online_mems(void);
void put_online_mems(void);

void mem_hotplug_begin(void);
void mem_hotplug_done(void);


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void pgdat_kswapd_lock(pg_data_t *pgdat)
{
 mutex_lock_nested(&pgdat->kswapd_lock, 0);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void pgdat_kswapd_unlock(pg_data_t *pgdat)
{
 mutex_unlock(&pgdat->kswapd_lock);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void pgdat_kswapd_lock_init(pg_data_t *pgdat)
{
 do { static struct lock_class_key __key; __mutex_init((&pgdat->kswapd_lock), "&pgdat->kswapd_lock", &__key); } while (0);
}
struct range arch_get_mappable_range(void);





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0)))
void pgdat_resize_lock(struct pglist_data *pgdat, unsigned long *flags)
{
 do { do { ({ unsigned long __dummy; typeof(*flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); *flags = _raw_spin_lock_irqsave(spinlock_check(&pgdat->node_size_lock)); } while (0); } while (0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0)))
void pgdat_resize_unlock(struct pglist_data *pgdat, unsigned long *flags)
{
 spin_unlock_irqrestore(&pgdat->node_size_lock, *flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0)))
void pgdat_resize_init(struct pglist_data *pgdat)
{
 do { static struct lock_class_key __key; __raw_spin_lock_init(spinlock_check(&pgdat->node_size_lock), "&pgdat->node_size_lock", &__key, LD_WAIT_CONFIG); } while (0);
}
extern void try_offline_node(int nid);
extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages,
    struct zone *zone, struct memory_group *group);
extern int remove_memory(u64 start, u64 size);
extern void __remove_memory(u64 start, u64 size);
extern int offline_and_remove_memory(u64 start, u64 size);
extern void __attribute__((__section__(".ref.text"))) __attribute__((__noinline__)) free_area_init_core_hotplug(struct pglist_data *pgdat);
extern int __add_memory(int nid, u64 start, u64 size, mhp_t mhp_flags);
extern int add_memory(int nid, u64 start, u64 size, mhp_t mhp_flags);
extern int add_memory_resource(int nid, struct resource *resource,
          mhp_t mhp_flags);
extern int add_memory_driver_managed(int nid, u64 start, u64 size,
         const char *resource_name,
         mhp_t mhp_flags);
extern void move_pfn_range_to_zone(struct zone *zone, unsigned long start_pfn,
       unsigned long nr_pages,
       struct vmem_altmap *altmap, int migratetype);
extern void remove_pfn_range_from_zone(struct zone *zone,
           unsigned long start_pfn,
           unsigned long nr_pages);
extern int sparse_add_section(int nid, unsigned long pfn,
  unsigned long nr_pages, struct vmem_altmap *altmap,
  struct dev_pagemap *pgmap);
extern void sparse_remove_section(unsigned long pfn, unsigned long nr_pages,
      struct vmem_altmap *altmap);
extern struct page *sparse_decode_mem_map(unsigned long coded_mem_map,
       unsigned long pnum);
extern struct zone *zone_for_pfn_range(int online_type, int nid,
  struct memory_group *group, unsigned long start_pfn,
  unsigned long nr_pages);
extern int arch_create_linear_mapping(int nid, u64 start, u64 size,
          struct mhp_params *params);
void arch_remove_linear_mapping(u64 start, u64 size);
extern bool mhp_supports_memmap_on_memory(unsigned long size);

void build_all_zonelists(pg_data_t *pgdat);
void wakeup_kswapd(struct zone *zone, gfp_t gfp_mask, int order,
     enum zone_type highest_zoneidx);
bool __zone_watermark_ok(struct zone *z, unsigned int order, unsigned long mark,
    int highest_zoneidx, unsigned int alloc_flags,
    long free_pages);
bool zone_watermark_ok(struct zone *z, unsigned int order,
  unsigned long mark, int highest_zoneidx,
  unsigned int alloc_flags);
bool zone_watermark_ok_safe(struct zone *z, unsigned int order,
  unsigned long mark, int highest_zoneidx);




enum meminit_context {
 MEMINIT_EARLY,
 MEMINIT_HOTPLUG,
};

extern void init_currently_empty_zone(struct zone *zone, unsigned long start_pfn,
         unsigned long size);

extern void lruvec_init(struct lruvec *lruvec);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) struct pglist_data *lruvec_pgdat(struct lruvec *lruvec)
{

 return lruvec->pgdat;



}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int local_memory_node(int node_id) { return node_id; };
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool zone_is_zone_device(struct zone *zone)
{
 return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool managed_zone(struct zone *zone)
{
 return zone_managed_pages(zone);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool populated_zone(struct zone *zone)
{
 return zone->present_pages;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int zone_to_nid(struct zone *zone)
{
 return zone->node;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void zone_set_nid(struct zone *zone, int nid)
{
 zone->node = nid;
}
extern int movable_zone;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int is_highmem_idx(enum zone_type idx)
{




 return 0;

}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int is_highmem(struct zone *zone)
{
 return is_highmem_idx(((zone) - (zone)->zone_pgdat->node_zones));
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool has_managed_dma(void)
{
 return false;
}
extern int numa_off;
extern s16 __cpuid_to_node[64];
extern nodemask_t numa_nodes_parsed __attribute__((__section__(".init.data")));

struct numa_memblk {
 u64 start;
 u64 end;
 int nid;
};


struct numa_meminfo {
 int nr_blks;
 struct numa_memblk blk[((1 << 6)*2)];
};

extern int __attribute__((__section__(".init.text"))) __attribute__((__cold__)) numa_add_memblk(int nodeid, u64 start, u64 end);

extern void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) early_numa_add_cpu(int cpuid, s16 node);
extern void numa_add_cpu(unsigned int cpu);
extern void numa_remove_cpu(unsigned int cpu);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void numa_clear_node(int cpu)
{
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void set_cpuid_to_node(int cpuid, s16 node)
{
 __cpuid_to_node[cpuid] = node;
}

extern int early_cpu_to_node(int cpu);

extern struct pglist_data *node_data[];



extern void setup_zero_pages(void);



extern struct pglist_data *first_online_pgdat(void);
extern struct pglist_data *next_online_pgdat(struct pglist_data *pgdat);
extern struct zone *next_zone(struct zone *zone);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) struct zone *zonelist_zone(struct zoneref *zoneref)
{
 return zoneref->zone;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int zonelist_zone_idx(struct zoneref *zoneref)
{
 return zoneref->zone_idx;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int zonelist_node_idx(struct zoneref *zoneref)
{
 return zone_to_nid(zoneref->zone);
}

struct zoneref *__next_zones_zonelist(struct zoneref *z,
     enum zone_type highest_zoneidx,
     nodemask_t *nodes);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) struct zoneref *next_zones_zonelist(struct zoneref *z,
     enum zone_type highest_zoneidx,
     nodemask_t *nodes)
{
 if (__builtin_expect(!!(!nodes && zonelist_zone_idx(z) <= highest_zoneidx), 1))
  return z;
 return __next_zones_zonelist(z, highest_zoneidx, nodes);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) struct zoneref *first_zones_zonelist(struct zonelist *zonelist,
     enum zone_type highest_zoneidx,
     nodemask_t *nodes)
{
 return next_zones_zonelist(zonelist->_zonerefs,
       highest_zoneidx, nodes);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool movable_only_nodes(nodemask_t *nodes)
{
 struct zonelist *zonelist;
 struct zoneref *z;
 int nid;

 if (__nodes_empty(&(*nodes), (1 << 6)))
  return false;






 nid = __first_node(&(*nodes));
 zonelist = &(node_data[(nid)])->node_zonelists[ZONELIST_FALLBACK];
 z = first_zones_zonelist(zonelist, ZONE_NORMAL, nodes);
 return (!z->zone) ? true : false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long pfn_to_section_nr(unsigned long pfn)
{
 return pfn >> (29 - 14);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long section_nr_to_pfn(unsigned long sec)
{
 return sec << (29 - 14);
}
struct mem_section_usage {

 unsigned long subsection_map[((((1UL << (29 - 21))) + ((sizeof(long) * 8)) - 1) / ((sizeof(long) * 8)))];


 unsigned long pageblock_flags[0];
};

void subsection_map_init(unsigned long pfn, unsigned long nr_pages);

struct page;
struct page_ext;
struct mem_section {
 unsigned long section_mem_map;

 struct mem_section_usage *usage;





 struct page_ext *page_ext;
 unsigned long pad;





};
extern struct mem_section **mem_section;




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long *section_to_usemap(struct mem_section *ms)
{
 return ms->usage->pageblock_flags;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) struct mem_section *__nr_to_section(unsigned long nr)
{
 unsigned long root = ((nr) / (((1UL) << 14) / sizeof (struct mem_section)));

 if (__builtin_expect(!!(root >= ((((1UL << (48 - 29))) + ((((1UL) << 14) / sizeof (struct mem_section))) - 1) / ((((1UL) << 14) / sizeof (struct mem_section))))), 0))
  return ((void *)0);


 if (!mem_section || !mem_section[root])
  return ((void *)0);

 return &mem_section[root][nr & ((((1UL) << 14) / sizeof (struct mem_section)) - 1)];
}
extern size_t mem_section_usage_size(void);
enum {
 SECTION_MARKED_PRESENT_BIT,
 SECTION_HAS_MEM_MAP_BIT,
 SECTION_IS_ONLINE_BIT,
 SECTION_IS_EARLY_BIT,



 SECTION_MAP_LAST_BIT,
};
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) struct page *__section_mem_map_addr(struct mem_section *section)
{
 unsigned long map = section->section_mem_map;
 map &= (~(((((1UL))) << (SECTION_MAP_LAST_BIT)) - 1));
 return (struct page *)map;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int present_section(struct mem_section *section)
{
 return (section && (section->section_mem_map & ((((1UL))) << (SECTION_MARKED_PRESENT_BIT))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int present_section_nr(unsigned long nr)
{
 return present_section(__nr_to_section(nr));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int valid_section(struct mem_section *section)
{
 return (section && (section->section_mem_map & ((((1UL))) << (SECTION_HAS_MEM_MAP_BIT))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int early_section(struct mem_section *section)
{
 return (section && (section->section_mem_map & ((((1UL))) << (SECTION_IS_EARLY_BIT))));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int valid_section_nr(unsigned long nr)
{
 return valid_section(__nr_to_section(nr));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int online_section(struct mem_section *section)
{
 return (section && (section->section_mem_map & ((((1UL))) << (SECTION_IS_ONLINE_BIT))));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int online_device_section(struct mem_section *section)
{
 return 0;
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int online_section_nr(unsigned long nr)
{
 return online_section(__nr_to_section(nr));
}


void online_mem_sections(unsigned long start_pfn, unsigned long end_pfn);
void offline_mem_sections(unsigned long start_pfn, unsigned long end_pfn);


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) struct mem_section *__pfn_to_section(unsigned long pfn)
{
 return __nr_to_section(pfn_to_section_nr(pfn));
}

extern unsigned long __highest_present_section_nr;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int subsection_map_index(unsigned long pfn)
{
 return (pfn & ~((~((1UL << (29 - 14))-1)))) / (1UL << (21 - 14));
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int pfn_section_valid(struct mem_section *ms, unsigned long pfn)
{
 int idx = subsection_map_index(pfn);

 return ((__builtin_constant_p(idx) && __builtin_constant_p((uintptr_t)(ms->usage->subsection_map) != (uintptr_t)((void *)0)) && (uintptr_t)(ms->usage->subsection_map) != (uintptr_t)((void *)0) && __builtin_constant_p(*(const unsigned long *)(ms->usage->subsection_map))) ? const_test_bit(idx, ms->usage->subsection_map) : generic_test_bit(idx, ms->usage->subsection_map));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int pfn_valid(unsigned long pfn)
{
 struct mem_section *ms;







 if (((unsigned long)((((phys_addr_t)(pfn) << 14)) >> 14)) != pfn)
  return 0;

 if (pfn_to_section_nr(pfn) >= (1UL << (48 - 29)))
  return 0;
 ms = __pfn_to_section(pfn);
 if (!valid_section(ms))
  return 0;




 return early_section(ms) || pfn_section_valid(ms, pfn);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int pfn_in_present_section(unsigned long pfn)
{
 if (pfn_to_section_nr(pfn) >= (1UL << (48 - 29)))
  return 0;
 return present_section(__pfn_to_section(pfn));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long next_present_section_nr(unsigned long section_nr)
{
 while (++section_nr <= __highest_present_section_nr) {
  if (present_section_nr(section_nr))
   return section_nr;
 }

 return -1;
}
void sparse_init(void);
void topology_normalize_cpu_scale(void);
int topology_update_cpu_topology(void);





struct device_node;
bool topology_parse_cpu_capacity(struct device_node *cpu_node, int cpu);

extern __attribute__((__section__(".discard"))) __attribute__((unused)) char __pcpu_scope_cpu_scale; extern __attribute__((section(".data..percpu" ""))) __typeof__(unsigned long) cpu_scale;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long topology_get_cpu_scale(int cpu)
{
 return (*({ do { const void *__vpp_verify = (typeof((&(cpu_scale)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*((&(cpu_scale)))) *)((&(cpu_scale)))); (typeof((typeof(*((&(cpu_scale)))) *)((&(cpu_scale))))) (__ptr + (((__per_cpu_offset[(cpu)])))); }); }));
}

void topology_set_cpu_scale(unsigned int cpu, unsigned long capacity);

extern __attribute__((__section__(".discard"))) __attribute__((unused)) char __pcpu_scope_arch_freq_scale; extern __attribute__((section(".data..percpu" ""))) __typeof__(unsigned long) arch_freq_scale;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long topology_get_freq_scale(int cpu)
{
 return (*({ do { const void *__vpp_verify = (typeof((&(arch_freq_scale)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*((&(arch_freq_scale)))) *)((&(arch_freq_scale)))); (typeof((typeof(*((&(arch_freq_scale)))) *)((&(arch_freq_scale))))) (__ptr + (((__per_cpu_offset[(cpu)])))); }); }));
}

void topology_set_freq_scale(const struct cpumask *cpus, unsigned long cur_freq,
        unsigned long max_freq);
bool topology_scale_freq_invariant(void);

enum scale_freq_source {
 SCALE_FREQ_SOURCE_CPUFREQ = 0,
 SCALE_FREQ_SOURCE_ARCH,
 SCALE_FREQ_SOURCE_CPPC,
};

struct scale_freq_data {
 enum scale_freq_source source;
 void (*set_freq_scale)(void);
};

void topology_scale_freq_tick(void);
void topology_set_scale_freq_source(struct scale_freq_data *data, const struct cpumask *cpus);
void topology_clear_scale_freq_source(enum scale_freq_source source, const struct cpumask *cpus);

extern __attribute__((__section__(".discard"))) __attribute__((unused)) char __pcpu_scope_thermal_pressure; extern __attribute__((section(".data..percpu" ""))) __typeof__(unsigned long) thermal_pressure;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long topology_get_thermal_pressure(int cpu)
{
 return (*({ do { const void *__vpp_verify = (typeof((&(thermal_pressure)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*((&(thermal_pressure)))) *)((&(thermal_pressure)))); (typeof((typeof(*((&(thermal_pressure)))) *)((&(thermal_pressure))))) (__ptr + (((__per_cpu_offset[(cpu)])))); }); }));
}

void topology_update_thermal_pressure(const struct cpumask *cpus,
          unsigned long capped_freq);

struct cpu_topology {
 int thread_id;
 int core_id;
 int cluster_id;
 int package_id;
 cpumask_t thread_sibling;
 cpumask_t core_sibling;
 cpumask_t cluster_sibling;
 cpumask_t llc_sibling;
};





extern cpumask_t cpus_on_node[];



struct pci_bus;
extern int pcibus_to_node(struct pci_bus *);



extern unsigned char node_distances[(1 << 6)][(1 << 6)];

void numa_set_distance(int from, int to, int distance);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void arch_fix_phys_package_id(int num, u32 slot) { }
int arch_update_cpu_topology(void);
extern int __attribute__((__section__(".data..read_mostly"))) node_reclaim_distance;






extern __attribute__((__section__(".discard"))) __attribute__((unused)) char __pcpu_scope_numa_node; extern __attribute__((section(".data..percpu" ""))) __typeof__(int) numa_node;



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int numa_node_id(void)
{
 return ({ typeof(numa_node) pscr_ret__; do { const void *__vpp_verify = (typeof((&(numa_node)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(numa_node)) { case 1: pscr_ret__ = ({ *({ do { const void *__vpp_verify = (typeof((&(numa_node)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(numa_node))) *)(&(numa_node))); (typeof((typeof(*(&(numa_node))) *)(&(numa_node)))) (__ptr + ((__my_cpu_offset))); }); }); }); break; case 2: pscr_ret__ = ({ *({ do { const void *__vpp_verify = (typeof((&(numa_node)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(numa_node))) *)(&(numa_node))); (typeof((typeof(*(&(numa_node))) *)(&(numa_node)))) (__ptr + ((__my_cpu_offset))); }); }); }); break; case 4: pscr_ret__ = ({ *({ do { const void *__vpp_verify = (typeof((&(numa_node)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(numa_node))) *)(&(numa_node))); (typeof((typeof(*(&(numa_node))) *)(&(numa_node)))) (__ptr + ((__my_cpu_offset))); }); }); }); break; case 8: pscr_ret__ = ({ *({ do { const void *__vpp_verify = (typeof((&(numa_node)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(numa_node))) *)(&(numa_node))); (typeof((typeof(*(&(numa_node))) *)(&(numa_node)))) (__ptr + ((__my_cpu_offset))); }); }); }); break; default: __bad_size_call_parameter(); break; } pscr_ret__; });
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int cpu_to_node(int cpu)
{
 return (*({ do { const void *__vpp_verify = (typeof((&(numa_node)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*((&(numa_node)))) *)((&(numa_node)))); (typeof((typeof(*((&(numa_node)))) *)((&(numa_node))))) (__ptr + (((__per_cpu_offset[(cpu)])))); }); }));
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void set_numa_node(int node)
{
 do { do { const void *__vpp_verify = (typeof((&(numa_node)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(numa_node)) { case 1: do { __percpu_write(&(numa_node), (unsigned long)(node), sizeof(numa_node)); } while (0);break; case 2: do { __percpu_write(&(numa_node), (unsigned long)(node), sizeof(numa_node)); } while (0);break; case 4: do { __percpu_write(&(numa_node), (unsigned long)(node), sizeof(numa_node)); } while (0);break; case 8: do { __percpu_write(&(numa_node), (unsigned long)(node), sizeof(numa_node)); } while (0);break; default: __bad_size_call_parameter();break; } } while (0);
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void set_cpu_numa_node(int cpu, int node)
{
 (*({ do { const void *__vpp_verify = (typeof((&(numa_node)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*((&(numa_node)))) *)((&(numa_node)))); (typeof((typeof(*((&(numa_node)))) *)((&(numa_node))))) (__ptr + (((__per_cpu_offset[(cpu)])))); }); })) = node;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int numa_mem_id(void)
{
 return numa_node_id();
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int cpu_to_mem(int cpu)
{
 return cpu_to_node(cpu);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) const struct cpumask *cpu_smt_mask(int cpu)
{
 return (&cpu_sibling_map[cpu]);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) const struct cpumask *cpu_cpu_mask(int cpu)
{
 return (&cpus_on_node[cpu_to_node(cpu)]);
}






int sched_numa_find_nth_cpu(const struct cpumask *cpus, int cpu, int node);
int sched_numa_find_next_cpu(const struct cpumask *cpus, int cpu, int node, unsigned int *hop);

struct vm_area_struct;





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int gfp_migratetype(const gfp_t gfp_flags)
{
 (void)({ int __ret_warn_on = !!((gfp_flags & ((( gfp_t)0x10u)|(( gfp_t)0x08u))) == ((( gfp_t)0x10u)|(( gfp_t)0x08u))); if (__builtin_expect(!!(__ret_warn_on), 0)) do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/gfp.h\"; .popsection; .long 10002b - .; .short 18; .short (1 << 0)|(((9) << 8)); .popsection; 10001: break 1");; do { } while(0); } while (0); __builtin_expect(!!(__ret_warn_on), 0); });
 do { __attribute__((__noreturn__)) extern void __compiletime_assert_103(void) __attribute__((__error__("BUILD_BUG_ON failed: " "(1UL << GFP_MOVABLE_SHIFT) != ___GFP_MOVABLE"))); if (!(!((1UL << 3) != 0x08u))) __compiletime_assert_103(); } while (0);
 do { __attribute__((__noreturn__)) extern void __compiletime_assert_104(void) __attribute__((__error__("BUILD_BUG_ON failed: " "(___GFP_MOVABLE >> GFP_MOVABLE_SHIFT) != MIGRATE_MOVABLE"))); if (!(!((0x08u >> 3) != MIGRATE_MOVABLE))) __compiletime_assert_104(); } while (0);
 do { __attribute__((__noreturn__)) extern void __compiletime_assert_105(void) __attribute__((__error__("BUILD_BUG_ON failed: " "(___GFP_RECLAIMABLE >> GFP_MOVABLE_SHIFT) != MIGRATE_RECLAIMABLE"))); if (!(!((0x10u >> 3) != MIGRATE_RECLAIMABLE))) __compiletime_assert_105(); } while (0);
 do { __attribute__((__noreturn__)) extern void __compiletime_assert_106(void) __attribute__((__error__("BUILD_BUG_ON failed: " "((___GFP_MOVABLE | ___GFP_RECLAIMABLE) >> GFP_MOVABLE_SHIFT) != MIGRATE_HIGHATOMIC"))); if (!(!(((0x08u | 0x10u) >> 3) != MIGRATE_HIGHATOMIC))) __compiletime_assert_106(); } while (0);


 if (__builtin_expect(!!(page_group_by_mobility_disabled), 0))
  return MIGRATE_UNMOVABLE;


 return ( unsigned long)(gfp_flags & ((( gfp_t)0x10u)|(( gfp_t)0x08u))) >> 3;
}



static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool gfpflags_allow_blocking(const gfp_t gfp_flags)
{
 return !!(gfp_flags & (( gfp_t)0x400u));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) enum zone_type gfp_zone(gfp_t flags)
{
 enum zone_type z;
 int bit = ( int) (flags & ((( gfp_t)0x01u)|(( gfp_t)0x02u)|(( gfp_t)0x04u)|(( gfp_t)0x08u)));

 z = (( (ZONE_NORMAL << 0 * 2) | (ZONE_NORMAL << 0x01u * 2) | (ZONE_NORMAL << 0x02u * 2) | (ZONE_DMA32 << 0x04u * 2) | (ZONE_NORMAL << 0x08u * 2) | (ZONE_NORMAL << (0x08u | 0x01u) * 2) | (ZONE_MOVABLE << (0x08u | 0x02u) * 2) | (ZONE_DMA32 << (0x08u | 0x04u) * 2)) >> (bit * 2)) &
      ((1 << 2) - 1);
 do { if (__builtin_expect(!!((( 1 << (0x01u | 0x02u) | 1 << (0x01u | 0x04u) | 1 << (0x04u | 0x02u) | 1 << (0x01u | 0x04u | 0x02u) | 1 << (0x08u | 0x02u | 0x01u) | 1 << (0x08u | 0x04u | 0x01u) | 1 << (0x08u | 0x04u | 0x02u) | 1 << (0x08u | 0x04u | 0x01u | 0x02u) ) >> bit) & 1), 0)) do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/gfp.h\"; .popsection; .long 10002b - .; .short 136; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } while (0);
 return z;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int gfp_zonelist(gfp_t flags)
{

 if (__builtin_expect(!!(flags & (( gfp_t)0x200000u)), 0))
  return ZONELIST_NOFALLBACK;

 return ZONELIST_FALLBACK;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) struct zonelist *node_zonelist(int nid, gfp_t flags)
{
 return (node_data[(nid)])->node_zonelists + gfp_zonelist(flags);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void arch_free_page(struct page *page, int order) { }


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void arch_alloc_page(struct page *page, int order) { }


struct page *__alloc_pages(gfp_t gfp, unsigned int order, int preferred_nid,
  nodemask_t *nodemask);
struct folio *__folio_alloc(gfp_t gfp, unsigned int order, int preferred_nid,
  nodemask_t *nodemask);

unsigned long __alloc_pages_bulk(gfp_t gfp, int preferred_nid,
    nodemask_t *nodemask, int nr_pages,
    struct list_head *page_list,
    struct page **page_array);

unsigned long alloc_pages_bulk_array_mempolicy(gfp_t gfp,
    unsigned long nr_pages,
    struct page **page_array);


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long
alloc_pages_bulk_list(gfp_t gfp, unsigned long nr_pages, struct list_head *list)
{
 return __alloc_pages_bulk(gfp, numa_mem_id(), ((void *)0), nr_pages, list, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long
alloc_pages_bulk_array(gfp_t gfp, unsigned long nr_pages, struct page **page_array)
{
 return __alloc_pages_bulk(gfp, numa_mem_id(), ((void *)0), nr_pages, ((void *)0), page_array);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned long
alloc_pages_bulk_array_node(gfp_t gfp, int nid, unsigned long nr_pages, struct page **page_array)
{
 if (nid == (-1))
  nid = numa_mem_id();

 return __alloc_pages_bulk(gfp, nid, ((void *)0), nr_pages, ((void *)0), page_array);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void warn_if_node_offline(int this_node, gfp_t gfp_mask)
{
 gfp_t warn_gfp = gfp_mask & ((( gfp_t)0x200000u)|(( gfp_t)0x2000u));

 if (warn_gfp != ((( gfp_t)0x200000u)|(( gfp_t)0x2000u)))
  return;

 if (node_state((this_node), N_ONLINE))
  return;

 ({ do { if (__builtin_constant_p("\001" "4" "%pGg allocation from offline node %d\n") && __builtin_constant_p(((void *)0))) { static const struct pi_entry _entry __attribute__((__used__)) = { .fmt = __builtin_constant_p("\001" "4" "%pGg allocation from offline node %d\n") ? ("\001" "4" "%pGg allocation from offline node %d\n") : ((void *)0), .func = __func__, .file = "include/linux/gfp.h", .line = 223, .level = __builtin_constant_p(((void *)0)) ? (((void *)0)) : ((void *)0), .subsys_fmt_prefix = ((void *)0), }; static const struct pi_entry *_entry_ptr __attribute__((__used__)) __attribute__((__section__(".printk_index"))) = &_entry; } } while (0); _printk("\001" "4" "%pGg allocation from offline node %d\n", &gfp_mask, this_node); });
 dump_stack();
}





static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) struct page *
__alloc_pages_node(int nid, gfp_t gfp_mask, unsigned int order)
{
 do { if (__builtin_expect(!!(nid < 0 || nid >= (1 << 6)), 0)) do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/gfp.h\"; .popsection; .long 10002b - .; .short 234; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } while (0);
 warn_if_node_offline(nid, gfp_mask);

 return __alloc_pages(gfp_mask, order, nid, ((void *)0));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0)))
struct folio *__folio_alloc_node(gfp_t gfp, unsigned int order, int nid)
{
 do { if (__builtin_expect(!!(nid < 0 || nid >= (1 << 6)), 0)) do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/gfp.h\"; .popsection; .long 10002b - .; .short 243; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0); } while (0);
 warn_if_node_offline(nid, gfp);

 return __folio_alloc(gfp, order, nid, ((void *)0));
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) struct page *alloc_pages_node(int nid, gfp_t gfp_mask,
      unsigned int order)
{
 if (nid == (-1))
  nid = numa_mem_id();

 return __alloc_pages_node(nid, gfp_mask, order);
}


struct page *alloc_pages(gfp_t gfp, unsigned int order);
struct folio *folio_alloc(gfp_t gfp, unsigned order);
struct folio *vma_alloc_folio(gfp_t gfp, int order, struct vm_area_struct *vma,
  unsigned long addr, bool hugepage);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) struct page *alloc_page_vma(gfp_t gfp,
  struct vm_area_struct *vma, unsigned long addr)
{
 struct folio *folio = vma_alloc_folio(gfp, 0, vma, addr, false);

 return &folio->page;
}

extern unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order);
extern unsigned long get_zeroed_page(gfp_t gfp_mask);

void *alloc_pages_exact(size_t size, gfp_t gfp_mask) __attribute__((__alloc_size__(1))) __attribute__((__malloc__));
void free_pages_exact(void *virt, size_t size);
__attribute__((__section__(".meminit.text"))) __attribute__((__cold__)) __attribute__((patchable_function_entry(0, 0))) void *alloc_pages_exact_nid(int nid, size_t size, gfp_t gfp_mask) __attribute__((__alloc_size__(2))) __attribute__((__malloc__));







extern void __free_pages(struct page *page, unsigned int order);
extern void free_pages(unsigned long addr, unsigned int order);

struct page_frag_cache;
extern void __page_frag_cache_drain(struct page *page, unsigned int count);
extern void *page_frag_alloc_align(struct page_frag_cache *nc,
       unsigned int fragsz, gfp_t gfp_mask,
       unsigned int align_mask);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void *page_frag_alloc(struct page_frag_cache *nc,
        unsigned int fragsz, gfp_t gfp_mask)
{
 return page_frag_alloc_align(nc, fragsz, gfp_mask, ~0u);
}

extern void page_frag_free(void *addr);




void page_alloc_init_cpuhp(void);
void drain_zone_pages(struct zone *zone, struct per_cpu_pages *pcp);
void drain_all_pages(struct zone *zone);
void drain_local_pages(struct zone *zone);

void page_alloc_init_late(void);
extern gfp_t gfp_allowed_mask;


bool gfp_pfmemalloc_allowed(gfp_t gfp_mask);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool gfp_has_io_fs(gfp_t gfp)
{
 return (gfp & ((( gfp_t)0x40u) | (( gfp_t)0x80u))) == ((( gfp_t)0x40u) | (( gfp_t)0x80u));
}

extern gfp_t vma_thp_gfp_mask(struct vm_area_struct *vma);



extern int alloc_contig_range(unsigned long start, unsigned long end,
         unsigned migratetype, gfp_t gfp_mask);
extern struct page *alloc_contig_pages(unsigned long nr_pages, gfp_t gfp_mask,
           int nid, nodemask_t *nodemask);

void free_contig_range(unsigned long pfn, unsigned long nr_pages);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool __attribute__((__warn_unused_result__)) __must_check_overflow(bool overflow)
{
 return __builtin_expect(!!(overflow), 0);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) size_t __attribute__((__warn_unused_result__)) size_mul(size_t factor1, size_t factor2)
{
 size_t bytes;

 if (__must_check_overflow(__builtin_mul_overflow(factor1, factor2, &bytes)))
  return (~(size_t)0);

 return bytes;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) size_t __attribute__((__warn_unused_result__)) size_add(size_t addend1, size_t addend2)
{
 size_t bytes;

 if (__must_check_overflow(__builtin_add_overflow(addend1, addend2, &bytes)))
  return (~(size_t)0);

 return bytes;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) size_t __attribute__((__warn_unused_result__)) size_sub(size_t minuend, size_t subtrahend)
{
 size_t bytes;

 if (minuend == (~(size_t)0) || subtrahend == (~(size_t)0) ||
     __must_check_overflow(__builtin_sub_overflow(minuend, subtrahend, &bytes)))
  return (~(size_t)0);

 return bytes;
}


struct percpu_ref;
typedef void (percpu_ref_func_t)(struct percpu_ref *);


enum {
 __PERCPU_REF_ATOMIC = 1LU << 0,
 __PERCPU_REF_DEAD = 1LU << 1,
 __PERCPU_REF_ATOMIC_DEAD = __PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD,

 __PERCPU_REF_FLAG_BITS = 2,
};


enum {







 PERCPU_REF_INIT_ATOMIC = 1 << 0,






 PERCPU_REF_INIT_DEAD = 1 << 1,




 PERCPU_REF_ALLOW_REINIT = 1 << 2,
};

struct percpu_ref_data {
 atomic_long_t count;
 percpu_ref_func_t *release;
 percpu_ref_func_t *confirm_switch;
 bool force_atomic:1;
 bool allow_reinit:1;
 struct callback_head rcu;
 struct percpu_ref *ref;
};

struct percpu_ref {




 unsigned long percpu_count_ptr;







 struct percpu_ref_data *data;
};

int __attribute__((__warn_unused_result__)) percpu_ref_init(struct percpu_ref *ref,
     percpu_ref_func_t *release, unsigned int flags,
     gfp_t gfp);
void percpu_ref_exit(struct percpu_ref *ref);
void percpu_ref_switch_to_atomic(struct percpu_ref *ref,
     percpu_ref_func_t *confirm_switch);
void percpu_ref_switch_to_atomic_sync(struct percpu_ref *ref);
void percpu_ref_switch_to_percpu(struct percpu_ref *ref);
void percpu_ref_kill_and_confirm(struct percpu_ref *ref,
     percpu_ref_func_t *confirm_kill);
void percpu_ref_resurrect(struct percpu_ref *ref);
void percpu_ref_reinit(struct percpu_ref *ref);
bool percpu_ref_is_zero(struct percpu_ref *ref);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void percpu_ref_kill(struct percpu_ref *ref)
{
 percpu_ref_kill_and_confirm(ref, ((void *)0));
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool __ref_is_percpu(struct percpu_ref *ref,
       unsigned long **percpu_countp)
{
 unsigned long percpu_ptr;
 percpu_ptr = ({ do { __attribute__((__noreturn__)) extern void __compiletime_assert_107(void) __attribute__((__error__("Unsupported access size for {READ,WRITE}_ONCE()."))); if (!((sizeof(ref->percpu_count_ptr) == sizeof(char) || sizeof(ref->percpu_count_ptr) == sizeof(short) || sizeof(ref->percpu_count_ptr) == sizeof(int) || sizeof(ref->percpu_count_ptr) == sizeof(long)) || sizeof(ref->percpu_count_ptr) == sizeof(long long))) __compiletime_assert_107(); } while (0); (*(const volatile typeof( _Generic((ref->percpu_count_ptr), char: (char)0, unsigned char: (unsigned char)0, signed char: (signed char)0, unsigned short: (unsigned short)0, signed short: (signed short)0, unsigned int: (unsigned int)0, signed int: (signed int)0, unsigned long: (unsigned long)0, signed long: (signed long)0, unsigned long long: (unsigned long long)0, signed long long: (signed long long)0, default: (ref->percpu_count_ptr))) *)&(ref->percpu_count_ptr)); });







 if (__builtin_expect(!!(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD), 0))
  return false;

 *percpu_countp = (unsigned long *)percpu_ptr;
 return true;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void percpu_ref_get_many(struct percpu_ref *ref, unsigned long nr)
{
 unsigned long *percpu_count;

 rcu_read_lock();

 if (__ref_is_percpu(ref, &percpu_count))
  do { do { const void *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(*percpu_count)) { case 1: do { unsigned long __flags; do { ({ unsigned long __dummy; typeof(__flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); __flags = arch_local_irq_save(); } while (0); do { *({ do { const void *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__my_cpu_offset))); }); }) += nr; } while (0); do { ({ unsigned long __dummy; typeof(__flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); do { if (__builtin_expect(!!(!arch_irqs_disabled()), 0)) warn_bogus_irq_restore(); } while (0); arch_local_irq_restore(__flags); } while (0); } while (0);break; case 2: do { unsigned long __flags; do { ({ unsigned long __dummy; typeof(__flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); __flags = arch_local_irq_save(); } while (0); do { *({ do { const void *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__my_cpu_offset))); }); }) += nr; } while (0); do { ({ unsigned long __dummy; typeof(__flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); do { if (__builtin_expect(!!(!arch_irqs_disabled()), 0)) warn_bogus_irq_restore(); } while (0); arch_local_irq_restore(__flags); } while (0); } while (0);break; case 4: ({ typeof(*percpu_count) __retval; do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __retval = (typeof(*percpu_count))__percpu_add(({ do { const void *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__my_cpu_offset))); }); }), (nr), sizeof(*percpu_count)); do { __asm__ __volatile__("": : :"memory"); __preempt_count_sub(1); } while (0); __retval; });break; case 8: ({ typeof(*percpu_count) __retval; do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __retval = (typeof(*percpu_count))__percpu_add(({ do { const void *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__my_cpu_offset))); }); }), (nr), sizeof(*percpu_count)); do { __asm__ __volatile__("": : :"memory"); __preempt_count_sub(1); } while (0); __retval; });break; default: __bad_size_call_parameter();break; } } while (0);
 else
  atomic_long_add(nr, &ref->data->count);

 rcu_read_unlock();
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void percpu_ref_get(struct percpu_ref *ref)
{
 percpu_ref_get_many(ref, 1);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool percpu_ref_tryget_many(struct percpu_ref *ref,
       unsigned long nr)
{
 unsigned long *percpu_count;
 bool ret;

 rcu_read_lock();

 if (__ref_is_percpu(ref, &percpu_count)) {
  do { do { const void *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(*percpu_count)) { case 1: do { unsigned long __flags; do { ({ unsigned long __dummy; typeof(__flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); __flags = arch_local_irq_save(); } while (0); do { *({ do { const void *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__my_cpu_offset))); }); }) += nr; } while (0); do { ({ unsigned long __dummy; typeof(__flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); do { if (__builtin_expect(!!(!arch_irqs_disabled()), 0)) warn_bogus_irq_restore(); } while (0); arch_local_irq_restore(__flags); } while (0); } while (0);break; case 2: do { unsigned long __flags; do { ({ unsigned long __dummy; typeof(__flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); __flags = arch_local_irq_save(); } while (0); do { *({ do { const void *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__my_cpu_offset))); }); }) += nr; } while (0); do { ({ unsigned long __dummy; typeof(__flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); do { if (__builtin_expect(!!(!arch_irqs_disabled()), 0)) warn_bogus_irq_restore(); } while (0); arch_local_irq_restore(__flags); } while (0); } while (0);break; case 4: ({ typeof(*percpu_count) __retval; do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __retval = (typeof(*percpu_count))__percpu_add(({ do { const void *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__my_cpu_offset))); }); }), (nr), sizeof(*percpu_count)); do { __asm__ __volatile__("": : :"memory"); __preempt_count_sub(1); } while (0); __retval; });break; case 8: ({ typeof(*percpu_count) __retval; do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __retval = (typeof(*percpu_count))__percpu_add(({ do { const void *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__my_cpu_offset))); }); }), (nr), sizeof(*percpu_count)); do { __asm__ __volatile__("": : :"memory"); __preempt_count_sub(1); } while (0); __retval; });break; default: __bad_size_call_parameter();break; } } while (0);
  ret = true;
 } else {
  ret = atomic_long_add_unless(&ref->data->count, nr, 0);
 }

 rcu_read_unlock();

 return ret;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool percpu_ref_tryget(struct percpu_ref *ref)
{
 return percpu_ref_tryget_many(ref, 1);
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool percpu_ref_tryget_live_rcu(struct percpu_ref *ref)
{
 unsigned long *percpu_count;
 bool ret = false;

 ({ int __ret_warn_on = !!(!rcu_read_lock_held()); if (__builtin_expect(!!(__ret_warn_on), 0)) do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/percpu-refcount.h\"; .popsection; .long 10002b - .; .short 280; .short (1 << 0)|((1 << 1) | ((9) << 8)); .popsection; 10001: break 1");; do { } while(0); } while (0); __builtin_expect(!!(__ret_warn_on), 0); });

 if (__builtin_expect(!!(__ref_is_percpu(ref, &percpu_count)), 1)) {
  do { do { const void *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(*percpu_count)) { case 1: do { unsigned long __flags; do { ({ unsigned long __dummy; typeof(__flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); __flags = arch_local_irq_save(); } while (0); do { *({ do { const void *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__my_cpu_offset))); }); }) += 1; } while (0); do { ({ unsigned long __dummy; typeof(__flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); do { if (__builtin_expect(!!(!arch_irqs_disabled()), 0)) warn_bogus_irq_restore(); } while (0); arch_local_irq_restore(__flags); } while (0); } while (0);break; case 2: do { unsigned long __flags; do { ({ unsigned long __dummy; typeof(__flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); __flags = arch_local_irq_save(); } while (0); do { *({ do { const void *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__my_cpu_offset))); }); }) += 1; } while (0); do { ({ unsigned long __dummy; typeof(__flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); do { if (__builtin_expect(!!(!arch_irqs_disabled()), 0)) warn_bogus_irq_restore(); } while (0); arch_local_irq_restore(__flags); } while (0); } while (0);break; case 4: ({ typeof(*percpu_count) __retval; do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __retval = (typeof(*percpu_count))__percpu_add(({ do { const void *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__my_cpu_offset))); }); }), (1), sizeof(*percpu_count)); do { __asm__ __volatile__("": : :"memory"); __preempt_count_sub(1); } while (0); __retval; });break; case 8: ({ typeof(*percpu_count) __retval; do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __retval = (typeof(*percpu_count))__percpu_add(({ do { const void *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__my_cpu_offset))); }); }), (1), sizeof(*percpu_count)); do { __asm__ __volatile__("": : :"memory"); __preempt_count_sub(1); } while (0); __retval; });break; default: __bad_size_call_parameter();break; } } while (0);
  ret = true;
 } else if (!(ref->percpu_count_ptr & __PERCPU_REF_DEAD)) {
  ret = atomic_long_inc_not_zero(&ref->data->count);
 }
 return ret;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool percpu_ref_tryget_live(struct percpu_ref *ref)
{
 bool ret = false;

 rcu_read_lock();
 ret = percpu_ref_tryget_live_rcu(ref);
 rcu_read_unlock();
 return ret;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void percpu_ref_put_many(struct percpu_ref *ref, unsigned long nr)
{
 unsigned long *percpu_count;

 rcu_read_lock();

 if (__ref_is_percpu(ref, &percpu_count))
  do { do { const void *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(*percpu_count)) { case 1: do { unsigned long __flags; do { ({ unsigned long __dummy; typeof(__flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); __flags = arch_local_irq_save(); } while (0); do { *({ do { const void *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__my_cpu_offset))); }); }) += -(typeof(*percpu_count))(nr); } while (0); do { ({ unsigned long __dummy; typeof(__flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); do { if (__builtin_expect(!!(!arch_irqs_disabled()), 0)) warn_bogus_irq_restore(); } while (0); arch_local_irq_restore(__flags); } while (0); } while (0);break; case 2: do { unsigned long __flags; do { ({ unsigned long __dummy; typeof(__flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); __flags = arch_local_irq_save(); } while (0); do { *({ do { const void *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__my_cpu_offset))); }); }) += -(typeof(*percpu_count))(nr); } while (0); do { ({ unsigned long __dummy; typeof(__flags) __dummy2; (void)(&__dummy == &__dummy2); 1; }); do { if (__builtin_expect(!!(!arch_irqs_disabled()), 0)) warn_bogus_irq_restore(); } while (0); arch_local_irq_restore(__flags); } while (0); } while (0);break; case 4: ({ typeof(*percpu_count) __retval; do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __retval = (typeof(*percpu_count))__percpu_add(({ do { const void *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__my_cpu_offset))); }); }), (-(typeof(*percpu_count))(nr)), sizeof(*percpu_count)); do { __asm__ __volatile__("": : :"memory"); __preempt_count_sub(1); } while (0); __retval; });break; case 8: ({ typeof(*percpu_count) __retval; do { __preempt_count_add(1); __asm__ __volatile__("": : :"memory"); } while (0); __retval = (typeof(*percpu_count))__percpu_add(({ do { const void *__vpp_verify = (typeof((&(*percpu_count)) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long __ptr; __ptr = (unsigned long) ((typeof(*(&(*percpu_count))) *)(&(*percpu_count))); (typeof((typeof(*(&(*percpu_count))) *)(&(*percpu_count)))) (__ptr + ((__my_cpu_offset))); }); }), (-(typeof(*percpu_count))(nr)), sizeof(*percpu_count)); do { __asm__ __volatile__("": : :"memory"); __preempt_count_sub(1); } while (0); __retval; });break; default: __bad_size_call_parameter();break; } } while (0);
 else if (__builtin_expect(!!(atomic_long_sub_and_test(nr, &ref->data->count)), 0))
  ref->data->release(ref);

 rcu_read_unlock();
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void percpu_ref_put(struct percpu_ref *ref)
{
 percpu_ref_put_many(ref, 1);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool percpu_ref_is_dying(struct percpu_ref *ref)
{
 return ref->percpu_count_ptr & __PERCPU_REF_DEAD;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u32 __hash_32_generic(u32 val)
{
 return val * 0x61C88647;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u32 hash_32(u32 val, unsigned int bits)
{

 return __hash_32_generic(val) >> (32 - bits);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) u32 hash_64_generic(u64 val, unsigned int bits)
{


 return val * 0x61C8864680B583EBull >> (64 - bits);




}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u32 hash_ptr(const void *ptr, unsigned int bits)
{
 return hash_64_generic((unsigned long)ptr, bits);
}


static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) u32 hash32_ptr(const void *ptr)
{
 unsigned long val = (unsigned long)ptr;


 val ^= (val >> 32);

 return (u32)val;
}









static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool kasan_enabled(void)
{
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool kasan_hw_tags_enabled(void)
{
 return false;
}



struct kmem_cache;
struct page;
struct slab;
struct vm_struct;
struct task_struct;
typedef unsigned int kasan_vmalloc_flags_t;
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int kasan_add_zero_shadow(void *start, unsigned long size)
{
 return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kasan_remove_zero_shadow(void *start,
     unsigned long size)
{}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kasan_enable_current(void) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kasan_disable_current(void) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool kasan_has_integrated_init(void)
{
 return kasan_hw_tags_enabled();
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kasan_unpoison_range(const void *address, size_t size) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kasan_poison_pages(struct page *page, unsigned int order,
          bool init) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool kasan_unpoison_pages(struct page *page, unsigned int order,
     bool init)
{
 return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kasan_poison_slab(struct slab *slab) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kasan_unpoison_object_data(struct kmem_cache *cache,
     void *object) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kasan_poison_object_data(struct kmem_cache *cache,
     void *object) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void *kasan_init_slab_obj(struct kmem_cache *cache,
    const void *object)
{
 return (void *)object;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool kasan_slab_free(struct kmem_cache *s, void *object, bool init)
{
 return false;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kasan_kfree_large(void *ptr) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kasan_slab_free_mempool(void *ptr) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void *kasan_slab_alloc(struct kmem_cache *s, void *object,
       gfp_t flags, bool init)
{
 return object;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void *kasan_kmalloc(struct kmem_cache *s, const void *object,
    size_t size, gfp_t flags)
{
 return (void *)object;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void *kasan_kmalloc_large(const void *ptr, size_t size, gfp_t flags)
{
 return (void *)ptr;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void *kasan_krealloc(const void *object, size_t new_size,
     gfp_t flags)
{
 return (void *)object;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) bool kasan_check_byte(const void *address)
{
 return true;
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kasan_unpoison_task_stack(struct task_struct *task) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) size_t kasan_metadata_size(struct kmem_cache *cache,
      bool in_object)
{
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) slab_flags_t kasan_never_merge(void)
{
 return 0;
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kasan_cache_create(struct kmem_cache *cache,
          unsigned int *size,
          slab_flags_t *flags) {}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kasan_cache_shrink(struct kmem_cache *cache) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kasan_cache_shutdown(struct kmem_cache *cache) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kasan_record_aux_stack(void *ptr) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kasan_record_aux_stack_noalloc(void *ptr) {}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void *kasan_reset_tag(const void *addr)
{
 return (void *)addr;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kasan_init_sw_tags(void) { }






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kasan_init_hw_tags_cpu(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kasan_init_hw_tags(void) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kasan_populate_early_vm_area_shadow(void *start,
             unsigned long size) { }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int kasan_populate_vmalloc(unsigned long start,
     unsigned long size)
{
 return 0;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kasan_release_vmalloc(unsigned long start,
      unsigned long end,
      unsigned long free_region_start,
      unsigned long free_region_end) { }

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void *kasan_unpoison_vmalloc(const void *start,
        unsigned long size,
        kasan_vmalloc_flags_t flags)
{
 return (void *)start;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kasan_poison_vmalloc(const void *start, unsigned long size)
{ }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) int kasan_alloc_module_shadow(void *addr, size_t size, gfp_t gfp_mask) { return 0; }
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kasan_free_module_shadow(const struct vm_struct *vm) {}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void kasan_non_canonical_hook(unsigned long addr) { }

struct list_lru;
struct mem_cgroup;



bool slab_is_available(void);

struct kmem_cache *kmem_cache_create(const char *name, unsigned int size,
   unsigned int align, slab_flags_t flags,
   void (*ctor)(void *));
struct kmem_cache *kmem_cache_create_usercopy(const char *name,
   unsigned int size, unsigned int align,
   slab_flags_t flags,
   unsigned int useroffset, unsigned int usersize,
   void (*ctor)(void *));
void kmem_cache_destroy(struct kmem_cache *s);
int kmem_cache_shrink(struct kmem_cache *s);
void * __attribute__((__warn_unused_result__)) krealloc(const void *objp, size_t new_size, gfp_t flags) __attribute__((__alloc_size__(2)));
void kfree(const void *objp);
void kfree_sensitive(const void *objp);
size_t __ksize(const void *objp);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void __free_kfree(void *p) { void * _T = *(void * *)p; if (_T) kfree(_T); }
size_t ksize(const void *objp);


bool kmem_valid_obj(void *object);
void kmem_dump_obj(void *object);
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) unsigned int arch_slab_minalign(void)
{
 return __alignof__(unsigned long long);
}
enum kmalloc_cache_type {
 KMALLOC_NORMAL = 0,

 KMALLOC_DMA = KMALLOC_NORMAL,




 KMALLOC_RANDOM_START = KMALLOC_NORMAL,
 KMALLOC_RANDOM_END = KMALLOC_RANDOM_START + 0,

 KMALLOC_RECLAIM = KMALLOC_NORMAL,







 KMALLOC_CGROUP,

 NR_KMALLOC_TYPES
};

extern struct kmem_cache *
kmalloc_caches[NR_KMALLOC_TYPES][(14 + 1) + 1];
extern unsigned long random_kmalloc_seed;

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) enum kmalloc_cache_type kmalloc_type(gfp_t flags, unsigned long caller)
{




 if (__builtin_expect(!!((flags & ((( gfp_t)0x10u) | (0 ? (( gfp_t)0x01u) : 0) | (1 ? (( gfp_t)0x400000u) : 0))) == 0), 1))





  return KMALLOC_NORMAL;
 if (0 && (flags & (( gfp_t)0x01u)))
  return KMALLOC_DMA;
 if (!1 || (flags & (( gfp_t)0x10u)))
  return KMALLOC_RECLAIM;
 else
  return KMALLOC_CGROUP;
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) unsigned int __kmalloc_index(size_t size,
          bool size_is_constant)
{
 if (!size)
  return 0;

 if (size <= (1 << 3))
  return 3;

 if ((1 << 3) <= 32 && size > 64 && size <= 96)
  return 1;
 if ((1 << 3) <= 64 && size > 128 && size <= 192)
  return 2;
 if (size <= 8) return 3;
 if (size <= 16) return 4;
 if (size <= 32) return 5;
 if (size <= 64) return 6;
 if (size <= 128) return 7;
 if (size <= 256) return 8;
 if (size <= 512) return 9;
 if (size <= 1024) return 10;
 if (size <= 2 * 1024) return 11;
 if (size <= 4 * 1024) return 12;
 if (size <= 8 * 1024) return 13;
 if (size <= 16 * 1024) return 14;
 if (size <= 32 * 1024) return 15;
 if (size <= 64 * 1024) return 16;
 if (size <= 128 * 1024) return 17;
 if (size <= 256 * 1024) return 18;
 if (size <= 512 * 1024) return 19;
 if (size <= 1024 * 1024) return 20;
 if (size <= 2 * 1024 * 1024) return 21;

 if (!0 && size_is_constant)
  do { __attribute__((__noreturn__)) extern void __compiletime_assert_108(void) __attribute__((__error__("unexpected size in kmalloc_index()"))); if (!(!(1))) __compiletime_assert_108(); } while (0);
 else
  do { do { } while(0); asm __inline volatile (".pushsection __bug_table, \"aw\"; .align 2; 10000: .long 10001f - .; .pushsection .rodata.str, \"aMS\", @progbits, 1; 10002: .string \"include/linux/slab.h\"; .popsection; .long 10002b - .; .short 479; .short 0; .popsection; 10001: break 1");; do { ; __builtin_unreachable(); } while (0); } while (0);


 return -1;
}
_Static_assert(14 <= 20, "PAGE_SHIFT <= 20");


void *__kmalloc(size_t size, gfp_t flags) __attribute__((__assume_aligned__(__alignof__(unsigned long long)))) __attribute__((__alloc_size__(1))) __attribute__((__malloc__));
void *kmem_cache_alloc(struct kmem_cache *cachep, gfp_t flags) __attribute__((__assume_aligned__(__alignof__(unsigned long long)))) __attribute__((__malloc__));
void *kmem_cache_alloc_lru(struct kmem_cache *s, struct list_lru *lru,
      gfp_t gfpflags) __attribute__((__assume_aligned__(__alignof__(unsigned long long)))) __attribute__((__malloc__));
void kmem_cache_free(struct kmem_cache *s, void *objp);
void kmem_cache_free_bulk(struct kmem_cache *s, size_t size, void **p);
int kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t size, void **p);

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) void kfree_bulk(size_t size, void **p)
{
 kmem_cache_free_bulk(((void *)0), size, p);
}

void *__kmalloc_node(size_t size, gfp_t flags, int node) __attribute__((__assume_aligned__(__alignof__(unsigned long long))))
        __attribute__((__alloc_size__(1))) __attribute__((__malloc__));
void *kmem_cache_alloc_node(struct kmem_cache *s, gfp_t flags, int node) __attribute__((__assume_aligned__(__alignof__(unsigned long long))))
          __attribute__((__malloc__));

void *kmalloc_trace(struct kmem_cache *s, gfp_t flags, size_t size)
      __attribute__((__assume_aligned__(__alignof__(unsigned long long)))) __attribute__((__alloc_size__(3))) __attribute__((__malloc__));

void *kmalloc_node_trace(struct kmem_cache *s, gfp_t gfpflags,
    int node, size_t size) __attribute__((__assume_aligned__(__alignof__(unsigned long long))))
      __attribute__((__alloc_size__(4))) __attribute__((__malloc__));
void *kmalloc_large(size_t size, gfp_t flags) __attribute__((__assume_aligned__(((1UL) << 14))))
           __attribute__((__alloc_size__(1))) __attribute__((__malloc__));

void *kmalloc_large_node(size_t size, gfp_t flags, int node) __attribute__((__assume_aligned__(((1UL) << 14))))
            __attribute__((__alloc_size__(1))) __attribute__((__malloc__));
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __attribute__((__alloc_size__(1))) __attribute__((__malloc__)) void *kmalloc(size_t size, gfp_t flags)
{
 if (__builtin_constant_p(size) && size) {
  unsigned int index;

  if (size > (1UL << (14 + 1)))
   return kmalloc_large(size, flags);

  index = __kmalloc_index(size, true);
  return kmalloc_trace(
    kmalloc_caches[kmalloc_type(flags, (unsigned long)__builtin_return_address(0))][index],
    flags, size);
 }
 return __kmalloc(size, flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__always_inline__)) __attribute__((__alloc_size__(1))) __attribute__((__malloc__)) void *kmalloc_node(size_t size, gfp_t flags, int node)
{
 if (__builtin_constant_p(size) && size) {
  unsigned int index;

  if (size > (1UL << (14 + 1)))
   return kmalloc_large_node(size, flags, node);

  index = __kmalloc_index(size, true);
  return kmalloc_node_trace(
    kmalloc_caches[kmalloc_type(flags, (unsigned long)__builtin_return_address(0))][index],
    flags, node, size);
 }
 return __kmalloc_node(size, flags, node);
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__alloc_size__(1, 2))) __attribute__((__malloc__)) void *kmalloc_array(size_t n, size_t size, gfp_t flags)
{
 size_t bytes;

 if (__builtin_expect(!!(__must_check_overflow(__builtin_mul_overflow(n, size, &bytes))), 0))
  return ((void *)0);
 if (__builtin_constant_p(n) && __builtin_constant_p(size))
  return kmalloc(bytes, flags);
 return __kmalloc(bytes, flags);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__alloc_size__(2, 3))) void * __attribute__((__warn_unused_result__)) krealloc_array(void *p,
              size_t new_n,
              size_t new_size,
              gfp_t flags)
{
 size_t bytes;

 if (__builtin_expect(!!(__must_check_overflow(__builtin_mul_overflow(new_n, new_size, &bytes))), 0))
  return ((void *)0);

 return krealloc(p, bytes, flags);
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__alloc_size__(1, 2))) __attribute__((__malloc__)) void *kcalloc(size_t n, size_t size, gfp_t flags)
{
 return kmalloc_array(n, size, flags | (( gfp_t)0x100u));
}

void *__kmalloc_node_track_caller(size_t size, gfp_t flags, int node,
      unsigned long caller) __attribute__((__alloc_size__(1))) __attribute__((__malloc__));
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__alloc_size__(1, 2))) __attribute__((__malloc__)) void *kmalloc_array_node(size_t n, size_t size, gfp_t flags,
         int node)
{
 size_t bytes;

 if (__builtin_expect(!!(__must_check_overflow(__builtin_mul_overflow(n, size, &bytes))), 0))
  return ((void *)0);
 if (__builtin_constant_p(n) && __builtin_constant_p(size))
  return kmalloc_node(bytes, flags, node);
 return __kmalloc_node(bytes, flags, node);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__alloc_size__(1, 2))) __attribute__((__malloc__)) void *kcalloc_node(size_t n, size_t size, gfp_t flags, int node)
{
 return kmalloc_array_node(n, size, flags | (( gfp_t)0x100u), node);
}




static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) void *kmem_cache_zalloc(struct kmem_cache *k, gfp_t flags)
{
 return kmem_cache_alloc(k, flags | (( gfp_t)0x100u));
}






static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__alloc_size__(1))) __attribute__((__malloc__)) void *kzalloc(size_t size, gfp_t flags)
{
 return kmalloc(size, flags | (( gfp_t)0x100u));
}







static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__alloc_size__(1))) __attribute__((__malloc__)) void *kzalloc_node(size_t size, gfp_t flags, int node)
{
 return kmalloc_node(size, flags | (( gfp_t)0x100u), node);
}

extern void *kvmalloc_node(size_t size, gfp_t flags, int node) __attribute__((__alloc_size__(1))) __attribute__((__malloc__));
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__alloc_size__(1))) __attribute__((__malloc__)) void *kvmalloc(size_t size, gfp_t flags)
{
 return kvmalloc_node(size, flags, (-1));
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__alloc_size__(1))) __attribute__((__malloc__)) void *kvzalloc_node(size_t size, gfp_t flags, int node)
{
 return kvmalloc_node(size, flags | (( gfp_t)0x100u), node);
}
static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__alloc_size__(1))) __attribute__((__malloc__)) void *kvzalloc(size_t size, gfp_t flags)
{
 return kvmalloc(size, flags | (( gfp_t)0x100u));
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__alloc_size__(1, 2))) __attribute__((__malloc__)) void *kvmalloc_array(size_t n, size_t size, gfp_t flags)
{
 size_t bytes;

 if (__builtin_expect(!!(__must_check_overflow(__builtin_mul_overflow(n, size, &bytes))), 0))
  return ((void *)0);

 return kvmalloc(bytes, flags);
}

static inline __attribute__((__gnu_inline__)) __attribute__((__unused__)) __attribute__((patchable_function_entry(0, 0))) __attribute__((__alloc_size__(1, 2))) __attribute__((__malloc__)) void *kvcalloc(size_t n, size_t size, gfp_t flags)
{
 return kvmalloc_array(n, size, flags | (( gfp_t)0x100u));
}

extern void *kvrealloc(const void *p, size_t oldsize, size_t newsize, gfp_t flags)
        __attribute__((__alloc_size__(3)));
extern void kvfree(const void *addr);
extern void kvfree_sensitive(const void *addr, size_t len);

unsigned int kmem_cache_size(struct kmem_cache *s);
size_t kmalloc_size_roundup(size_t size);

void __attribute__((__section__(".init.text"))) __attribute__((__cold__)) kmem_cache_init_late(void);

#include "fortify_test.c"
